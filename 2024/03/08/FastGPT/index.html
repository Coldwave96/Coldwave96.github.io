<!DOCTYPE html><html lang="zh-Hans"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="基于本地大模型的 FastGPT 框架部署"><meta name="keywords" content="FastGPT,Docker Compose"><meta name="author" content="ColdSnap"><meta name="copyright" content="ColdSnap"><title>基于本地大模型的 FastGPT 框架部署 | ColdSnap の Blog</title><link rel="shortcut icon" href="/favicon.ico"><link rel="stylesheet" href="/css/index.css?version=1.8.2"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.8.2"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script><script>(adsbygoogle = window.adsbygoogle || []).push({
  google_ad_client: 'ca-pub-4826942730187324',
  enable_page_level_ads: 'true'
});
</script><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"找不到您查询的内容:${query}"}},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  hexoVersion: '6.3.0'
} </script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="ColdSnap の Blog" type="application/atom+xml">
</head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%89%8D%E8%A8%80"><span class="toc-number">1.</span> <span class="toc-text">前言</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B4%E4%BD%93%E6%A1%86%E6%9E%B6%E8%AF%B4%E6%98%8E"><span class="toc-number">2.</span> <span class="toc-text">整体框架说明</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#FastGPT"><span class="toc-number">2.1.</span> <span class="toc-text">FastGPT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%80%E5%8F%91%E7%BB%84%E4%BB%B6%E4%BE%9D%E8%B5%96"><span class="toc-number">2.1.1.</span> <span class="toc-text">开发组件依赖</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="toc-number">2.2.</span> <span class="toc-text">数据库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#OneAPI"><span class="toc-number">2.3.</span> <span class="toc-text">OneAPI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%AC%E5%9C%B0%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.</span> <span class="toc-text">本地大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.1.</span> <span class="toc-text">对话大模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Embedding-%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text">Embedding 模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ReRank-%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.3.</span> <span class="toc-text">ReRank 模型</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E-Docker-Compose-%E7%9A%84%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2"><span class="toc-number">3.</span> <span class="toc-text">基于 Docker Compose 的快速部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E6%9C%AC%E5%9C%B0%E6%A8%A1%E5%9E%8B-OneAPI"><span class="toc-number">3.1.</span> <span class="toc-text">部署本地模型 + OneAPI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%85%8D%E7%BD%AE-OneAPI"><span class="toc-number">3.2.</span> <span class="toc-text">配置 OneAPI</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2-FastGPT"><span class="toc-number">3.3.</span> <span class="toc-text">部署 FastGPT</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE-FastGPT"><span class="toc-number">3.4.</span> <span class="toc-text">访问 FastGPT</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2"><span class="toc-number">4.</span> <span class="toc-text">分布式部署</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2%E5%AF%B9%E8%AF%9D%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.1.</span> <span class="toc-text">部署对话大模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Baichuan2-13B-Chat"><span class="toc-number">4.1.1.</span> <span class="toc-text">Baichuan2-13B-Chat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Baichuan2-7B-Chat"><span class="toc-number">4.1.2.</span> <span class="toc-text">Baichuan2-7B-Chat</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ChatGLM2-6B"><span class="toc-number">4.1.3.</span> <span class="toc-text">ChatGLM2-6B</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2-Embedding-%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">部署 Embedding 模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#M3E-large"><span class="toc-number">4.2.1.</span> <span class="toc-text">M3E-large</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2-ReRank-%E9%87%8D%E6%8E%92%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.3.</span> <span class="toc-text">部署 ReRank 重排模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BGE-ReRanker-Base"><span class="toc-number">4.3.1.</span> <span class="toc-text">BGE-ReRanker-Base</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2-OneAPI"><span class="toc-number">4.4.</span> <span class="toc-text">部署 OneAPI</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Docker-%E9%83%A8%E7%BD%B2"><span class="toc-number">4.4.1.</span> <span class="toc-text">Docker 部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E4%BB%A4%E7%89%8C"><span class="toc-number">4.4.2.</span> <span class="toc-text">创建令牌</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A5%E5%85%A5%E6%9C%AC%E5%9C%B0%E9%97%AE%E7%AD%94%E5%A4%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.3.</span> <span class="toc-text">接入本地问答大模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A5%E5%85%A5%E6%9C%AC%E5%9C%B0-Embedding-%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.4.4.</span> <span class="toc-text">接入本地 Embedding 模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%83%A8%E7%BD%B2-FastGPT-1"><span class="toc-number">4.5.</span> <span class="toc-text">部署 FastGPT</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Docker-Compose-%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2"><span class="toc-number">4.5.1.</span> <span class="toc-text">Docker Compose 快速部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BF%E9%97%AE-FastGPT-1"><span class="toc-number">4.5.2.</span> <span class="toc-text">访问 FastGPT</span></a></li></ol></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.png"></div><div class="author-info__name text-center">ColdSnap</div><div class="author-info__description text-center">Welcome to ColdSnap の Blog</div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/Coldwave96">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">91</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">41</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">35</span></a></div><hr><div class="author-info-links"><div class="author-info-links__title text-center">Links</div><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="http://bodhidharmalu.github.io">Tigerlu</a><a class="author-info-links__name text-center" target="_blank" rel="noopener" href="http://ghealer.top">Ghealer</a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/background.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">ColdSnap の Blog</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/Library">Library</a><a class="site-page" href="/Tools">Tools</a><a class="site-page" href="/about">About</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> 搜索</span></a></span></div><div id="post-info"><div id="post-title">基于本地大模型的 FastGPT 框架部署</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2024-03-08</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/AI/">AI</a><i class="fa fa-angle-right" aria-hidden="true"></i><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/AI/Sites/">Sites</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2.9k</span><span class="post-meta__separator">|</span><span>阅读时长: 11 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><a target="_blank" rel="noopener" href="https://fastgpt.in/">FastGPT</a> 是一款强大的 LLM + RAG 解决方案，本文记录了基于本地大模型搭建 FastGPT 框架的过程。</p>
<span id="more"></span>

<h1 id="整体框架说明"><a href="#整体框架说明" class="headerlink" title="整体框架说明"></a>整体框架说明</h1><center>
    <img src="/img/FastGPT/sealos-fastgpt.png" width="850">
</center>

<ul>
<li>整体框架由 4 部分组成，分别为数据库、FastGPT、OneAPI、大模型</li>
</ul>
<h2 id="FastGPT"><a href="#FastGPT" class="headerlink" title="FastGPT"></a>FastGPT</h2><ul>
<li>框架本体，默认使用 OpenAI 的大模型接口</li>
</ul>
<h3 id="开发组件依赖"><a href="#开发组件依赖" class="headerlink" title="开发组件依赖"></a>开发组件依赖</h3><ul>
<li>Docker</li>
<li>Node.js v18.x</li>
<li>pnmp 版本 8.x.x</li>
</ul>
<h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul>
<li><p>MongoDB 用于存储 FastGPT 框架运行所需的数据</p>
</li>
<li><p>pgvector 用于存储知识库向量</p>
<table>
<thead>
<tr>
<th>环境</th>
<th>最低配置（单节点）</th>
<th>推荐配置</th>
</tr>
</thead>
<tbody><tr>
<td>测试</td>
<td>2c2g</td>
<td>2c4g</td>
</tr>
<tr>
<td>100w 组向量</td>
<td>4c8g 50GB</td>
<td>4c16g 50GB</td>
</tr>
<tr>
<td>500w 组向量</td>
<td>8c32g 200GB</td>
<td>16c64g 200GB</td>
</tr>
</tbody></table>
</li>
</ul>
<h2 id="OneAPI"><a href="#OneAPI" class="headerlink" title="OneAPI"></a>OneAPI</h2><ul>
<li>OneAPI 是大模型调用接口框架，负责对接大模型调用接口，提供权限控制和收费统计等功能</li>
</ul>
<h2 id="本地大模型"><a href="#本地大模型" class="headerlink" title="本地大模型"></a>本地大模型</h2><ul>
<li>提供的本地大模型接口需要符合 OpenAI 的接口规范</li>
<li>综合显存需求<ul>
<li>最少：32GB+</li>
<li>推荐：48GB+</li>
</ul>
</li>
</ul>
<h3 id="对话大模型"><a href="#对话大模型" class="headerlink" title="对话大模型"></a>对话大模型</h3><ul>
<li>Baichuan 2</li>
<li>ChatGLM 2&#x2F;3</li>
</ul>
<h3 id="Embedding-模型"><a href="#Embedding-模型" class="headerlink" title="Embedding 模型"></a>Embedding 模型</h3><ul>
<li>m3e-large</li>
</ul>
<h3 id="ReRank-模型"><a href="#ReRank-模型" class="headerlink" title="ReRank 模型"></a>ReRank 模型</h3><ul>
<li>bge-reranker-large</li>
</ul>
<h1 id="基于-Docker-Compose-的快速部署"><a href="#基于-Docker-Compose-的快速部署" class="headerlink" title="基于 Docker Compose 的快速部署"></a>基于 Docker Compose 的快速部署</h1><h2 id="部署本地模型-OneAPI"><a href="#部署本地模型-OneAPI" class="headerlink" title="部署本地模型 + OneAPI"></a>部署本地模型 + OneAPI</h2><ul>
<li><p>部署实例中使用的本地模型组合为 Baichuan2-7B-Chat + M3E-large + BGE-ReRanker-base</p>
</li>
<li><p>硬件资源和前置需求见下方分布式部署相关章节</p>
</li>
<li><p>在本地创建文件夹并下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> llm</span><br><span class="line"><span class="built_in">cd</span> llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 docker-compose.yml 文件</span></span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-DockerCompose/docker-compose.yml</span><br></pre></td></tr></table></figure>
</li>
<li><p>根据<code>docker-compose.yml</code>文件中的提示修改对应的信息</p>
</li>
<li><p>通过以下命令控制相关容器</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">docker compose up -d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">docker compose down</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="配置-OneAPI"><a href="#配置-OneAPI" class="headerlink" title="配置 OneAPI"></a>配置 OneAPI</h2><ul>
<li>登陆<code>http://&lt;gpu_server_ip&gt;:8000</code>，初始账号密码<code>root/123456</code> ，登陆成功后及时修改默认密码</li>
<li>点击<code>令牌</code>→<code>添加新的令牌</code>，输入名称，内部使用可设置<code>永不过期 + 设置无限额度</code></li>
<li>回到<code>令牌</code>，点击<code>复制</code>即可获取<code>Token</code></li>
<li>点击<code>渠道</code>→<code>添加新的渠道</code><ul>
<li>添加 Baichuan2-7B-Chat<ul>
<li>类型：<code>OpenAI</code></li>
<li>名称：<code>Baichuan2-7B-Chat</code>（随意）</li>
<li>模型：<code>Baichuan2-7B-Chat</code>（随意，FastGPT 配置文件中与之对应即可）</li>
<li>密钥：本地大模型接口的<code>SK-KEY</code> 值</li>
<li>代理：<code>http://&lt;gpu_server_ip&gt;:8001</code></li>
</ul>
</li>
<li>添加 M3E-large<ul>
<li>类型：<code>自定义渠道</code></li>
<li>Base URL：<code>http://&lt;gpu_server_ip&gt;:8002</code></li>
<li>名称：<code>M3E-large</code>（随意）</li>
<li>模型：<code>M3E-large</code>（随意，FastGPT 配置文件中与之对应即可）</li>
<li>密钥：本地模型接口的<code>SK-KEY</code>值</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="部署-FastGPT"><a href="#部署-FastGPT" class="headerlink" title="部署 FastGPT"></a>部署 FastGPT</h2><ul>
<li><p>在本地创建文件夹并下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> fastgpt</span><br><span class="line"><span class="built_in">cd</span> fastgpt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载相关文件</span></span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/docker-compose.yml</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/config.json</span><br></pre></td></tr></table></figure>
<ul>
<li>注意: docker-compose.yml 配置文件中 Mongo 为 5.x，部分服务器不支持，需手动更改其镜像版本为 4.4.24</li>
</ul>
</li>
<li><p>修改 docker-compose.yml 中的 OPENAI_BASE_URL（API 接口的地址，需要加&#x2F;v1）和CHAT_API_KEY（API 接口的凭证)</p>
</li>
<li><p>使用 OneAPI 的话，OPENAI_BASE_URL&#x3D;OneAPI访问地址&#x2F;v1；CHAT_API_KEY&#x3D;令牌</p>
</li>
<li><p>修改 config.json 中的本地问答大模型、Embedding 模型以及 ReRank 模型的相关信息</p>
</li>
<li><p>在 docker-compose.yml 同级目录下执行</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入项目目录</span></span><br><span class="line"><span class="built_in">cd</span> 项目目录</span><br><span class="line"><span class="comment"># 创建 mongo 密钥</span></span><br><span class="line">openssl rand -<span class="built_in">base64</span> 756 &gt; ./mongodb.key</span><br><span class="line"><span class="comment"># 600不行可以用chmod 999</span></span><br><span class="line"><span class="built_in">chmod</span> 600 ./mongodb.key</span><br><span class="line"><span class="built_in">chown</span> 999:root ./mongodb.key</span><br><span class="line"><span class="comment"># 启动容器</span></span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化 Mongo 副本集(4.6.8以前可忽略)</p>
<ul>
<li>Mongo 数据库需要修改副本集的host，从原来的mongo:27017修改为ip:27017。</li>
</ul>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 mongo 容器是否正常运行</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mongo bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">mongo -u myname -p mypassword --authenticationDatabase admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化副本集。如果需要外网访问，mongo:27017 可以改成 ip:27017。但是需要同时修改 FastGPT 连接的参数（MONGODB_URI=mongodb://myname:mypassword@mongo:27017/fastgpt?authSource=admin =&gt; MONGODB_URI=mongodb://myname:mypassword@ip:27017/fastgpt?authSource=admin）</span></span><br><span class="line">rs.initiate(&#123;</span><br><span class="line">  _id: <span class="string">&quot;rs0&quot;</span>,</span><br><span class="line">  members: [</span><br><span class="line">    &#123; _id: 0, host: <span class="string">&quot;mongo:27017&quot;</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># 检查状态。如果提示 rs0 状态，则代表运行成功</span></span><br><span class="line">rs.status()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="访问-FastGPT"><a href="#访问-FastGPT" class="headerlink" title="访问 FastGPT"></a>访问 FastGPT</h2><ul>
<li>目前可以通过<code>http://&lt;ip&gt;:8080</code>直接访问(注意防火墙)。登录用户名为<code>root</code>，密码为<code>docker-compose.yml</code>环境变量里设置的<code>DEFAULT_ROOT_PSW</code></li>
<li>如果需要域名访问，请自行安装并配置 Nginx</li>
</ul>
<h1 id="分布式部署"><a href="#分布式部署" class="headerlink" title="分布式部署"></a>分布式部署</h1><h2 id="部署对话大模型"><a href="#部署对话大模型" class="headerlink" title="部署对话大模型"></a>部署对话大模型</h2><ul>
<li>选择以下其中一种大模型部署即可</li>
</ul>
<h3 id="Baichuan2-13B-Chat"><a href="#Baichuan2-13B-Chat" class="headerlink" title="Baichuan2-13B-Chat"></a>Baichuan2-13B-Chat</h3><ul>
<li><p>推荐配置</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>内存</th>
<th>显存</th>
<th>硬盘空间</th>
<th>启动命令</th>
</tr>
</thead>
<tbody><tr>
<td>fp16</td>
<td>≥ 32GB</td>
<td>≥ 28GB</td>
<td>≥ 50GB</td>
<td>python openai_api.py</td>
</tr>
<tr>
<td>int8</td>
<td>≥ 32GB</td>
<td>≥ 17GB</td>
<td>≥ 50GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td>
</tr>
<tr>
<td>int4</td>
<td>≥ 32GB</td>
<td>≥ 9GB</td>
<td>≥ 50GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td>
</tr>
</tbody></table>
</li>
<li><p>部署环境要求</p>
<ul>
<li>Python 3.10</li>
<li>NVIDIA 驱动 + CUDA 等套件</li>
</ul>
</li>
<li><p>源码部署</p>
<ul>
<li><p>将 Baichuan2-13B-Chat 模型文件下载到本地</p>
<p>  <a target="_blank" rel="noopener" href="https://modelscope.cn/models/baichuan-inc/Baichuan2-13B-Chat">百川2-13B-对话模型</a></p>
</li>
<li><p>下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-13B-Chat/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-13B-Chat/requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p>
</li>
<li><p>修改<code>openai_api.py</code>文件中模型名称<code>baichuan-inc/Baichuan2-13B-Chat</code>为本地模型所在文件夹</p>
</li>
<li><p>运行启动命令<code>python openai_api.py</code></p>
</li>
</ul>
</li>
</ul>
<h3 id="Baichuan2-7B-Chat"><a href="#Baichuan2-7B-Chat" class="headerlink" title="Baichuan2-7B-Chat"></a>Baichuan2-7B-Chat</h3><ul>
<li><p>推荐配置</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>内存</th>
<th>显存</th>
<th>硬盘空间</th>
<th>启动命令</th>
</tr>
</thead>
<tbody><tr>
<td>fp16</td>
<td>≥ 16GB</td>
<td>≥ 16GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py</td>
</tr>
<tr>
<td>int8</td>
<td>≥ 16GB</td>
<td>≥ 9GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td>
</tr>
<tr>
<td>int4</td>
<td>≥ 16GB</td>
<td>≥ 6GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td>
</tr>
</tbody></table>
</li>
<li><p>部署环境要求</p>
<ul>
<li>Python 3.10</li>
<li>NVIDIA 驱动 + CUDA 等套件</li>
</ul>
</li>
<li><p>源码部署</p>
<ul>
<li><p>将 Baichuan2-7B-Chat 模型文件下载到本地</p>
<p>  <a target="_blank" rel="noopener" href="https://modelscope.cn/models/baichuan-inc/Baichuan2-7B-Chat">百川2-7B-对话模型</a></p>
</li>
<li><p>下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-7B-Chat/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-7B-Chat/requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p>
</li>
<li><p>修改<code>openai_api.py</code>文件中模型名称<code>baichuan-inc/Baichuan2-7B-Chat</code>为本地模型所在文件夹</p>
</li>
<li><p>运行启动命令<code>python openai_api.py</code></p>
</li>
</ul>
</li>
</ul>
<h3 id="ChatGLM2-6B"><a href="#ChatGLM2-6B" class="headerlink" title="ChatGLM2-6B"></a>ChatGLM2-6B</h3><ul>
<li><p>推荐配置</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>内存</th>
<th>显存</th>
<th>硬盘空间</th>
<th>启动命令</th>
</tr>
</thead>
<tbody><tr>
<td>fp16</td>
<td>≥ 16GB</td>
<td>≥ 16GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py</td>
</tr>
<tr>
<td>int8</td>
<td>≥ 16GB</td>
<td>≥ 9GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td>
</tr>
<tr>
<td>int4</td>
<td>≥ 16GB</td>
<td>≥ 6GB</td>
<td>≥ 25GB</td>
<td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td>
</tr>
</tbody></table>
</li>
<li><p>部署环境要求</p>
<ul>
<li>Python 3.10</li>
<li>NVIDIA 驱动 + CUDA 等套件</li>
</ul>
</li>
<li><p>源码部署</p>
<ul>
<li><p>将 ChatGLM2-6B 模型文件下载到本地</p>
<p>  <a target="_blank" rel="noopener" href="https://modelscope.cn/models/ZhipuAI/chatglm2-6b/">chatglm2-6b</a></p>
</li>
<li><p>下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/ChatGLM2-6B/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/ChatGLM2-6B/requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p>
</li>
<li><p>修改<code>openai_api.py</code>文件中模型名称<code>THUDM/chatglm2-6b</code>为本地模型所在文件夹</p>
</li>
<li><p>运行启动命令<code>python openai_api.py</code></p>
</li>
</ul>
</li>
</ul>
<h2 id="部署-Embedding-模型"><a href="#部署-Embedding-模型" class="headerlink" title="部署 Embedding 模型"></a>部署 Embedding 模型</h2><h3 id="M3E-large"><a href="#M3E-large" class="headerlink" title="M3E-large"></a>M3E-large</h3><ul>
<li><p>推荐配置</p>
<table>
<thead>
<tr>
<th>内存</th>
<th>显存</th>
<th>硬盘空间</th>
<th>启动命令</th>
</tr>
</thead>
<tbody><tr>
<td>≥ 8GB</td>
<td>≥ 6GB</td>
<td>≥ 10GB</td>
<td>python openai_api.py</td>
</tr>
</tbody></table>
</li>
<li><p>部署环境要求</p>
<ul>
<li>Python 3.8</li>
<li>NVIDIA 驱动 + CUDA 等套件</li>
</ul>
</li>
<li><p>源码部署</p>
<ul>
<li><p>将 M3E-large 模型文件下载到本地</p>
<p>  <a target="_blank" rel="noopener" href="https://modelscope.cn/models/Jerry0/M3E-large/">M3E-large</a></p>
</li>
<li><p>下载相关文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Embedding/M3E-large/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Embedding/M3E-large/requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p>
</li>
<li><p>修改<code>openai_api.py</code>文件中模型名称<code>moka-ai/m3e-large</code>为本地模型所在文件夹</p>
</li>
<li><p>运行启动命令<code>python openai_api.py</code></p>
</li>
</ul>
</li>
</ul>
<h2 id="部署-ReRank-重排模型"><a href="#部署-ReRank-重排模型" class="headerlink" title="部署 ReRank 重排模型"></a>部署 ReRank 重排模型</h2><h3 id="BGE-ReRanker-Base"><a href="#BGE-ReRanker-Base" class="headerlink" title="BGE-ReRanker-Base"></a>BGE-ReRanker-Base</h3><ul>
<li><p>推荐配置</p>
<table>
<thead>
<tr>
<th>类型</th>
<th>内存</th>
<th>显存</th>
<th>硬盘空间</th>
<th>启动命令</th>
</tr>
</thead>
<tbody><tr>
<td>base</td>
<td>≥ 4GB</td>
<td>≥ 3GB</td>
<td>≥ 8GB</td>
<td>python api.py</td>
</tr>
</tbody></table>
</li>
<li><p>部署环境要求</p>
<ul>
<li>Python 3.10</li>
<li>NVIDIA 驱动 + CUDA 等套件</li>
</ul>
</li>
<li><p>源码部署</p>
<ul>
<li><p>将 BGE-ReRanker-base 模型下载到本地</p>
<p>  <a target="_blank" rel="noopener" href="https://huggingface.co/BAAI/bge-reranker-base">BAAI&#x2F;bge-reranker-base · Hugging Face</a></p>
</li>
<li><p>下载相关文件（与存放模型的文件夹在同一级）</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-ReRanker/BGE-ReRanker-base/api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-ReRanker/BGE-ReRanker-base/requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>安装依赖</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure>
</li>
<li><p>添加环境变量<code>export ACCESS_TOKEN=XXXXXX</code>配置 token，这里的 token 只是加一层验证，防止接口被人盗用，默认值为<code>ACCESS_TOKEN</code></p>
</li>
<li><p>修改<code>api.py</code>文件中<code>bge-reranker-large</code>为存储本地模型文件夹名称</p>
</li>
<li><p>运行启动命令<code>python api.py</code></p>
</li>
</ul>
</li>
</ul>
<h2 id="部署-OneAPI"><a href="#部署-OneAPI" class="headerlink" title="部署 OneAPI"></a>部署 OneAPI</h2><h3 id="Docker-部署"><a href="#Docker-部署" class="headerlink" title="Docker 部署"></a>Docker 部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 SQLite 的部署命令：</span></span><br><span class="line">docker run --name one-api -d --restart always -p <span class="number">3000</span>:<span class="number">3000</span> -e TZ=Asia/Shanghai -v /home/ubuntu/data/one-api:/data justsong/one-api</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 MySQL 的部署命令，在上面的基础上添加 `-e SQL_DSN=&quot;root:123456@tcp(localhost:3306)/oneapi&quot;`，请自行修改数据库连接参数，不清楚如何修改请参见下面环境变量一节。</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line">docker run --name one-api -d --restart always -p <span class="number">3000</span>:<span class="number">3000</span> -e SQL_DSN=<span class="string">&quot;root:123456@tcp(localhost:3306)/oneapi&quot;</span> -e TZ=Asia/Shanghai -v /home/ubuntu/data/one-api:/data justsong/one-api</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>其中，-p 3000:3000 中的第一个 3000 是宿主机的端口，可以根据需要进行修改</li>
<li>数据和日志将会保存在宿主机的<code>/home/ubuntu/data/one-api</code>目录，请确保该目录存在且具有写入权限，或者更改为合适的目录</li>
<li>如果启动失败，请添加<code>-privileged=true</code></li>
<li>访问<code>http://&lt;ip&gt;:3000/</code>并登录。初始账号用户名为<code>root</code>，密码为<code>123456</code></li>
</ul>
<h3 id="创建令牌"><a href="#创建令牌" class="headerlink" title="创建令牌"></a>创建令牌</h3><ul>
<li>可设置永不过期，无限额度</li>
</ul>
<h3 id="接入本地问答大模型"><a href="#接入本地问答大模型" class="headerlink" title="接入本地问答大模型"></a>接入本地问答大模型</h3><ul>
<li>类型：OpenAI</li>
<li>名称：随便写</li>
<li>模型：自定义模型名称</li>
<li>代理：本地大模型开放的 OpenAI 格式的 API 接口地址</li>
</ul>
<h3 id="接入本地-Embedding-模型"><a href="#接入本地-Embedding-模型" class="headerlink" title="接入本地 Embedding 模型"></a>接入本地 Embedding 模型</h3><ul>
<li>类型：自定义渠道</li>
<li>Base URL：Embedding 模型开放的 OpenAI 格式的 API 接口地址</li>
<li>名称：随便写</li>
<li>模型：自定义模型名称</li>
<li>密钥：开放接口定义的密钥</li>
</ul>
<h2 id="部署-FastGPT-1"><a href="#部署-FastGPT-1" class="headerlink" title="部署 FastGPT"></a>部署 FastGPT</h2><h3 id="Docker-Compose-快速部署"><a href="#Docker-Compose-快速部署" class="headerlink" title="Docker Compose 快速部署"></a>Docker Compose 快速部署</h3><ul>
<li><p>依次执行下面命令，创建 FastGPT 文件并拉取docker-compose.yml和config.json，执行完后目录下会有 2 个文件</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> fastgpt</span><br><span class="line"><span class="built_in">cd</span> fastgpt</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/docker-compose.yml</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/config.json</span><br></pre></td></tr></table></figure>
<ul>
<li>注意: docker-compose.yml 配置文件中 Mongo 为 5.x，部分服务器不支持，需手动更改其镜像版本为 4.4.24</li>
</ul>
</li>
<li><p>修改 docker-compose.yml 中的 OPENAI_BASE_URL（API 接口的地址，需要加&#x2F;v1）和CHAT_API_KEY（API 接口的凭证)</p>
</li>
<li><p>使用 OneAPI 的话，OPENAI_BASE_URL&#x3D;OneAPI访问地址&#x2F;v1；CHAT_API_KEY&#x3D;令牌</p>
</li>
<li><p>修改 config.json 中的本地问答大模型、Embedding 模型以及 ReRank 模型的相关信息</p>
</li>
<li><p>在 docker-compose.yml 同级目录下执行</p>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入项目目录</span></span><br><span class="line"><span class="built_in">cd</span> 项目目录</span><br><span class="line"><span class="comment"># 创建 mongo 密钥</span></span><br><span class="line">openssl rand -<span class="built_in">base64</span> 756 &gt; ./mongodb.key</span><br><span class="line"><span class="comment"># 600不行可以用chmod 999</span></span><br><span class="line"><span class="built_in">chmod</span> 600 ./mongodb.key</span><br><span class="line"><span class="built_in">chown</span> 999:root ./mongodb.key</span><br><span class="line"><span class="comment"># 启动容器</span></span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure>
</li>
<li><p>初始化 Mongo 副本集(4.6.8以前可忽略)</p>
<ul>
<li>Mongo 数据库需要修改副本集的host，从原来的mongo:27017修改为ip:27017。</li>
</ul>
  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 mongo 容器是否正常运行</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mongo bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">mongo -u myname -p mypassword --authenticationDatabase admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化副本集。如果需要外网访问，mongo:27017 可以改成 ip:27017。但是需要同时修改 FastGPT 连接的参数（MONGODB_URI=mongodb://myname:mypassword@mongo:27017/fastgpt?authSource=admin =&gt; MONGODB_URI=mongodb://myname:mypassword@ip:27017/fastgpt?authSource=admin）</span></span><br><span class="line">rs.initiate(&#123;</span><br><span class="line">  _id: <span class="string">&quot;rs0&quot;</span>,</span><br><span class="line">  members: [</span><br><span class="line">    &#123; _id: 0, host: <span class="string">&quot;mongo:27017&quot;</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># 检查状态。如果提示 rs0 状态，则代表运行成功</span></span><br><span class="line">rs.status()</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="访问-FastGPT-1"><a href="#访问-FastGPT-1" class="headerlink" title="访问 FastGPT"></a>访问 FastGPT</h3><ul>
<li>目前可以通过 ip:8080 直接访问(注意防火墙)。登录用户名为 root，密码为 docker-compose.yml 环境变量里设置的 DEFAULT_ROOT_PSW</li>
<li>如果需要域名访问，请自行安装并配置 Nginx</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">ColdSnap</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://coldwave96.github.io/2024/03/08/FastGPT/">https://coldwave96.github.io/2024/03/08/FastGPT/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://coldwave96.github.io">ColdSnap の Blog</a>！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/FastGPT/">FastGPT</a><a class="post-meta__tags" href="/tags/Docker-Compose/">Docker Compose</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2024/03/31/SecLLM/"><i class="fa fa-chevron-left">  </i><span>Framework of building SecLLM</span></a></div><div class="next-post pull-right"><a href="/2024/02/22/WebshellCNN/"><span>基于深度神经网络的Webshell静态检测</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="vcomment"></div><script src="https://cdn1.lncld.net/static/js/3.0.4/av-min.js"></script><script src="https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js"></script><script>var notify = 'false' == 'true';
var verify = 'false' == 'true';
var record_ip = '' == 'true';
var GUEST_INFO = ['nick','mail','link'];
var guest_info = 'nick,mail,link'.split(',').filter(function(item){
  return GUEST_INFO.indexOf(item) > -1
});
guest_info = guest_info.length == 0 ? GUEST_INFO :guest_info;
window.valine = new Valine({
  el:'#vcomment',
  notify:notify,
  verify:verify,
  recordIP:record_ip,
  appId:'R82KAmOmkqMWNJs72Fx4ub1O-gzGzoHsz',
  appKey:'Sq4B8VcTqxJ2qCeJLJa3KbKz',
  placeholder:'Please leave something ^.^',
  avatar:'mm',
  guest_info:guest_info,
  pageSize:'10',
  lang: 'zh-cn'
})</script></div></div><footer class="footer-bg" style="background-image: url(/img/background.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2025 By ColdSnap</div><div class="framework-info"><span>驱动 - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="icp"><a><span>Welcome to my world ^.^</span></a></div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.8.2"></script><script src="/js/fancybox.js?version=1.8.2"></script><script src="/js/sidebar.js?version=1.8.2"></script><script src="/js/copy.js?version=1.8.2"></script><script src="/js/fireworks.js?version=1.8.2"></script><script src="/js/transition.js?version=1.8.2"></script><script src="/js/scroll.js?version=1.8.2"></script><script src="/js/head.js?version=1.8.2"></script><script src="/js/search/local-search.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">本地搜索</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章"></div></div></div><hr><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>由</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a>
 <span>提供支持</span></div></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>